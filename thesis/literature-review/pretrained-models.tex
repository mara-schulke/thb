\subsection{Pre-trained Language Models}

% TODO: Condense

The advantages of combining specialized neural networks with general-purpose pre-trained language models
led to a pivotal point in the NL2SQL research field towards focusing increasingly on the application of pre-trained
language models for NLIDBs. Models like BERT or T5 offer noticable performance improvements (especially when it
comes to language understanding) over specialized NL2SQL networks due to training happening on unrestrained amounts
of natural language data, instead of pure NL2SQL datasets which are often fairly limited in size and therfore natural
language use — \textsc{Spider2.0} which is a contemprary NL2SQL benchmark consists of just 632 real-world questions 
\citep{Spider2}. Thus PLM-based (or at least augumented) NL2SQL systems can observe dramatic performance improvements
through the language models's ability to understand patterns and identify semantic relationships of natural language
query elements. 

\subsubsection{Early Pre-trained Language Model Adaptations}

The above outlined benefits have led to concrete research efforts focusing on the question whether the sole application
of pre-trained language models could outperform neural state-of-the-art approaches — which often implicitly require
a far more sophisticate architecture when it comes to natural language analysis.

In the time of emerging PLM applicationm \textsc{Grappa} was introduced by \citeauthor*{GRAPPA} in \citeyear{GRAPPA} —
a novel grammar-augmented pre-training approach built on RoBERTa\textsubscript{\tiny{LARGE}} (a derivative model from BERT).
It generates synthetic training data (ie. natural language and sql pairs) using a synchronous context-free grammar (SCFG)
which analyses and identifies patterns in natural language queries that can be used as templates for sythesizing training
data. The specialized pre-training helps \textsc{Grappa} to establish a robust connection between natural-language and
database schema elements, showing significant improvements on existing approaches on multiple contemporary benchmarks like 
\textsc{Spider} and \textsc{WikiSQL} \citep{GRAPPA}.

Several NL2SQL approaches in this era focused on \textit{schema understanding} and \textit{schema linking} — the 
generalizability of PLMs required advanced techniques on ensuring that models both understand the semantic intent
of users when querying and correctly identify database schema elements in natural language queries. Thus improving
semantic accuracy of generated SQL queries. \textsc{StruG} (Structure-Grounded-Pretraining) was introduced in
\citeyear{STRUG} by \citeauthor*{STRUG} and presented a novel pretraining approach that improves model abilities when it comes to
\textit{schema linking}, it separates the problem in three facets: column grounding, value grounding and
column-value mapping. In direct comparison with \textsc{Grappa}, \textsc{StruG} achieves similar performance
while being significantly cheaper to train \citep{STRUG}.

In parallel, \citeauthor*{GAZP} released GAZP (Grounded Adaptation for Zero-shot Executable Semantic Parsing) in
\citeyear{GAZP}. \citeauthor*{GAZP} specifically addressed the challenge of adapting semantic parsers across databases
/ domains which was a apparent problem with neural approaches which had a strong tendency to overfit on benchmark datasets. 
Its novel contribution was the combination of forward semantic parsing with a backward utterance generator which allowed
for data synthesis in unseen environments which could then be used to adapt the semantic parser \citep{GAZP}. This
approach enables a improvement in robustness and accuracy in situations where training and inference environments
differ without requiring manually annotated examples \citep{GAZP}.

\subsubsection{Advanced Pre-trained Language Model Approaches}

Building on earlier foundational research on PLM application for NL2SQL tasks, researchers have developed increasingly complex
systems that leveraged pre-trained language models whilst addressing their limitations when it comes to generating valid SQL.

\citeauthor*{RYANSQL} introduced \textsc{Ryansql} (Recursively Yielding Annotation Network for SQL) in \citeyear{RYANSQL},
which implements a sketch-based approach for decomposing complex SQL generation into multiple smaller problems. \textsc{Ryansql}
transformed nested statements into a set of top-level statements using the Statement Position Code (SPC) technique. This
flattening of structure allowed \textsc{Ryansql} to limit the complexity of the query generation problem whilst maintaining
its ability to answer complex questions by recomposing complex queries from their parts. This approach allowed \textsc{Ryansql}
to achieve 58.2\% accuracy on the \textsc{Spider} benchmark, representing a 3.2\% improvement over contemporary state-of-the-art
approaches at the time \citep{RYANSQL}. The sketch-based approach makes \textsc{Ryansql} a PLM-augumented successor of 
\textsc{SQLNet} which was a early neural approach to employ sketch-based query generation \citep{RYANSQL, SQLNet}.

A significant advancement in terms of execution accuracy was reached with the application of T5-Models for NL2SQL tasks. T5
(Text-to-Text Transfer Transformer) Models have proven themselves as well-suited for for query generation — T5-3B for NL2SQL
yielded 71.4\% execution accuracy and thus presented a breakthrough in this domain of research \citep{T2SQL-LLM-Bench-3}.
This established a new baseline for PLM-based approaches and demonstrated that general-purpose language models could not
only compete but outperform specialized architectures by far when properly fine-tuned \citep{T2SQL-LLM-Bench-3}.

Following the adavancements through T5, \citeauthor*{GRAPHIX} introduced GRAPHIX-T5 in \citeyear{GRAPHIX}, which combined
the T5 PLMs with a further graph-aware layers for NL2SQL tasks. This architecture could leverage both pre-trained knowledge
of T5 models aswell as the database schema structure during inference. GRAPHIX-T5 constructs a schema graph where nodes
represent tables and columns and edges represent relationships between them, such as foreign keys or columns association.
This architecture allows the model to deeply understand relationships and the layout of the database schema \citep{GRAPHIX}. 
GRAPHIX-T5 outperformed standard T5 models significantly, with GRAPHIX-T5\textsubscript{\tiny{LARGE}} showing 6.6\% increase
in execution accuracy over T5\textsubscript{\tiny{LARGE}}. When both GRAPHIX and the baseline T5 models were combined with
\textsc{Picard} (a novel constrained decoding mechanism) absolute $\delta$ between them jumped to 7.6\% (81.0\% in absolute
numbers), evaluated on \textsc{Spider-Dev} \citep{GRAPHIX}.

In parallel, \citeauthor*{RESDSQL} proposed \textsc{Resdsql} in \citeyear{RESDSQL}, which proposed to decouple
\textit{schema linking} and \textit{skeleton parsing}. This addressed the typical challenges sequence-to-sequence models
faced when simultaneously trying to link both schema elements and generate the query skeleton (e.g. 
\texttt{SELECT <columns> FROM <table>}). \textsc{Resdsql} further employed a ranking approach to filter schema elements
before passing them to the model for query generation, which reduced noise (when working with large database schemas)
and enabled passing only the most relevant parts. This twofold approach allowed \textsc{Resdsql} to achieve state-of-the-art
performance when being evaluated on \textsc{Spider}, outperforming GRAPHIX-T5\textsubscript{\tiny{3B}}-PICARD by 0.8\% in
execution accuracy. When combined with \textsc{NatSQL} (a contemporary intermediate representation approach introduced
by \cite{NATSQL}) absolute improvement over GRAPHIX-T5\textsubscript{\tiny{3B}}-PICARD jumped to 3.1\% emphasizing the
robustness gain decoupeled architectures have over model-oriented approaches.

These advancement showed rapid improvements over earlier methods — far surpassing neural approaches — through advanced
mechanisms when it comes to schema understanding and query generation. The wide language understanding inherited from
PLM-basemodels further strengthens robustness and shows effectiveness through a large gain on the \textsc{Spider}
benchmark. Collectively these approaches represent a leap in NL2SQL research, emphasizing their usability potential
and real-world feasibility. This era primed the research field for the transition towards large language model adoption.

\subsubsection{Constrained Decoding and Ranking Techniques}

A major challenge in NL2SQL research is making sure that model generated queries are not just semantically accurate
but also syntactically valid queries and thus executable. To address this issue \citeauthor*{PICARD} released \textsc{Picard}
(Parsing Incrementally for Constrained Auto-Regressive Decoding) in \citeyear{PICARD}, a constrained decoding mechanism
for language models which utilizes the SQL grammar and constrained decoding mechanisms to incrementally parse the generate SQL,
rejecting invalid tokens based on the grammar. \textsc{Picard} showed to significantly improve the performance of pre-trained
language models (like T5 or BERT) when it comes to NL2SQL tasks, lifting them from mid-level to state-of-the-art solutions
on the \textsc{Spider} benchmark \citep{PICARD}.

\textsc{Picard} operates as a incremental parser during model output decoding of pre-trained language models and continuously
evaluates the probability of each token. Instead of just passing model outputs to a database for execution \textsc{Picard}
incrementally parses and validates the generated SQL, rejecting tokens if neeeded thus significantly improving the valid
output accuracy (sometimes referred to as VA) of language models. This approach is addressing a significant issue associated
with pre-trained language models — while they outperform in natural language understanding and reasoning, they often lack
SQL grammer knowledge and tend to generate queries that are not executable due to ther unconstrained output space \citep{PICARD}.

The above introduces RESDSQL built ontop of \textsc{Picard}'s foundations and used a ranking-enhanced framework for input
encoding. These two approaches represent a unqiue class of approaches that utilize input and output constraining in order
to increase the performance characteristics of pre-trained language models \citep{RESDSQL}.

\subsubsection{Advantages of PLM Approaches}

PLM approaches to NL2SQL tasks have yielded significant performance improvements for the NL2SQL domain and represent a
leap in NLIDB-research. They primed the research field towards using language models which led to a transition towards
large language models in the following years. Namely PLM approaches brought a series of upsides with them:

\begin{enumerate}
    \item \textbf{Compute Efficiency} — PLMs like \textsc{RESDSQL} achieve high accuracy (up to 84.1\% on \textsc{Spider}
        depending on variants) wilst using far fewer parameters than conteporary LLMs, making them significantly more efficient
        and therefore reduce hardware requirements for their deployment \citep{RESDSQL}.
    \item \textbf{Transferability} — Approaches like \textsc{Grappa} and \textsc{StruG} can incorporate domain-specific
        understanding
        of natural language, table structures and SQL syntax during pre-training which addressed one of the primary issues with
        neural approaches \citep{GRAPPA, STRUG}.
    \item \textbf{Vocabulary} — PLMs offer a larger vocabulary due to the vast amounts of training data available. This enables
        them to handle a wide variety of natural language patterns which addresses the benchmark-overfitting tendency of neural 
        approaches which primarily trained on the development sets of contemporary benchmarks.
\end{enumerate}

\subsubsection{Limitations of PLM Approaches}

Although representing the state-of-the-art at the time, PLMs introduce a class of problems which are associated with their
non-NL2SQL associated nature. There have been an array of approaches to mitigate these shortcommings but nonetheless they
must be considered when using a PLM-based approach to NL2SQL:

\begin{enumerate}

    \item \textbf{Fine-tuning Requirements} — Most PLMs require substantial domain-specific, or at least NL2SQL specific,
          fine-tuning, limiting a straight forward adaptation to new domains or databases. Although being significantly
          more efficient than LLM-based approaches the potential need for initial fine-tuning represent a significant
          computational resource burden. Furthermore when not using synthetic data generation (e.g. \textsc{Grappa})
          annotated datasets of training data are needed to achieve appropiate performance characteristics \citep{GRAPPA}.
    \item \textbf{Wide Input \& Output Space} — Due to the general nature of PLMs their input and output space is often
          far larger than needed 
          NL2SQL tasks. ``Large pre-trained language models for textual data have an unconstrained output space; at each
          decoding step, they can produce any of 10,000s of sub-word tokens'' \citep{PICARD}. This applies to both the
          input and output token space, therefore multiple approaches have been researched which focus on constraining
          these to the subset needed for NL2SQL tasks. Namely GRAPHIX-T5 and \textsc{Picard} have proposed potential
          (and promising) solutions to this issue \citep{GRAPHIX, PICARD}.
    \item \textbf{Limited Schema Awareness} — Due to being general purpose, and non-NL2SQL optimized, PLMs tend to
          incorporate limited amounts of schema awareness when being applied out of the box for NL2SQL tasks. Multiple
          research efforts focused on improving this situation, most notably \textsc{Resdsql} and GRAPHIX-T5 tried to
          improve the schema linking \& awareness of PLMs \citep{RESDSQL, GRAPHIX}, nonetheless the non-specialized nature
          of PLMs prevents NL2SQL being part of the fundamental model architecture.
\end{enumerate}

These characteristics positioned PLMs as powerful but comparatively resource-intensive solutions for NL2SQL (especially
in direct comparison with neural approaches), ultimately yielding the research domain to transition toward exploring
Large Language Model approaches that promise even greater flexibility in adaptation and potentially superior handling
of complex queries through advanced in-context learning approaches.

\subsubsection{Comparison with Large Language Models}

The research on applying pre-trained language models for NL2SQL tasks primed the field for the transition towards LLM usage. While
PLMs like T5 and BERT range from millions to a few billion parameters, prevalent LLMs such as GPT-3 and GPT-4 operate at
significantly larger scales, ranging from a few billions to hundred of billions parameters. The scale of LLMs enables in-context
learning techniques that enable significantly easier and cheaper transferability of NL2SQL systems across domains \citep{DAIL-SQL}.
The $\delta$ of deployment, inference and training requirements of these two approaches are significant due to the size difference
in models, which transfer to hardware requirements and therefore cost. While PLMs can require extensive fine-tuning on
domain-specific data which may aswell be resource intensive \citep{GRAPHIX, RESDSQL, GRAPPA, STRUG}, LLMs transfer the cost to the 
inference environment, where model modificants are less impactful, due to the extensive pre-training that took place. Approaches 
like \textsc{DinSQL} show that with the application of LLMs the engineering challenges around model instruction gained relevance 
while model training became less of a central problem to solve \citep{DINSQL}.
