\subsection{Pre-trained Language Models}

Combining specialized neural networks with pre-trained language models led to a
pivotal point in the NL2SQL research field towards focusing on the application
of pre-trained language models for NL2SQL systems. Models like BERT or T5
offered performance improvements over specialized neural networks due to
training on large natural language data, instead of smaller NL2SQL datasets —
\textsc{Spider2.0} which is a contemporary NL2SQL benchmark consists of just
632 real-world questions \citep{Spider2}. PLM-based NL2SQL systems observed
performance improvements through improved understanding of patterns and
identification of semantic relationships.

\subsubsection{Early Pre-trained Language Model Adaptations}

\textsc{Grappa} presented a grammar-augmented pre-training approach in
\citeyear*{GRAPPA} which was built on RoBERTa\textsubscript{\tiny{LARGE}},
is a derivative model from BERT \citep{GRAPPA}. It was trained by generating
synthetic training data using a Synchronous Context-Free Grammar (SCFG) which
identifies patterns in natural language queries as templates for synthesizing
training data. Specialized pre-training allowed \textsc{Grappa} to establish a
connection between natural-language and database schema elements, showing
improvements on existing approaches on benchmarks like \textsc{Spider} and
\textsc{WikiSQL} \citep{GRAPPA}.

\textsc{StruG} (Structure-Grounded-Pretraining) was subsequently introduced in
\citeyear{STRUG} by \citeauthor*{STRUG} and presented a pretraining approach
that improves model abilities for schema linking, separating the
problem in three facets: column grounding, value grounding and column-value
mapping. \textsc{StruG} achieves similar performance to \textsc{Grappa} while
being cheaper to train \citep{STRUG}.

\subsubsection{Advanced Pre-trained Language Model Approaches}

\citeauthor*{RYANSQL} introduced \textsc{Ryansql} (Recursively Yielding
Annotation Network for SQL) in \citeyear{RYANSQL}, which implemented a
sketch-based approach for decomposing complex SQL generation into smaller
problems. \textsc{Ryansql} transformed nested statements into top-level
statements using the Statement Position Code (SPC) technique, achieving 58.2\%
accuracy on \textsc{Spider}, a 3.2\% improvement over state-of-the-art
\citep{RYANSQL}.

T5-3B for NL2SQL yielded 71.4\% execution accuracy, a breakthrough in this
domain \citep{T2SQL-LLM-Bench-3}.

\citeauthor*{GRAPHIX} introduced GRAPHIX-T5 in \citeyear{GRAPHIX}, which
combined T5 PLMs with graph-aware layers for NL2SQL tasks. GRAPHIX-T5
constructs a schema graph where nodes represent tables and columns and edges
represent relationships between them, such as foreign keys or column
association \citep{GRAPHIX}. GRAPHIX-T5 outperformed standard T5 models, with
GRAPHIX-T5\textsubscript{\tiny{LARGE}} showing 6.6\% increase in execution
accuracy over T5\textsubscript{\tiny{LARGE}}. When both GRAPHIX and baseline T5
models were combined with \textsc{Picard} (a constrained decoding mechanism)
absolute $\Delta$ between them jumped to 7.6\% (81.0\% in absolute numbers),
evaluated on \textsc{Spider} (dev) \citep{GRAPHIX}.

\citeauthor*{RESDSQL} proposed \textsc{Resdsql} in \citeyear{RESDSQL}, which
decoupled \textit{schema linking} and \textit{skeleton parsing}. \textsc{Resdsql}
employed a ranking approach to filter schema elements before passing them to
the model for query generation, improving token efficiency and achieving
state-of-the-art performance on \textsc{Spider}, outperforming
GRAPHIX-T5\textsubscript{\tiny{3B}}-PICARD by 0.8\% in execution accuracy.
When combined with \textsc{NatSQL} \citep{NATSQL} absolute improvement jumped
to 3.1\%.

\subsubsection{Constrained Decoding and Ranking Techniques}

A major challenge in NL2SQL research is ensuring model generated queries are
syntactically valid. \citeauthor*{PICARD} released \textsc{Picard} (Parsing
Incrementally for Constrained Auto-Regressive Decoding) in \citeyear{PICARD},
a constrained decoding mechanism which utilizes SQL grammar to incrementally
parse generated SQL, rejecting invalid tokens. \textsc{Picard} significantly
improved performance of pre-trained language models (like T5 or BERT),
lifting them from mid-level to state-of-the-art solutions on \textsc{Spider}
\citep{PICARD}.

This addresses an issue associated with pre-trained language models — while
they outperform in natural language understanding, they often lack SQL grammar
knowledge and tend to generate queries that are not executable due to
unconstrained output space \citep{PICARD}.

\subsubsection{Advantages of PLM Approaches}

PLM approaches to NL2SQL tasks yielded performance improvements and primed the
field towards large language models. PLM approaches brought upsides:

\begin{enumerate}
    \item \textbf{Compute Efficiency} — PLMs like \textsc{RESDSQL} achieve high
        accuracy (up to 84.1\% on \textsc{Spider}) whilst using fewer parameters
        than contemporary LLMs \citep{RESDSQL}.
    \item \textbf{Transferability} — Approaches like \textsc{Grappa} and
        \textsc{StruG} incorporate domain-specific understanding of natural
        language, table structures and SQL syntax during pre-training, addressing
        a primary issue with neural approaches \citep{GRAPPA, STRUG}.
    \item \textbf{Vocabulary} — PLMs offer larger vocabulary due to vast
        training data, enabling them to handle variety of natural language
        patterns and addressing benchmark-overfitting of neural approaches.
\end{enumerate}

\subsubsection{Limitations of PLM Approaches}

Although representing state-of-the-art at the time, PLMs introduce problems
associated with their non-NL2SQL nature:

\begin{enumerate}
    \item \textbf{Fine-tuning Requirements} — Most PLMs require substantial
        domain-specific fine-tuning, limiting adaptation to new domains.
        Furthermore when not using synthetic data generation (e.g.\
        \textsc{Grappa}) annotated training datasets are needed \citep{GRAPPA}.
    \item \textbf{Wide Input \& Output Space} — ``Large pre-trained language
        models for textual data have an unconstrained output space; at each
        decoding step, they can produce any of 10,000s of sub-word tokens''
        \citep{PICARD}. GRAPHIX-T5 and \textsc{Picard} proposed solutions
        \citep{GRAPHIX, PICARD}.
    \item \textbf{Limited Schema Awareness} — PLMs tend to incorporate limited
        schema awareness out of the box. \textsc{Resdsql} and GRAPHIX-T5 tried
        to improve schema linking \& awareness \citep{RESDSQL, GRAPHIX},
        nonetheless the non-specialized nature prevents NL2SQL being part of
        fundamental model architecture.
\end{enumerate}

\subsubsection{Comparison with Large Language Models}

Research on applying pre-trained language models for NL2SQL primed the field
for transition towards LLM usage. While PLMs range from millions to a few
billion parameters, prevalent LLMs such as GPT-3 and GPT-4 operate at larger
scales, enabling in-context learning techniques for easier transferability
across domains \citep{DAIL-SQL}. While PLMs require extensive fine-tuning on
domain-specific data \citep{GRAPHIX, RESDSQL, GRAPPA, STRUG}, LLMs transfer
cost to inference. Approaches like \textsc{DinSQL} show that with LLMs the
engineering challenges around model instruction gained relevance while model
training became less of a central problem \citep{DINSQL}.
