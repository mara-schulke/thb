\subsection{Neural NL2SQL Approaches}

The limitations regarding brittleness, transferability and scalability
of traditional approaches prompted the research community to explore
neural network based NL2SQL systems. After substantial improvements in language
understanding and accuracy weree shown, a larger paradigm shift took place in
research community in the mid-2010s.

\subsubsection{Early and Intermediate Neural Approaches}

\textsc{Seq2SQL} introduced in \citeyear{Seq2SQL} by \citeauthor{Seq2SQL}
marked a breakthrough in the community. The authors of \textsc{Seq2SQL} framed
NL2SQL as a reinforcement learning problem, introduced WikiSQL as training
dataset and proved the viability of neural approaches.

With an execution accuracy of 59.4\% \textsc{Seq2SQL} demonstrated its
capabilities compared to previously existing NL2SQL systems, but showed that
order-sensitivity of sequence-to-sequence neural networks is harming accuracy. 
Subsequent approaches addressed specific limitations: \textsc{SQLNet}
eliminated order-sensitivity through a sketch-based generation approach which
outperformed \textsc{Seq2SQL} by 7.7\% on WikiSQL (dev) and by 6.4\% on WikiSQL
(test). \textsc{TypeSQL} incorporated type-aware encoding of inputs, further
improving performance by 4.7\% above \textsc{SQLNet} while reducing the
required training time by half \citep{TypeSQL}. 

\textsc{SyntaxSQLNet} improved the abaility of NL2SQL systems on complex
queries and marked the transitional period to \textsc{Spider} (see section
\ref{lit:benchmarks:spider}, \cite{Spider}). \textsc{Spider} is a comparatively
large NL2SQL benchmark incorporating 10181 questions answer pairs across 138
domains. Subsequently, \textsc{IRNet} substantially improved the execution
accuracy over \textsc{SyntaxSQLNet} by +28.4pp on \textsc{Spider} (dev) and by
+19.5pp on \textsc{Spider} (test). \textsc{IRNet} introduced an intermediate
representation (IR) format (SemQL) and approac fhed NL2SQL with three-phase processing: 
\textit{schema linking}, IR synthesis using SemQL, and deterministic SQL
translation \citep{IRNet}. Notably, graph neural networks (GNNs) were explored
for neutral network representation of database schemas. \textsc{GNN}
outperformed \textsc{SyntaxSQLNet} and performed on par ($\Delta$ of +1.3pp)
with \textsc{IRNet}, achieving 54.5\% on \textsc{Spider} (dev). No evaluations
were performed on \textsc{Spider} (test) \citep{GNN}

\subsubsection{Relation-Aware Transformer Approaches}

The release of RAT-SQL (Relation-Aware Transformer for SQL) by \cite{RATSQL}
represented a leap for neural NL2SQL approaches. RAT-SQL diverged from earlier
research through modeling the relationship between natural language and the
database schema elements using relation-aware self-attention representing a
novel approach for solving \textit{schema linking} \citep{RATSQL}.

RAT-SQL's primary innovations was the ability to infer, understand and utilize
the relationship between individual tokens in the natural language query and
link it to the database schema. Thus allowing for reasoning capabilities on the
actual database schema while generating the query.

This architecture yielded a 57.2\% in exact match accuracy when being evaluated
on the \textsc{Spider} benchmark, substantially outperforming comparative
approaches like GNN, IRNet and IRNet V2 by 10.5\%, 9.8\% and 8.7\%
respectively. Although overall accuracy improved across all approaches when
being paired with BERT (Bidirectional Encoder Representations from
Transformers, a popular pre-trained lanugage model from Google) the $\Delta$
between the individual approaches remained steady. RAT-SQL outperformed
state-of-the-art approaches by 5\% to 12.2\% which demonstrated its capability
advancement \citep{RATSQL}.

\subsubsection{Comparative Analysis}

Neural approaches progressively improved eachother in multiple dimensions:

\begin{enumerate}
    \item \textbf{Model Complexity} — From from sequence-to-sequence
        (\textsc{Seq2SQL}) to sketch-based and type-augmented approaches
        (\textsc{SQLNet}, \textsc{TypeSQL}), graph-based methods
        (\textsc{GNN}), and relation-aware self-attention (\textsc{RAT-SQL})
        demonstrated increasing architectural complexity in order to achieve
        state-of-the-art performance on benchmarks like \textsc{Spider}
        \citep{Seq2SQL, TypeSQL, GNN, SyntaxSQLNet, RATSQL}.
    \item \textbf{Transferability} — The cross-domain generalization
        capabilities improved substantially, with RAT-SQL achieving the
        strongest performance on \textsc{Spider}'s independent test set,
        thus preventing overfitting on training data \citep{RATSQL}.
    \item \textbf{Robustness} — Structured decoding (\textsc{SyntaxSQLNet}),
        intermediate representations (\textsc{IRNet}), and relation-aware
        attention (\textsc{RAT-SQL}) increasingly enabled the robust handling
        of synonyms, spelling mistakes, and language variations, thus
        successfully addressing the limitations of traditional approaches
        \citep{SyntaxSQLNet, IRNet, RATSQL}.
    \item \textbf{Query Complexity} —The performance on complex queries
        involving multiple tables, relying on complex aggregations, nested
        structures and joins improved dramatically. \textsc{RAT-SQL} marked the 
        the at-the-time capability ceiling on complex queries using neural
        approaches, outperforming \textsc{IRNet} by 10.5\% \citep{IRNet,
        RATSQL}.
    \item \textbf{Schema Understanding} — Approaches like \textsc{TypeSQL},
        \textsc{GNN}, \textsc{RAT-SQL} each demonstrated enhanced \textit{schema
        linking} capabilities, yielding the ability to accurately reason about
        user intent and traverse the database schema during query generation
        \citep{TypeSQL, GNN, RATSQL}.
\end{enumerate}

\subsubsection{Limitations}

Despite the substantial accuracy, transferability and robustness improvements
that were observed \citep{IRNet, RATSQL}, neural NL2SQL approaches still
suffered from serious shortcommings and unsolved challenges:

\begin{enumerate}
    \item \textbf{Training Data} — Neural networks required substantially more
        training data than traditional systems which in turn required
        sophisticated efforts for data collection \citep{Spider}.
    \item \textbf{Correctness} — The misalignment between neural networks
        and formal languages introduced frequent cases where models produced
        invalid SQL. Approaches like \textsc{SyntaxSQLNet} improved this by
        utilizing syntax trees during decoding but nonetheless syntactic
        correctness remained a challenge across multiple iterations of neural
        systems \citep{SyntaxSQLNet}.
    \item \textbf{Domain Language} — Despite the increased transferability
        of neural approache, they still had a limited vocabulary and
        inter-domain understanding of terminology and relation
        between concepts. This yielded highly domain specific natural language
        to be challenging for these systems.
    \item \textbf{Observability} — The black-box nature of neural networks made
        approaches relying on them, particularly the systems utilizing complex
        neural network architectures, hard do understand and explain in case
        where they yielded undesirable output. 
\end{enumerate}

These limitations, combined with demonstrated performance improvements when
paired with pre-trained language models \citep{RATSQL}, motivated further
research into language model-based approaches.
