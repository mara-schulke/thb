\subsection{Benchmarking}

In \citeyear{Spider} \textsc{Spider} was introduced as a standardized benchmark
and was recently followed by \textsc{Bird} in \citeyear{BIRD} and
\textsc{Spider 2.0} in \citeyear{Spider2}. These benchmarks emerged to allow
cross system comparisons within the NL2SQL research community.

\subsubsection{Spider}\label{lit:benchmarks:spider}

\textsc{Spider}, introduced by \citeauthor{Spider} in \citeyear{Spider}, has
become the de facto standard benchmark for evaluating complex and cross-domain
Text-to-SQL systems. It consists of 10,181 questions and 5,693 unique SQL
queries spanning 200 databases across 138 domains. \textsc{Spider}
was split in training and test sets which contain different database in order
to prevent overfitting models from succeeding. \textsc{Spider} evaluates both
\textit{exact matching accuracy} and \textit{execution accuracy}, with
contemporary SOTA systems achieving approximately 85-90\% \textit{execution
accuracy} (as of 2025) \citep{Spider, OmniSQL, XiYan, CHASE}.

\subsubsection{Bird}\label{lit:benchmarks:bird}

The \textsc{Bird} benchmark, released in \citeyear{BIRD}, and addresses the gap between
academic benchmarks and real-world applications by focusing on large-scale
databases with actual data content \citep{BIRD}. \textsc{Bird} contains 12,751
text-to-SQL pairs and 95 databases with a total size of 33.4 GB across 37
professional domains. \textsc{Bird} emphasizes challenges
related to dirty database contents, external knowledge between natural language
questions and database values, and SQL efficiency in massive databases.
Even SOTA LLMs like GPT-4 achieve only
54.89\% execution accuracy on \textsc{Bird}, compared to human performance of
92.96\%, highlighting the significant challenges posed by real-world database
scenarios on NLIDBs \citep{BIRD}.

\subsubsection{Spider 2.0}\label{lit:benchmarks:spider2}

\textsc{Spider 2.0} which was introduced by \citeauthor*{Spider2} in
\citeyear{Spider2} represents the most recent advancements of benchmarks for
NL2SQL systems. \textsc{Spider 2.0} is much smaller with only 632 real-world
problems which were derived from enterprise database use cases. \textsc{Spider
2.0} moves towards testing the deep understanding of the database, requiring
models to understand metadata, SQL dialect documentation and project-level
codebases \citep{Spider2}. The tasks contained in \textsc{Spider 2.0} often
demand multiple complex SQL queries often exceeding the 100-line mark.
\citeauthor{Spider2} further highlights the gap between academic research and
enterprise-level environments with even advanced approaches achieving only
21.3\% on \textsc{Spider2.0} compared to 91.2\% on \textsc{Spider 1.0}
\citep{Spider, Spider2}.
