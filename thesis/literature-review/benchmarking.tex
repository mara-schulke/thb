\subsection{Benchmarking}

% TODO: Condense

In order to evaluate NL2SQL systems standardized benchmarks like \textsc{Spider} and \textsc{Bird} have emerged. These
benchmarks can measure performance across different approaches and models, enable meaningful ablation studies and are a
useful indicator for the state of the research field. In the past decade significant advancements have been made with
\textsc{Spider} being released in \citeyear{Spider} the first major, widely adopted, benchmark emerged in this field
\citep{Spider}.

\subsubsection{Spider}

\textsc{Spider}, introduced by \citeauthor{Spider} in \citeyear{Spider}, has become the de facto standard benchmark for
evaluating complex and cross-domain Text-to-SQL systems. It consists of 10,181 questions and 5,693 unique SQL queries
spanning 200 databases across 138 domains. Previous benchmarks like lacked complexity and cross-domain distrubtion of
datapoints which prevented the \textit{transferability} of approaches or models to be accounted for in benchmarks.
With \textsc{Spider} the capability to be database agnostic was required to achieve meaningful accuracy scores. Furthermore
\textsc{Spider} was split in training and test sets which contain different database in order to prevent overfitting
models from succeeding. This design specifically tests a model's ability to handle schema linking and generalization
challenges rather than memorizing specific database patterns. \textsc{Spider} evaluates both \textit{exact matching
accuracy} and \textit{execution accuracy}, with contemporary state-of-the-art systems achieving approximately 85-90\%
\textit{execution accuracy} (as of 2025) \citep{Spider, OmniSQL, XiYan, CHASE}.

\subsubsection{Bird}

The \textsc{Bird} benchmark (BIg bench for large-scale database gRounded Text-to-SQLs), released in \citeyear{BIRD},
and addresses the gap between academic benchmarks and real-world applications by focusing on large-scale databases
with actual data content \citep{BIRD}. \textsc{Bird} contains 12,751 text-to-SQL pairs and 95 databases with a total
size of 33.4 GB across 37 professional domains. Unlike \textsc{Spider}, which primarily evaluates against database
schemas with minimal content, \textsc{Bird} emphasizes challenges related to dirty database contents, external
knowledge between natural language questions and database values, and SQL efficiency in massive databases. This
places \textsc{Bird} as a relevant benchmark for real world feasibility of approaches and models. Even state-of-the-art
LLMs like GPT-4 achieve only 54.89\% execution accuracy on \textsc{Bird}, compared to human performance of 92.96\%,
highlighting the significant challenges posed by real-world database scenarios on NLIDBs \citep{BIRD}.

\subsubsection{Spider 2.0}

\textsc{Spider 2.0} which was introduced by \citeauthor*{Spider2} in \citeyear{Spider2} represents the most recent
advancements of benchmarks for NL2SQL systems. It represents a significant evolution in NL2SQL benchmarking and focuses
primarily on enterprise level database challenges. \textsc{Spider 2.0} is much smaller with only 632 real-world problems
which were derived from enterprise database usecases, but yet represents a meaningful indicator for the complexity of
databases that approaches and models can handle. \textsc{Spider 2.0} goes beyond simple query generation tasks and moves
towards testing the deep understanding of the database, requiring models to understand metadata, SQL dialect documentation
and project-level codebases \citep{Spider2}. The tasks contained in \textsc{Spider 2.0} often demand multiple complex SQL
queries often exceeding the 100-line mark and require incorporating a diverse set of database operation from transformation
to analytics. \citeauthor{Spider2} further highlights the gap between academic research and enterprise-level environments
with even advanced approaches achieving only 21.3\% on \textsc{Spider2.0} compared to 91.2\% on \textsc{Spider 1.0}
\citep{Spider, Spider2}.

\subsection{Research Gaps}

This literature review highlights that significant progress was made in the development of real world feasible NL2SQL systems
and that the research domain has undergone multiple large paradigm shifts from rule-based to neural-network-based to PLM-based
and most recently to LLM-based approaches. Despite these advancement several critical research gaps remained open or emerged
which limit the practical deployment of NL2SQL systems and their widespread adoption as natural language database interfaces
in real world scenarios.

\subsubsection{Advanced Open-Source Approaches}

While recent research papers have made remarkable progress on both open-source model development (\textsc{CodeS} and
\textsc{OmniSQL}) aswell as advanced architecture development like CHASE and \textsc{XiYan-SQL} (which utilize multi-path
generation and candidate selection, self-correction and RAG), a significant research gap evolved around state-of-the-art
approaches that dont rely on proprietary LLMs and only utilize open-source models. Most contemporary research proposes
solutions which (partially) rely on the baseline proprietary LLM in order to achieve state-of-the-art results. Whilst
these efforts give meaningful signal to the effect that specific prompt engineering techniques, self-correction mechanisms
and candidate selection models have, they are not yet feasible for real world scenarios due to the cost and data privacy
concerns they introduce.

\begin{enumerate}
    \item \textbf{Limited Exploration of Technique Synergies} — Contemporary research primarily focuses on techniques like
          candidate selection, RAG, and self-correction in isolation or with specific closed-source LLMs. A gap remains
          in understanding how these techniques can be optimally combined, especially with emerging open-source models.
          For instance, the potential of synergy of specialized open-source LLM like \textsc{OmniSQL} with candidate-selection,
          advanced self-correction and subset-encoding of database schemas has yet to be researched.
    \item \textbf{Efficiency-Accuracy Tradeoffs} — Whilst recent research efforts focus primarily on achieving a new
          state-of-the-art execution accuracy metric on prevalent benchmarks, the relationship of computational requirements and
          performance gains achieved remains unclear. Some research papers present ablation studies indicating the relevance
          of certain architecture components but the overall relationship between state-of-the-art performance and cost
          is still underexplored. 
     \item \textbf{Cross-Technique Optimization} — There is limited research on how to optimize the interplay between the various
          NL2SQL techniques. For example, it is yet unclear how subset-encoding of database schemas might impact the effectiveness
          of multi-path generation and candidate selection, or whether synthetic example generation for in-context-learning is 
          effective when working with specialized models versus general-purpose LLMs.
\end{enumerate}

