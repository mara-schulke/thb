\subsection{Functions}\label{design:components}

The following sets are used to introduce the functions $\sigma$, $\phi$, $\pi$, $\rho$ and $\nu$.

\begin{enumerate}
    \item $\mathcal{Q}$ – The set of all possible natural language queries.
    \item $\mathcal{S}$ – The set of all possible database schemas.
    \item $\mathcal{E}$ – The set of all possible execution functions for SQL validation.
    \item $\mathcal{V}$ – The set of all possible embedding spaces.
    \item $\mathcal{D}$ – The set of all possible distance indices over observed databases.
    \item $\mathcal{C}$ – The set of all possible candidate sets $\mathcal{C}'$.
    \item $\mathcal{Q}_{\mathcal{S}}$ – The set of all valid queries over the database schema $S$.
\end{enumerate}

\subsubsection{Example Selection – $\sigma$}\label{design:selection-function}

The selection function $\sigma$ retrieves relevant examples from the
embedding space $v$ based on semantic similarity to the
input query. This function implements the example selection mechanism
that enables few-shot learning for SQL generation.

$$
\sigma: \mathcal{Q} \times \Omega_{\mathcal{S}} \times \mathcal{S} \times \mathcal{V} \times \mathcal{D} \rightarrow \mathcal{C}'
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
\sigma(q, z, s, v, d) = \{(q_1, s_1, \omega_1, d_1), \ldots, (q_k, s_k, \omega_k, d_k)\}
$$

\vspace{0.5em}

where $q \in \mathcal{Q}$ is the input natural language query, $z \in \Omega_{\mathcal{S}}$
is the zero-shot inference result, $s \in \mathcal{S}$ is the database schema,
$v \in \mathcal{V}$ is the embedding space, $d \in \mathcal{D}$ is the distance index,
and the result is a candidate set $\mathcal{C}' \in \mathcal{C}$ containing $k$
tuples of natural language queries, schemas, corresponding SQL queries and their
combined distance.

The function uses cosine similarity in the embedding space to identify examples
that are semantically closest to the input query $q$, considering natural
language representations and schema distance as described in Section~\ref{design:embedding}.
This approach ensures examples are weighted by the following properties:

\begin{enumerate}
    \item \textbf{Question similarity} – Semantic similarity between the input
        query and observed natural language queries in the embedding space
        $v$, measured using cosine distance between their embeddings.
    \item \textbf{SQL similarity} – Structural similarity between the zero-shot inference
        result $z$ and candidate SQL queries using code representations, enabling
        pattern recognition across different database domains.
    \item \textbf{Database similarity} – Schema compatibility measured through the distance
        index $d$, ensuring selected examples operate on analogous database
        structures with similar table relationships and column types.
\end{enumerate}

The exact weighting of these can be adjusted through three constants, $w_q$, $w_s$, and $w_d$.

\begin{algorithm}
\caption{$\sigma$ - Example Selection}\label{algorithms:sigma}
\begin{algorithmic}[1]
\Require $q \in \mathcal{Q}$, $z \in \Omega_{\mathcal{S}}$, $s \in \mathcal{S}$, $v \in \mathcal{V}$, $d \in \mathcal{D}$
\Require $k \in \mathbb{N}$, $k \geq 1$                            \Comment{Number of examples to select}
\State $candidates  \gets \emptyset$                               \Comment{Initialize candidate set}
\For{$(q_i, s_i, \omega_i) \in v$}                                 \Comment{For each sample}
    \State $sim_q \gets cosine(\iota(q), \iota(q_i))$              \Comment{Calculate question similarity}
    \State $sim_s \gets cosine(\iota(z), \iota(\omega_i))$         \Comment{Calculate SQL similarity}
    \State $sim_d \gets d(s, s_i)$                                 \Comment{Calculate schema distance}
    \State $score \gets w_q \cdot sim_q + w_s \cdot sim_s + w_d \cdot sim_d$
    \State $candidates \gets candidates \cup \{(q_i, s_i, \omega_i, score)\}$
\EndFor
\State \Return $top_{k}(candidates)$                               \Comment{Return k highest scoring examples}
\end{algorithmic}
\end{algorithm}

\subsubsection{Schema Subsetting – $\phi$}\label{design:subsetting-function}

The subsetting function $\phi$ reduces the database schema to include
tables, columns, and relationships that are relevant to the candidates
$C$. This schema pruning mechanism reduces the complexity of the SQL generation task
and improves token efficiency by focusing on relevant schema elements.

$$
\phi: \mathcal{C}' \times \mathcal{S} \rightarrow \mathcal{S}'
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
\phi(c, s) = s'
$$

\vspace{0.5em}

where $c \in \mathcal{C}'$ is a candidate set containing selected examples, $s \in \mathcal{S}$
is the full database schema, and $s' \subseteq s$ is the reduced schema subset containing
only relevant tables and columns.

The function analyzes the selected examples in $c$ to identify which schema elements
are commonly referenced in similar queries. This enables
focusing on relevant portions of potentially large
and complex database schemas, improving accuracy and computational
efficiency and preventing exceeding limited context windows.

\begin{algorithm}
\caption{$\phi$ - Schema Subsetting}\label{algorithms:phi}
\begin{algorithmic}[1]
\Require $c \in \mathcal{C}'$, $s \in \mathcal{S}$
\State $s' \gets \emptyset$                          \Comment{Empty schema subset}
\For{$table \in s$}
    \If{$\exists~r \in references(c, table)$}        \Comment{Any candidates reference the table}
        \State $s' \gets s' \cup \{table\}$          \Comment{Extend subsetted schema}
    \EndIf
\EndFor
\State \Return $s'$                                  \Comment{Return subsetted schema}
\end{algorithmic}
\end{algorithm}

\subsubsection{Query Projection – $\pi$}\label{design:projection-function}

The projection function $\pi$ represents the translation mechanism that converts
natural language queries into SQL statements using the selected examples and database
schema. This function encapsulates LLM inference that generates
candidate SQL queries based on the provided context.

$$
\pi: \mathcal{Q} \times \mathcal{S} \times \mathcal{C}' \rightarrow \Omega_{\mathcal{S}}
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
\pi(q, s, c) = \omega
$$

\vspace{0.5em}

where $q \in \mathcal{Q}$ is the input natural language query, $s \in \mathcal{S}$ is
the database schema, $c \in \mathcal{C}'$ is the set of selected examples,
and $\omega \in \Omega_{\mathcal{S}}$ is the generated SQL query.

The function operates by constructing a prompt combining the natural language
query, relevant schema information, and selected examples in a format
optimized for LLM inference. The model generates a SQL query
capturing the semantic intent of the natural language input while
adhering to the constraints of the database schema.

\begin{algorithm}
\caption{$\pi$ - Query Projection}\label{algorithms:pi}
\begin{algorithmic}[1]
\Require $q \in \mathcal{Q}$, $s \in \mathcal{S}$, $c \in \mathcal{C}'$
\Require $\tau \in \mathbb{R}$, $\tau > 0$                     \Comment{Relevance threshold}
\State $c_{rel} \gets \{~(q_i, s_i, \omega_i, d_i) \in c : d_i < \tau~\}$  \Comment{Filter by relevance}
\State $ex \gets \{~fmt(q_i, \omega_i, d_i)~|~(q_i, s_i, \omega_i, d_i) \in c_{rel}~\}$  \Comment{Format examples}
\State $prompt \gets prompt(q, s, ex)$                         \Comment{Construct prompt}
\State $out \gets model(prompt)$                               \Comment{Model inference}
\State $\Omega_{cand} \gets extract_{sql}(out)$                \Comment{Extract SQL candidates}
\For{$\omega_{raw} \in \Omega_{cand}$}                         \Comment{Validate candidates}
    \If{$\omega_{raw} \in \Omega_{\mathcal{S}}$}               \Comment{Check syntactic validity}
        \State \Return $\omega_{raw}$                          \Comment{Return first valid query}
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\subsubsection{Self Refinement – $\rho$}\label{design:refinement-function}

The refinement function $\rho$ implements a self-correction mechanism to iteratively
improve generated SQL queries by identifying and correcting syntax errors,
semantic inconsistencies, and execution failures. This function enables
learning from mistakes and producing higher-quality outputs through
feedback loops.

$$
\rho: \mathcal{Q} \times \mathcal{S} \times \mathcal{E} \times \Omega_{\mathcal{S}} \rightarrow \Omega_{\mathcal{S}}
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
\rho(q, s, e, \omega_{raw}) = \omega_{refined}
$$

\vspace{0.5em}

where $q \in \mathcal{Q}$ is the original natural language query, $s \in \mathcal{S}$
is the database schema, $e \in \mathcal{E}$ is the execution function for
validation, $\omega_{raw} \in \Omega_{\mathcal{S}}$ is the previously generated SQL
query and $\omega_{refined} \in \Omega_{\mathcal{S}}$ is the improved SQL query.

The function operates by executing the candidate query against the database, analyzing
any errors or unexpected results, and prompting the language model to generate
an improved version based on identified issues. This
process continues until a valid, executable query is produced or a maximum number
of attempts is reached.

\begin{algorithm}
\caption{$\rho$ - Self Refinement}\label{algorithms:rho}
\begin{algorithmic}[1]
\Require $q \in \mathcal{Q}$, $s \in \mathcal{S}$, $e \in \mathcal{E}$, $\omega_{raw} \in \Omega_{\mathcal{S}}$
\State $exec \gets e(s, \omega_{raw})$                          \Comment{Verify execution output}
\State $prompt \gets prompt_{refine}(q, s, \omega_{raw}, exec)$ \Comment{Construct refinement prompt}
\State $out \gets model(prompt)$                                \Comment{Generate refinement output}
\State $\Omega_{ref} \gets extract_{sql}(out)$                  \Comment{Extract refined candidates}
\For{$\omega_{ref} \in \Omega_{ref}$}                           \Comment{Validate refined queries}
    \If{$\omega_{ref} \in \Omega_{\mathcal{S}}$}                \Comment{Check syntactic validity}
        \State \Return $\omega_{ref}$                           \Comment{Return first valid refinement}
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\subsubsection{Voting – $\nu$}\label{design:voting-function}

The voting function $\nu$ implements a consensus mechanism that selects the most
reliable SQL query from multiple candidates generated through the pipeline.
This function enhances robustness by leveraging result distribution of multiple
attempts to identify the most likely correct answer.

$$
\nu: \mathcal{C} \times \mathcal{E} \rightarrow \Omega_{\mathcal{S}}
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
\nu(C, e) = \omega_{consensus}
$$

where $C \in \mathcal{C}$ is a set of candidate SQL queries, $e \in \mathcal{E}$ is
the execution function for validation, and $\omega_{consensus} \in \Omega_{\mathcal{S}}$
is the selected consensus query.

\vspace{0.5em}

The voting function $\nu$ implements a majority voting algorithm similar to
OmniSQL \citep{OmniSQL}, where the most frequent result
across multiple attempts is deemed the most likely correct answer.
The function applies frequency-based selection among valid candidates. In cases where
no clear majority exists, the function may apply additional heuristics such as query
complexity or execution performance to determine the final query candidate $\omega_{consensus}$.

\begin{algorithm}
\caption{$\nu$ - Consensus Voting}\label{algorithms:nu}
\begin{algorithmic}[1]
\Require $C \in \mathcal{C}$, $e \in \mathcal{E}$
\State $results \gets \{\}$                               \Comment{Map from result sets to candidate lists}
\For{$\omega \in C$}                                      \Comment{Execute each candidate}
    \State $result \gets e(\omega)$                       \Comment{Execute query}
    \State $results[result] \gets results[result] \cup \{\omega\}$
\EndFor
\State $dist \gets \{results[r] : r \in results\}$        \Comment{Get all result groups}
\State $dist \gets sort(dist, |group| \mapsto |group|)$   \Comment{Sort by group size (largest first)}
\State \Return $top_1(top_1(dist))$                       \Comment{Return largest group}
\end{algorithmic}
\end{algorithm}
