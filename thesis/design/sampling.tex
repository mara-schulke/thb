\subsection{Sampling}\label{design:sampling}

Building upon DAIL-SQL's masked example selection, \textsc{Natural} utilizes 
cosine similarity in an embedding space $\mathcal{V}$ to search for examples based on the % TODO: Fix notation consistency with definitions
masked natural language query $q'$ and the respective masked SQL query $\omega'$.
This allows $\sigma$ to use cosine distance to search through historically observed
question answer pairs; eg, from previous user interactions or prevalent training
datasets like \textsc{Spider}, \textsc{Bird} or \textsc{SynSQL-2.5m}.

\subsubsection{Natural Language Masking}

The natural language masking process replaces all words which are not deemed
\textit{structurally relevant} with the \texttt{<mask>} token. All words $w$
in a predefined whitelist $\mathcal{W}$ are determined structurally relevant. % TODO: Fix typo
% TODO: Fix reference to implementation section
Choosing a sound set $\mathcal{W}$ is described in section~\ref{implementation:whitelisting}.

This process enables the embedding space to capture semantic patterns independently
of specific database artifacts and domain terminology, improving the generalizability
across different schemas and domains. Thus given $\mathcal{W} = \{\text{List},
\text{Show}, \text{all}, \text{the}, \text{with}, \text{name}\}$ the natural language
questions:

\begin{verbatim}
Show all customers with the firstname John
List all products with the name iPhone
\end{verbatim}

Will get masked, such that the semantic words are maintained, but domain specific
terminology is discarded:

\begin{verbatim}
Show all <mask> with the <mask> <mask>
List all <mask> with the name <mask>
\end{verbatim}

Now latent space embeddings can be computed for the masked versions of the natural
language questions.

\subsubsection{Query Masking}

Query masking applies similar abstraction principles to the corresponding SQL queries.
This enables \textsc{Natural} to identify SQL patterns and structures.
The SQL masking process transforms all identifiers to \texttt{<mask>} and all
literals to \texttt{<value>}. Thus, the query:

\begin{verbatim}
SELECT name FROM customers WHERE age > 25
\end{verbatim}

gets masked to:

\begin{verbatim}
SELECT <mask> FROM <mask> WHERE <mask> > <value>
\end{verbatim}

This abstraction enables pattern recognition across different domains while
preserving the essential logical structure of the original SQL statements.

\subsubsection{Embedding Space}

To construct an embedding space $\mathcal{V}_{\mathcal{S}}$ over a set of samples $\mathcal{S} = \{(q_1, s_1), ..., (q_i, s_i)\}$
where $q$ is the natural language question and $s$ is the corresponding SQL query, we need to compute
the embeddings of $mask(q)$ and $mask(s)$ respectively through an embedding function $\iota$. % TODO: Specify embedding model

Thus the embedding space $\mathcal{V}_{\mathcal{S}}$ can be defined as:

$$\mathcal{V}_{\mathcal{S}} = \{~(q, \iota(mask(q)), s, \iota(mask(s)))~|~(q, s) \in \mathcal{S}~\}$$

% TODO: Explain how this relates to the selection function σ from definitions
% TODO: Discuss potential limitations of masked embedding approach

% ALIGNMENT IMPROVEMENTS NEEDED:
% TODO: Add formal mathematical definition using enumerate style like indexing section
% TODO: Use consistent tuple notation (q,s,ω,d) matching definitions section  
% TODO: Add cross-references to Section~\ref{design:indexing} for schema distances
% TODO: Include similarity scoring methodology with weights w_q, w_s, w_sql
% TODO: Add subsection on "Distance Calculation" parallel to indexing section
% TODO: Use consistent variable naming (mathcal{V} vs V vs v)
% TODO: Add example showing multi-dimensional scoring like indexing graphs
% TODO: Include legend/explanation of masking tokens like indexing legend
