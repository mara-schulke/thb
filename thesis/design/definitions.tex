\subsection{Function Definitions}\label{design:function-definitions}

The following sets are subsequently used to introduce the functions

\begin{enumerate}
    \item $\mathcal{Q}$ – The set of all possible natural language queries
    \item $\mathcal{S}$ – The set of all possible database schemas
    \item $\mathcal{E}$ – The set of execution functions for SQL validation
    \item $\mathcal{V}$ – The embedding space of historically observed samples
    \item $\mathcal{D}$ – The space of distance indices over historically observed databases
    \item $\mathcal{C}$ – The set of all possible candidate sets $\mathcal{C}'$
    \item $\mathcal{Q}_{\mathcal{S}}$ – The set of all valid queries over the database schema $S$.
\end{enumerate}

\subsubsection{Query Function – $nq$}

\vspace{0.5em}

$$
nq: \mathcal{Q} \times \mathcal{S} \times E \times \mathcal{V} \times \mathcal{D} \rightarrow \Omega_{\mathcal{S}}
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
nq(q, s, e, v, d) = \omega
$$

\vspace{0.5em}

where $q \in \mathcal{Q}$, $s \in \mathcal{S}$, $e \in \mathcal{E}$, $v \in \mathcal{V}$, $d \in \mathcal{D}$, and $\omega \in \Omega_{\mathcal{S}}$.

\subsubsection{Selection Function – $\sigma$}\label{design:selection-function}

The selection function $\sigma$ retrieves the most relevant examples from the
historical embedding space $\mathcal{V}$ based on semantic similarity to the
input query. This function implements the core example selection mechanism 
that subsequently enables few-shot learning for SQL generation.

$$
\sigma: \mathcal{Q} \times \Omega_{\mathcal{S}} \times \mathcal{S} \times \mathcal{V} \times \mathcal{D} \rightarrow \mathcal{C}'
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
\sigma(q, z, s, v, d) = \{(q_1, s_1, \omega_1, d_1), \ldots, (q_k, s_k, \omega_k, d_k)\}
$$

\vspace{0.5em}

where $q \in \mathcal{Q}$ is the input natural language query, $z \in \Omega_{\mathcal{S}}$ is the zero-shot inference result, $s \in \mathcal{S}$ is the database schema, $v \in \mathcal{V}$ is the embedding space, $d \in \mathcal{D}$ is the distance index, and the result is a candidate set $\mathcal{C}' \in \mathcal{C}$ containing $k$ tuples of natural language queries, schemas, corresponding SQL queries and their combined similarity distances.

The function utilizes cosine similarity in the embedding space to identify examples
that are semantically closest to the input query $q$, considering both masked natural
language representations and schema distance as described in Section~\ref{design:sampling}.
This approach ensures that examples are weighted by the following properties:

\begin{enumerate}
    \item \textbf{Question similarity} – Semantic similarity between the masked input query and historically observed natural language queries in the embedding space $\mathcal{V}$, measured using cosine distance between their respective embeddings
    \item \textbf{Query similarity} – Structural similarity between the zero-shot inference result $z$ and candidate SQL queries using masked SQL representations, enabling pattern recognition across different database domains
    \item \textbf{Database similarity} – Schema compatibility measured through the distance index $\mathcal{D}$, ensuring selected examples operate on analogous database structures with similar table relationships and column types
\end{enumerate}

\subsubsection{Subsetting Function – $\phi$}\label{design:subsetting-function}

The subsetting function $\phi$ intelligently reduces the database schema to only include
tables, columns, and relationships that are relevant to the current query context.
This schema pruning mechanism reduces the complexity of the SQL generation task and
improves model performance by focusing on pertinent schema elements.

$$
\phi: \mathcal{C}' \times \mathcal{S} \rightarrow \mathcal{S}'
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
\phi(c, s) = s'
$$

\vspace{0.5em}

where $c \in \mathcal{C}'$ is a candidate set containing selected examples, $s \in \mathcal{S}$
is the full database schema, and $s' \subseteq s$ is the reduced schema subset containing
only relevant tables and columns.

The function analyzes the selected examples in $c$ to identify which schema elements
(tables, columns, foreign key relationships) are commonly referenced in similar queries.
This analysis enables the system to steer the model's attention on the most relevant
portions of potentially large and complex database schemas, thereby improving both accuracy
and computational efficiency.

\subsubsection{Projection Function – $\pi$}\label{design:projection-function}

The projection function $\pi$ represents the core translation mechanism that converts
natural language queries into SQL statements using the selected examples and reduced schema.
This function encapsulates the language model inference process that generates candidate
SQL queries based on the provided context.

$$
\pi: \mathcal{Q} \times \mathcal{S}' \times \mathcal{C}' \rightarrow \Omega_{\mathcal{S}'}
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
\pi(q, s', c) = \omega
$$

\vspace{0.5em}

where $q \in \mathcal{Q}$ is the input natural language query, $s' \in \mathcal{S}'$ is
the reduced schema subset, $c \in \mathcal{C}'$ is the set of selected examples,
and $\omega \in \Omega_{\mathcal{S}'}$ is the generated SQL query.

The function operates by constructing a prompt that combines the natural language
query, the relevant schema information, and the selected examples in a format
optimized for large language model inference. The model then generates a SQL query
that attempts to capture the semantic intent of the natural language input while
adhering to the constraints imposed by the database schema.

\subsubsection{Refinement Function – $\rho$}\label{design:refinement-function}

The refinement function $\rho$ implements a self-correction mechanism that iteratively
improves generated SQL queries by identifying and correcting syntax errors,
semantic inconsistencies, and execution failures. This function enables the system
to learn from its mistakes and produce higher-quality outputs through automated
feedback loops.

$$
\rho: \mathcal{Q} \times \mathcal{S}' \times \mathcal{E} \times \Omega_{\mathcal{S}'} \rightarrow \Omega_{\mathcal{S}'}
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
\rho(q, s', e, \omega_{prev}) = \omega_{refined}
$$

\vspace{0.5em}

where $q \in \mathcal{Q}$ is the original natural language query, $s' \in \mathcal{S}'$
is the relevant schema subset, $e \in \mathcal{E}$ is the execution environment for
validation, $\omega_{prev} \in \Omega_{\mathcal{S}'}$ is the previously generated SQL
query, and $\omega_{refined} \in \Omega_{\mathcal{S}'}$ is the improved SQL query.

The function operates by executing the candidate query against the database, analyzing
any errors or unexpected results, and then prompting the language model to generate
an improved version based on the identified issues. This iterative refinement
process continues until a valid, executable query is produced or a maximum number
of refinement attempts is reached.


\subsubsection{Voting Function – $\nu$}\label{design:voting-function}

The voting function $\nu$ implements a consensus mechanism that selects the most
reliable SQL query from multiple candidate solutions generated through the pipeline.
This function enhances robustness by leveraging the wisdom of multiple generation
attempts to identify the most likely correct answer.

$$
\nu: \mathcal{C} \times \mathcal{E} \rightarrow \Omega_{\mathcal{S}'}
$$

\vspace{0.5em}

For a specific input instance, we write:

$$
\nu(C, e) = \omega_{consensus}
$$

where $C \in \mathcal{C}$ is a set of candidate SQL queries, $e \in \mathcal{E}$ is
the execution environment for validation, and $\omega_{consensus} \in \Omega_{\mathcal{S}'}$
is the selected consensus query.

\vspace{0.5em}

The voting function $\nu$ implements a majority voting algorithm similar to that
described by OmniSQL \citep{OmniSQL}, where the result that appears most frequently
across multiple generation attempts is deemed to be the most likely correct answer.
The function first validates all candidates for syntactic correctness and executability,
then applies frequency-based selection among the valid candidates. In cases where
no clear majority exists, the function may apply additional heuristics such as query
complexity or execution performance to determine the final selection.
