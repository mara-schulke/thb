\documentclass{article}

\usepackage[a4paper]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{float}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage[natbibapa]{apacite}

\graphicspath{{./images/}}

\pagestyle{fancyplain}
\fancyhf{}
\lhead{\fancyplain{}{Mara Schulke} }
\rhead{\fancyplain{}{\today}}
\cfoot{\fancyplain{}{\thepage}}

\newcommand{\figuresource}[1]{
	\begin{center}Quelle: {#1}\end{center}
}

\titleformat{\chapter}{\normalfont\Large\bfseries}{\thechapter.}{20pt}{\Large}
\titleformat{\section}{\normalfont\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\small\bfseries}{\thesubsubsection}{1em}{}

\begin{document}

\begin{titlepage}
    \begin{center}
        \begin{Large}
            Brandenburg University of Applied Sciences \\[1em]
        \end{Large}
        IT Security \\
        Computerscience \\
        Prof. Dr. Oleg Lobachev \\
        MSc. Florian Eich
    \end{center}

    \vfill

    \begin{center}
        \Large{A Natural Language Interface Implementation for PostgreSQL using LLMs, Self-Correction and Incremental Schema Analysis}\\[0.5em]
        \large{Bachelor Thesis}\\[1em]
        
        \begin{normalsize}
            Summer semester 2025\\[0.25em]
            \today
        \end{normalsize}
    \end{center}

    \vfill

    \begin{center}
        Mara Schulke – Matr-Nr. 20215853
    \end{center}
\end{titlepage}

\begin{abstract}
This thesis explores the integration of large language models (LLMs) into PostgreSQL database systems in order
to make the database accessible via natural language instead of the postgres SQL dialect. The research focuses
on implementation strategies, performance optimization, and practical applications of this concept.
\end{abstract}

\tableofcontents

\listoffigures

\section*{List of Abbreviations}
\begin{tabular}{ll}
GPT & Generative Pretrained Transformer \\
SQL & Structured Query Language \\
API & Application Programming Interface \\
LLM & Large Language Model \\
DBMS & Database Management System \\
NL2SQL & Natural Language to SQL \\
\end{tabular}

\newpage

% 1/3 Vorarbeit, Literature Review
% 1/3 Decomp und System Design
% 1/3 Implementation und Evaluation

% Introduction
\section{Introduction}
\subsection{Problem Statement and Motivation}

Database systems represent a backbone of modern computer science, allowing for rapid advancements
whilst shielding us from the problem categories that come along with managing and querying large amounts
of, usually structured, data efficiently. However, most Database Management Systems (DBMS) have
traditionally required specialized knowledge, usually of the Structured Query Language (SQL), in order
to become useable. Whilst this barrier may be percieved differently across diverse usergroups it
represents a fundamental misalignment between end-user goals (e.g. analysts, researchers, domain experts
etc.) and the underlying DBMS, thus often requiring software engineering efforts in order to reduce this friction.

This barrier is the reason entire classes of software projects exists (for example, admin / support panels),
data analytics tools etc. which therefore introduce significant churn and delay between the implementation
of a database system and reaching the desired end user impact. Often these projects span multiple years, require
costly staffing and yield little to no novel technical value.

Emerging technologies such as Large Language Models (LLMs) have proven themselves as a sensible tool for bridging
fuzzy user provided input into discrete, machine readable formats. Prominent models in this field have demostrated
outstanding capabilities that enable computer scientists to tackle new problem classes, that used to be
challenging / yielded unsatisfying results with discrete programming approaches.

This thesis is exploring ways to overcome the above outlined barrier using natural language queries, so that domain experts,
business owners, support staff etc. are able to seamlessly interact with their data, essentially eliminating the
requirement of learning SQL (and its pitfalls). By translating natural language to SQL using Large Language Models
this translation becomes very robust (e.g. against different kinds of phrasing) and enables novel applications
in how businesses, researchers and professionals interact with their data — it represents a fundamental shift 
(ie. moving away from SQL) towards a more inclusive and data driven world. 


\subsection{Objectives of the Thesis}

This thesis aims to address the aforementioned challanges when it comes to database accessibility.
The following objectives are the core research area of this thesis:

\begin{enumerate}
    \item Develop a database extension that can translate natural language queries into semantically
          accurate SQL queries using Large Language Models.
    \item To evaluate the effectiveness and feasibility of different Models aswell as prompt engineering
          techniques in order to improve the performance of the system.
    \item Identify and address issues when it comes to handling amibguous, complex and domain specific user input.
    \item Benchmark the performance of the implementation against common natural language to SQL (NL2SQL) benchmarks.
    \item Idenitfy potential use cases for real world scenarios that could deliver a noticable upsides to users.
    \item Analyze the short commings and limitations of this approach and propose potential solutions to overcome them.
\end{enumerate}  

\subsection{Research Questions}

% \subsubsection{Primary Research Questions}

\subsubsection*{RQ1 — Are natural language database interfaces feasible for real world application?}

The primary research questions when it comes to natural language database interfaces evolve around their
semantic accuracy and reliability, therefore questioning their feasibility for real world usage.
LLMs have notoriously been known for their ability to hallucinate / produce false, but promising outputs.
This behaviour can be especially  dangerous when opting for data driven decisions that rely on false data
due to a mistranslation from natural language to SQL. LLMs could cause hard to understand and debug behaviour,
like false computation of distributions when the intermediate format is not being shown to the user. This
thesis tries to determine whether such hallucinations could be reasonably prevented and whether the associated
performance and hardware requirements are suitable for a real world deployment, outside of research situations.

Specifically the two big underlying questions are:

\begin{enumerate}
    \item Is the semantic accuracy of natural language database interfaces high enough to yield a noticable
          benefit to users?
    \item Is it possible to run such an interface on reasonable, mass available hardware (e.g. excluding high end research GPUs).
\end{enumerate}

\subsubsection*{RQ2 — What approaches are most effective in resolving ambiguity when translating natural language queries into SQL?}

To provide semantically correct results ambiguity in the user-provided natural language queries must be 
adequately addressed. This thesis investigates various approaches to ambiguity management and
resolution. Natural language queries can demonstrate ambiguity even at low levels of complexity —
e.g. there are two different types of "sales" in a database schema, and the user asks to retireve
"all sales".

Such situations present the second major challenge associated with the practical implementation of natural
language database interfaces. The success of this concept will significantly depend on whether suitable 
designs and mitigation techniques can be implemented without creating problems with regards to the 
aforementioned performance and hardware requirements. The research focus lies on both preventative measures
through optimized pre-processing stages and prompt engineering techniques as well as reactive strategies
that post process LLM output, either on the basis of further user input or context inference.

\subsubsection*{RQ3 — Which strategies are increasing semantic accuracy of queries?}

In order to enhance the semantic accuracy a series of improvements may be applied to the pipeline.
Potential optimizations include supplying (parts of) the schema during LLM prompting, implementation of
interactive contextual reasoning through a conversational interface which would allow for user
refinement, the implementation of a robust SQL parsing and validation mechanism and a hybrid approach
partly relying on traditional NLP preprocessing techniques. This research will quantify semantic accuracy
using popular NL2SQL benchmarks and empirically evaluate the impact each approach has on the benchmark
performance. Furthermore this research will take a look at the optimal combination of the aforementioned
solutions in order to develop a system that strikes the right balance between accuracy and performance.

% \subsubsection{Secondary Research Questions}

% \subsubsection*{RQ4 — What use cases are most suitable for natural language database interfaces?}

% \subsubsection*{RQ5 — Where are the limitations of natural language database interfaces?}

% \subsubsection*{RQ6 — How is NL2SQL model performance impacted through model selection, fine tuning and pre/post processing?}

\newpage

\subsection{Structure of the Thesis}

This thesis is following a research and development methodology in order to implement a natural language
interface for databases, in particular postgres is used.

\begin{enumerate}
    \item \textbf{Literature Review} — An analysis of the existing research in the fields of
          natural language interfaces (NLI) for databases, GPU integration for acceleration
          of database operations, and LLM/AI Model integration within database systems.
          This phase establishes the theoretical foundation for this research and identifies current 
          state-of-the-art approaches, their benefits and shortcomings.
     \item \textbf{Decomposition \& Requirements} — Decomposing the problem statement into its
          fundamentals and deriving system requirements for the design phase from it. The goal
          of this section is to arrive at a list of functional and non-functional requirements that
          must be taken into account and fulfilled by the design and implementation phases respectively.
     \item \textbf{System Design} — Design of a system architecture that can utilize GPU acceleration
          for LLM integration from within postgres. The primary goals of the system design phase
          are to arrive at an architecture that yields low latency natural language processing,
          schema-aware SQL query generation, ambiguity detection and resolution whilst maintaining
          a high semantic accuracy.
     \item \textbf{Implementation} — The implementation of a PostgreSQL extension according to the
          above system design that relies on \texttt{rust} and \texttt{pgrx}. This extension will
          provide a GPU accelerated framework for executing LLMs, implement a natural language
          to query generation pipeline that relies on the SQL schema and create database functions
          and operators for both query generation and execution.
     \item \textbf{Evaluation and Benchmarking} — An assesment framework and benchmark that introspects
          the implementations performance in multiple dimensions. Namely the most relevant dimensions
          for this thesis are:
          \begin{enumerate}
              \item Semantic Accuracy — Measuring the overall accuracy of results delivered for a given
                    natural language input.
              \item Ambiguity Resolution Capabilities — How well the system performs when confronted with
                    ambiguous natural language input and database schemas.
              \item Performance Metrics — Measuring the latency, throughput and resource utilization 
                    of the implementation.
          \end{enumerate}
      \item \textbf{Discussion} — Analysis and interpretation of the evaluation phase results against
            the research goals of this thesis. Evaluating the performance and accuracy results recorded
            during the benchmarks against the question whether real world deployments of NILs are feasible.
            Furthermore the effectiveness of ambiguity resolution capabilities and semantic accuracy enhancement
            strategies are showing a statistically significant effect.
       \item \textbf{Summary and Outlook} — Summarizes the contributions, addresses limitations
            of this thesis and the implementation, and proposes directions for future research alongside
            possible applications. Primary future research topics include advanced GPU optimization
            techniques (e.g. further quantization), accuracy and performance impact of model fine tuning,
            techniques, scalability of such a system in enterprise scenarios and the evaluation of security
            and privacy considerations (e.g. managing access control).
\end{enumerate}

\newpage

% Theoretical Foundations
\section{Literature Review}

In this section a comprehensive literature review is performed to asses the research landscape on NL2SQL
(sometimes also referred to as Text-to-SQL or T2SQL) and NLIDBs. Following their development starting in
the late 1990s and early 2000s \citep{NLIDBs, NLIDBTheory, ILPParsing, ILPParsing2} until now, observing multiple 
larger paradigm shifts happening over time \citep{GRAPPA, STRUG, Seq2SQL, NALIR, SQLizer}. In particular this 
research focuses on the recent advancements when it comes to language models and how they can be harnessed for effective NL2SQL systems \citep{LLM-Sql, T2SQL-LLM-Bench, T2SQL-LLM-Bench-2, T2SQL-LLM-Bench-3, SPIDER2, BIRD}.

This literature review is covering the foundational concepts, challenges, key advancements and research gaps
associated with using natural language instead of SQL. It lays the foundation for this thesis and helps to set
the research questions introduced in the previous chapter in context.

\subsection{Foundations of Natural Language Interfaces to Databases}

One of the first corner stone research papers on Natural Language Database Interfaces (NLIDBs) was published
over three decades ago by \citeauthor*{NLIDBs} where an introduction and an overview of the state-of-the-art
in the field were provided. \citep{NLIDBs} Their work outlined multiple key issues and challenges associated
with NLIDBs, and compared them against existing / competing solutions like formal query languages, form-based
interfaces and graphical interfaces. These challenges (like unobvious limits, linguistic ambiguities, semantic 
inaccuracy, tedious configuration etc.) have shaped this field of research and are still considered relevant
metrics today.

Early NLIDBs primarily relied on traditional natural language processing (NLP) techniques in order to achieve
natural language understanding capabilities. With \textsc{Chill} an inductive logic programming (ILP) approach
was first introduced for NL2SQL systems, marking one of the key events when it comes to machine learning usage.
\citep{ILPParsing} In \citeyear{ILPParsing2} \citeauthor*{ILPParsing2} have extended the approach of ILP
parsing for natural language database queries with multi clause construction, yielding promising results
in the field of NLIDBs. \citep{ILPParsing2}

Building on the systematic overview of \citeauthor*{NLIDBs} and the first machine learning approaches from
\citeauthor*{ILPParsing} aswell as \citeauthor*{ILPParsing2}, \citeauthor{NLIDBTheory} have proposed a novel 
approach for implementing NLIDBs and outperformed at the time state-of-the-art solutions from \cite{ILPParsing} 
\cite{ILPParsing2} — achieving 80\% semantic accuracy. \citep{NLIDBTheory} The novelty of the \textsc{Percise}
system lies in its natural language processing approach, specifically its lexical mapping strategy, allowing 
\textsc{Percise} to identify questions it can, and can't answer (introducing the concept of \textit{semantically 
tractable questions}) which therefore results in a better and interactive end user experience. Their experiements 
also showed that this approach is \textit{transferrable} and \textit{unbiased} — it is possible to feed in new, 
unknown questions into the system and maintain performance characteristics, whereas it was shown that 
\cite{ILPParsing} were suffering from a distribution drift of the questions asked. \citep{NLIDBTheory}

% Welche approaches fuer llm integration
% Welche approaches fuer natural to sql

\newpage

\section{Decomposition \& Requirements}

\subsection{Problem Decomposition}

\subsection{Requirements}

% Conceptual Design
\section{System Design}

\subsection{Architecture Design}
\subsubsection{Interface Design}
\subsubsection{Data Model}
\subsubsection{Integration into SQL}

\subsection{Technical Implementation Strategies}

\newpage

% Implementation
\section{Implementation}

\subsection{Development Environment and Tools}

\subsection{Integration of the Model}


\subsection{Development of the PostgreSQL Extension}


\subsection{Optimization}

\newpage

% Evaluation
\section{Evaluation}

% https://github.com/petavue/NL2SQL-Benchmark/blob/main/results/Report/NL2SQL%20Benchmark%20Report.pdf

\subsection{Test Environment and Methodology}

\subsection{Performance Tests}
\subsubsection{Latency}
\subsubsection{Throughput}
\subsubsection{Scalability}

\subsection{Use Cases}
\subsubsection{Natural Language Queries}
\subsubsection{Text Generation Within the Database}
\subsubsection{Semantic Search and Text Classification}

\subsection{Comparison with Alternative Approaches}

\newpage

% Discussion
\section{Discussion}

\subsection{Interpretation of Results}
\subsection{Limitations of the Implementation}
\subsection{Ethical and Data Privacy Considerations}
\subsection{Potential Future Developments}

\newpage

% Summary and Outlook
\section{Summary and Outlook}

\subsection{Summary of Results}
\subsection{Addressing the Research Questions}
\subsection{Outlook for Future Research and Development}

\newpage

\newpage

\appendix
\section*{Appendix}
\subsection*{Installation Guide}
\subsection*{API Documentation}
\subsection*{Code Examples}
\subsection*{Test Data and Results}

\nocite{*}

\bibliographystyle{apacite}
\bibliography{references}

\end{document}