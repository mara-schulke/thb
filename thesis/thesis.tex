\documentclass{article}

\usepackage[a4paper]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{float}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage[natbibapa]{apacite}

\graphicspath{{./images/}}

\pagestyle{fancyplain}
\fancyhf{}
\lhead{\fancyplain{}{Mara Schulke} }
\rhead{\fancyplain{}{\today}}
\cfoot{\fancyplain{}{\thepage}}

\newcommand{\figuresource}[1]{
	\begin{center}Quelle: {#1}\end{center}
}

\titleformat{\chapter}{\normalfont\Large\bfseries}{\thechapter.}{20pt}{\Large}
\titleformat{\section}{\normalfont\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\small\bfseries}{\thesubsubsection}{1em}{}

\begin{document}

\begin{titlepage}
    \begin{center}
        \begin{Large}
            Brandenburg University of Applied Sciences \\[1em]
        \end{Large}
        IT Security \\
        Computerscience \\
        Prof. Dr. Oleg Lobachev \\
        MSc. Florian Eich
    \end{center}

    \vfill

    \begin{center}
        \Large{Reliable Natural Language Interfaces using LLMs, Self-Correction and Incremental Schema Analysis}\\[0.5em]
        \large{Bachelor Thesis}\\[1em]
        
        \begin{normalsize}
            Summer semester 2025\\[0.25em]
            \today
        \end{normalsize}
    \end{center}

    \vfill

    \begin{center}
        Mara Schulke – Matr-Nr. 20215853
    \end{center}
\end{titlepage}

\begin{abstract}
This thesis explores the integration of large language models (LLMs) into PostgreSQL database systems in order
to make the database accessible via natural language instead of the postgres SQL dialect. The research focuses
on implementation strategies, performance optimization, and practical applications of this concept.
\end{abstract}

\tableofcontents

\listoffigures

\section*{List of Abbreviations}
\begin{tabular}{ll}
GPT & Generative Pretrained Transformer \\
SQL & Structured Query Language \\
API & Application Programming Interface \\
LLM & Large Language Model \\
DBMS & Database Management System \\
NL2SQL & Natural Language to SQL \\
\end{tabular}

\newpage

% 1/3 Vorarbeit, Literature Review
% 1/3 Theory, Decomp und System Design
% 1/3 Implementation und Evaluation

% Introduction
\section{Introduction}
\subsection{Problem Statement and Motivation}

Database systems represent a backbone of modern computer science, allowing for rapid advancements
whilst shielding us from the problem categories that come along with managing and querying large amounts
of, usually structured, data efficiently. However, most Database Management Systems (DBMS) have
traditionally required specialized knowledge, usually of the Structured Query Language (SQL), in order
to become useable. Whilst this barrier may be percieved differently across diverse usergroups it
represents a fundamental misalignment between end-user goals (e.g. analysts, researchers, domain experts
etc.) and the underlying DBMS, thus often requiring software engineering efforts in order to reduce this friction.

This barrier is the reason entire classes of software projects exists (for example, admin / support panels),
data analytics tools etc. which therefore introduce significant churn and delay between the implementation
of a database system and reaching the desired end user impact. Often these projects span multiple years, require
costly staffing and yield little to no novel technical value.

Emerging technologies such as Large Language Models (LLMs) have proven themselves as a sensible tool for bridging
fuzzy user provided input into discrete, machine readable formats. Prominent models in this field have demostrated
outstanding capabilities that enable computer scientists to tackle new problem classes, that used to be
challenging / yielded unsatisfying results with logical programming approaches.

This thesis is exploring ways to overcome the above outlined barrier using natural language queries, so that domain experts,
business owners, support staff etc. are able to seamlessly interact with their data, essentially eliminating the
requirement of learning SQL (and its pitfalls). By translating natural language to SQL using Large Language Models
this translation becomes very robust (e.g. against different kinds of phrasing) and enables novel applications
in how businesses, researchers and professionals interact with their data — it represents a fundamental shift 
(ie. moving away from SQL) towards a more inclusive and data driven world. 

\subsection{Objectives of the Thesis}

This thesis aims to address the aforementioned challanges when it comes to database accessibility.
The following objectives are the core research area of this thesis:

\begin{enumerate}
    \item Develop a database extension that can translate natural language queries into semantically
          accurate SQL queries using Large Language Models.
    \item To evaluate the effectiveness and feasibility of different Models aswell as prompt engineering
          techniques in order to improve the performance of the system.
    \item Identify and address issues when it comes to handling amibguous, complex and domain specific user input.
    \item Benchmark the performance of the implementation against common natural language to SQL (NL2SQL) benchmarks.
    \item Idenitfy potential use cases for real world scenarios that could deliver a noticable upsides to users.
    \item Analyze the short commings and limitations of this approach and propose potential solutions to overcome them.
\end{enumerate}  

\subsection{Research Questions}

% \subsubsection{Primary Research Questions}

\subsubsection*{RQ1 — Are natural language database interfaces feasible for real world application?}

The primary research questions when it comes to natural language database interfaces evolve around their
semantic accuracy and reliability, therefore questioning their feasibility for real world usage.
LLMs have notoriously been known for their ability to hallucinate / produce false, but promising outputs.
This behaviour can be especially  dangerous when opting for data driven decisions that rely on false data
due to a mistranslation from natural language to SQL. LLMs could cause hard to understand and debug behaviour,
like false computation of distributions when the intermediate format is not being shown to the user. This
thesis tries to determine whether such hallucinations could be reasonably prevented and whether the associated
performance and hardware requirements are suitable for a real world deployment, outside of research situations.

Specifically the two big underlying questions are:

\begin{enumerate}
    \item Is the semantic accuracy of natural language database interfaces high enough to yield a noticable
          benefit to users?
    \item Is it possible to run such an interface on reasonable, mass available hardware (e.g. excluding high end research GPUs).
\end{enumerate}

\subsubsection*{RQ2 — What approaches are most effective in resolving ambiguity when translating natural language queries into SQL?}

To provide semantically correct results ambiguity in the user-provided natural language queries must be 
adequately addressed. This thesis investigates various approaches to ambiguity management and
resolution. Natural language queries can demonstrate ambiguity even at low levels of complexity —
e.g. there are two different types of "sales" in a database schema, and the user asks to retireve
"all sales".

Such situations present the second major challenge associated with the practical implementation of natural
language database interfaces. The success of this concept will significantly depend on whether suitable 
designs and mitigation techniques can be implemented without creating problems with regards to the 
aforementioned performance and hardware requirements. The research focus lies on both preventative measures
through optimized pre-processing stages and prompt engineering techniques as well as reactive strategies
that post process LLM output, either on the basis of further user input or context inference.

\subsubsection*{RQ3 — Which strategies are increasing semantic accuracy of queries?}

In order to enhance the semantic accuracy a series of improvements may be applied to the pipeline.
Potential optimizations include supplying (parts of) the schema during LLM prompting, implementation of
interactive contextual reasoning through a conversational interface which would allow for user
refinement, the implementation of a robust SQL parsing and validation mechanism and a hybrid approach
partly relying on traditional NLP preprocessing techniques. This research will quantify semantic accuracy
using popular NL2SQL benchmarks and empirically evaluate the impact each approach has on the benchmark
performance. Furthermore this research will take a look at the optimal combination of the aforementioned
solutions in order to develop a system that strikes the right balance between accuracy and performance.

% \subsubsection{Secondary Research Questions}

% \subsubsection*{RQ4 — What use cases are most suitable for natural language database interfaces?}

% \subsubsection*{RQ5 — Where are the limitations of natural language database interfaces?}

% \subsubsection*{RQ6 — How is NL2SQL model performance impacted through model selection, fine tuning and pre/post processing?}

\newpage

\subsection{Structure of the Thesis}

This thesis is following a research and development methodology in order to implement a natural language
interface for databases, in particular postgres is used.

\begin{enumerate}
    \item \textbf{Literature Review} — An analysis of the existing research in the fields of
          natural language interfaces (NLI) for databases, GPU integration for acceleration
          of database operations, and LLM/AI Model integration within database systems.
          This phase establishes the theoretical foundation for this research and identifies current 
          state-of-the-art approaches, their benefits and shortcomings.
     \item \textbf{Decomposition \& Requirements} — Decomposing the problem statement into its
          fundamentals and deriving system requirements for the design phase from it. The goal
          of this section is to arrive at a list of functional and non-functional requirements that
          must be taken into account and fulfilled by the design and implementation phases respectively.
     \item \textbf{System Design} — Design of a system architecture that can utilize GPU acceleration
          for LLM integration from within postgres. The primary goals of the system design phase
          are to arrive at an architecture that yields low latency natural language processing,
          schema-aware SQL query generation, ambiguity detection and resolution whilst maintaining
          a high semantic accuracy.
     \item \textbf{Implementation} — The implementation of a PostgreSQL extension according to the
          above system design that relies on \texttt{rust} and \texttt{pgrx}. This extension will
          provide a GPU accelerated framework for executing LLMs, implement a natural language
          to query generation pipeline that relies on the SQL schema and create database functions
          and operators for both query generation and execution.
     \item \textbf{Evaluation and Benchmarking} — An assesment framework and benchmark that introspects
          the implementations performance in multiple dimensions. Namely the most relevant dimensions
          for this thesis are:
          \begin{enumerate}
              \item Semantic Accuracy — Measuring the overall accuracy of results delivered for a given
                    natural language input.
              \item Ambiguity Resolution Capabilities — How well the system performs when confronted with
                    ambiguous natural language input and database schemas.
              \item Performance Metrics — Measuring the latency, throughput and resource utilization 
                    of the implementation.
          \end{enumerate}
      \item \textbf{Discussion} — Analysis and interpretation of the evaluation phase results against
            the research goals of this thesis. Evaluating the performance and accuracy results recorded
            during the benchmarks against the question whether real world deployments of NILs are feasible.
            Furthermore the effectiveness of ambiguity resolution capabilities and semantic accuracy enhancement
            strategies are showing a statistically significant effect.
       \item \textbf{Summary and Outlook} — Summarizes the contributions, addresses limitations
            of this thesis and the implementation, and proposes directions for future research alongside
            possible applications. Primary future research topics include advanced GPU optimization
            techniques (e.g. further quantization), accuracy and performance impact of model fine tuning,
            techniques, scalability of such a system in enterprise scenarios and the evaluation of security
            and privacy considerations (e.g. managing access control).
\end{enumerate}

\newpage

\section{Literature Review}

% https://github.com/HKUSTDial/NL2SQL_Handbook/blob/main/assets/river.svg

In this section a comprehensive literature review is performed to asses the research landscape on NL2SQL
(sometimes also referred to as Text-to-SQL or T2SQL) and NLIDBs. From the time their development accelerated in
the late 1990s and early 2000s \citep{NLIDBs, NLIDBTheory, ILPParsing, ILPParsing2} until now, observing multiple 
larger paradigm shifts happening over time \citep{GRAPPA, STRUG, Seq2SQL, NALIR, SQLizer}. In particular this 
research focuses on the recent advancements when it comes to language models and how they can be harnessed for
effective NL2SQL systems \citep{LLM-Sql, DAIL-SQL, T2SQL-LLM-Bench-2, T2SQL-LLM-Bench-3, SPIDER2, BIRD}.

This literature review is covering the foundational concepts, challenges, key advancements and research gaps
associated with using natural language instead of SQL. It lays the foundation for this thesis and helps to set
the research questions introduced in the previous chapter in context.

% bessere einleitung

\subsection{Foundations of Natural Language Interfaces to Databases}

Earlier papers in the research landscape on Natural Language Database Interfaces (NLIDBs) date over half
a century back, into the early 1970s. Two decades after the first major research systems where developed
in this domain, \citeauthor*{NLIDBs} have published an introduction and an overview over NLIDBs where an overview of
state-of-the-art approaches were provided. \citep{NLIDBs} Their work outlined multiple key issues and challenges
associated with NLIDBs, and compared them against existing / competing solutions like formal query languages,
form-based interfaces and graphical interfaces. These challenges (like unobvious limits, linguistic ambiguities,
semantic inaccuracy, tedious configuration etc.) have shaped this field of research and are still considered relevant
metrics today.

Early NLIDBs primarily relied on traditional natural language processing (NLP) techniques in order to achieve
natural language understanding capabilities. With \textsc{Chill} an inductive logic programming (ILP) approach
was first introduced for NL2SQL systems, marking one of the key events when it comes to machine learning usage.
\citep{ILPParsing} In \citeyear{ILPParsing2} \citeauthor*{ILPParsing2} have extended the approach of ILP
parsing for natural language database queries with multi clause construction, yielding promising results
in the field of NLIDBs. \citep{ILPParsing2}

Building on the systematic overview of \citeauthor*{NLIDBs} and the first machine learning approaches from
\citeauthor*{ILPParsing} aswell as \citeauthor*{ILPParsing2}, \citeauthor{NLIDBTheory} have proposed a novel 
approach for implementing NLIDBs and outperformed at the time state-of-the-art solutions from \cite{ILPParsing} 
\cite{ILPParsing2} — achieving 80\% semantic accuracy. \citep{NLIDBTheory} The novelty of the \textsc{Percise}
system lies in its natural language processing approach, specifically its lexical mapping strategy, allowing 
\textsc{Percise} to identify questions it can, and can't answer (introducing the concept of \textit{semantically 
tractable questions}) which therefore results in a better and interactive end user experience. Their experiements 
also showed that this approach is \textit{transferrable} and \textit{unbiased} — it is possible to feed in new, 
unknown questions into the system and maintain performance characteristics, whereas it was shown that 
\cite{ILPParsing} were suffering from a distribution drift of the questions asked. \citep{NLIDBTheory}

The theoretical foundations and research questions highlighted by the aforementioned works, shaped the research
field and highlighted the following, ongoing research:

\begin{enumerate}
    \item The trade-off characteristics derived from choosing a machine learning vs. traditional NLP approach (e.g. 
          \textsc{Chill} versus \textsc{Percise}). E.g. coverage versus correctness. \citep{ILPParsing, NLIDBTheory}
    \item The linguistic challenges associated with bringing NLIDBs into use (e.g. semantic inaccuracy, linguistic 
          ambiguity, unclear language coverage etc.) \citep{NLIDBs}
    \item The value of systems and approaches which double down on reliability and semantic accuracy rather than giving
          promising but incorrect answers. \citep{NLIDBs, NLIDBTheory}
\end{enumerate}

Fundamentally this highlights the tension and mismatch between the characteristics of natural language, which is 
able to be ambiguous, \textit{semantically untractable} or able to be incomplete in meaning and formal languages
like SQL which always have on deterministic and \textit{semantically tractable} meaning they convey in each statement. 
As Schneiderman and Norman have pointed out according to \citeauthor*{NLIDBTheory}, users are ``unwilling to trade 
reliable and predictable user interfaces for intelligent but unreliable ones'' which induces performance expectations
on NLIDB implementations to be highly certain about the questions it can, and can't answer, whilst maintaing as high
as possible natural language coverage. \citep{NLIDBTheory}

\subsection{Traditional NL2SQL Approaches}

Prior to the wide-spread dominance of machine learning approaches for natural language processing a variety of
traditional, rather discrtete approaches have been explored in the field of NL2SQL / NLIDBs. These logical
programming approaches have laid the foundations for transitioning towards the application of machine learning
techniques for NL2SQL.

\subsubsection{Rule-based and Grammar-based Systems}

Foundational research of NL2SQL system mostly focused around applying rule engines that were tedious to set up and
expensive to maintain / transfer across database systems. These rule engines mostly relied on the systematic 
identification of linguistic patterns / were trying to template SQL from information that was derived from processing the 
natural language query. \citep{Rendezvous, Lunar, Ladder} These approaches mostly tried to formalize natural language 
queries into formal grammers which could then be deterministically mapped into a valid SQL query. \citep{Lunar} These 
approaches have strong downsides when it comes to the variety of natural language constructs they can process, aswell as 
runtime adoption of new / unknown databases, query constructs etc. A potential upside of this class of NL2SQL systems is 
that they can confidently and reproducably identify questions they can, and can't answer — thus leading to very reliable 
and predictable user interfaces.

\subsubsection{Semantic Parsing using String-Kernels}

A significiant milestone in parsing techniques of natural language queries was reached by \citeauthor*{StringKernels} in 
\citeyear{StringKernels}. The introduction of string kernels for semantic parsing represented a novel achievement, when 
it comes to fusing logical programming approaches using a formal grammer like LSNLIS developed by \cite{Lunar} and 
learning / training approaches to understand unseen language patterns / unknown natural language query structures. This 
allowed for more flexible pattern recognition when compared to traditional rule-based systems.

The core innovative characteristic of this approach lies in its capability to understand similarities between natural 
language expressions based on subsequence patterns rather than relying on exact matches. This made \textsc{Krisp}, the 
research NLIDB system developed by \cite{StringKernels} much more robust to language variations in phrasing and noise 
(e.g. spelling mistakes) in the input. As the \citeauthor*{StringKernels} demonstrated through experiments on real-world 
datasets, this approach compared favorably to existing systems of the time like \textsc{Chill}, especially in handling 
noisy inputs — a frequent challenge rigid rule-based systems faced in real world scenarios \citep{StringKernels, 
ILPParsing}.

\subsubsection{Graph Matching Methods}

\cite{GraphMatching} brought together several research threads and reapplied emerging graph matching research models
to natural language processing, specifically to natural language queries. Graph matching was applied once the natural 
language query was parsed using a Combinatory Categorial Grammar (CCG) approach into a semantic graph which denotes the 
relationship between semantic entities in it. This graph could then be matched against the actual graph derived from 
the database, since they share topological traits that can be used for matching \citep{GraphMatching}. This approach 
allowed to apply querying systems without having any question-answer pairs or manual annotations for training the system, 
which implies easier scalability / transferability across domains, since the system does not require any additional 
tweaks.

Even though this approach was novel and showed improved the performance over existing state-of-the-art approaches, it
was showing that graph matching quickly reaches its limitations. This approach relied heavily on the CCG parser's 
accuracy, with parsing errors accounting for 10-25\% (depending on the dataset) of system failures \citep{GraphMatching}. 
Furthermore it struggled with both ambiguous language constructs and potential mismatches between natural language 
representation of relationships and database layouts — more complicated database designs, which may not match the users 
intuitive understanding resulted in a different topology and hence could not be matched \citep[p.~387]{GraphMatching}.

\subsubsection{Interactive Systems}

In \citeyear{NALIR} \citeauthor*{NALIR} identified that perfect translation of natural language into SQL was challenging
due to natural language not being made for query expressions as it heavily relies on contextual information and clarifying
questions in order to disambiguate conversations \citep{NALIR}. These learnings relate to early prior art from 
\citeauthor{UnnaturalQueryLanguage} and \citeauthor{Rendezvous} which also made this observation — ``natural language
is not a natural query language.'' \citep{UnnaturalQueryLanguage}. The solution introduced by NaLIR further emphasized
how important an interactive, conversational usage model is, when offering a natural language interface \citep{NALIR}.

NaLIR could accept logically complex English language sentences as input and translate them into SQL queries with various 
complexities, including aggregation, nesting, and different types of joins etc. The key innovative characteristic of 
NaLIR lies in its interactive communication mechanism (much like \textsc{Rendezvous}) that could detect potential 
misinterpretations and engage users to resolve ambiguities present in their natural language query without forcing them
to entirely rephrase their query \cite{NALIR}. This approach, while showing awareness for its limitations (with regards
to entirely automating / deriving SQL generation from potentially ambiguous or faulty user input) showed that it was 
possible to overcome these limtations through choosing the right interaction model — ``In our system, we generate
multiple possible interpretations for a natural language query and translate all of them in natural language for the
user to choose from'' —, rather than optimizing the generation part of the system \cite{NALIR}.

\subsubsection{Query Synthesis}

\cite{SQLizer} introduced SQLizer, which synthesizes SQL queries from natural language \citep{SQLizer}. This paper
presents a novel approach when it comes to NL2SQL as it is merging prevalent semantic parsing techniques (outlined above)
with an program synthesis (or query synthesis) approach. SQLizer makes use of a three stage processing model for 
natural language models: first generating a sketch of the query using semantic parsing, then using type-directed
synthesis to complete the sketch and finally using automated repair, if required. 

\citeauthor*{SQLizer} show that alternating between repairing and synthesis yields results that beat state-of-the-art
NL2SQL approaches like NaLIR. SQLizer is fully automated and database-agnostic, requiring no knowledge of the underlying
schema. The authors evaluated SQLizer on 455 queries across three databases, where it ranked the correct query in the
top 5 results for roughly 90\% of the queries. This represents a significant improvement over NaLIR \citep{NALIR},
the previous state-of-the-art system \citep{SQLizer}.

Potential short commings of this approch include queries which yield empty results, dealing with language variations
as SQLizer is still using semantic parsing, and domain-specific terminology, all while still requiring users
to select from multiple query options which reduces the overall usability of the system \citep[p.22-23]{SQLizer}.

\subsubsection{Limitations of Traditional Approaches to NL2SQL}

Despite being innovative and achieving state-of-the-art results, many of the above outlined approaches face severe 
challenges when moving outside of an research environment. Many of these systems performed comparatively good on research
benchmarks that were often composed of controlled question types and limited data variety. Ultimatively no standard
benchmark existed for NL2SQL in this era, hence comparing different NL2SQL systems against each other is a problem on 
its own. Despite not having a standard benchmark that all approaches could be unifiably evaluated against, several 
fundamental challenges emerged / remained with these approaches:

\begin{enumerate}
    \item \textbf{Limited linguistic coverage} — Prevalent rule-based and semantic-parsing based systems were only able to 
          process the a small subset of the natural language they were programmed for. This severely limited their 
          ability to handle different phrasings of the same end-user goal \citep{StringKernels, UnnaturalQueryLanguage, 
          Lunar, Ladder}.
    \item \textbf{Transferability} — Traditional approaches typically required extensive manual configuration or at least
          a training phase / adaption for each database they were deployed for, hindering cross domain usage through being 
          expensive and time-consuming to adapt \citep{NLIDBs, Lunar}.
    \item \textbf{Brittleness} — Many of the systems introduced in this subchapter did not handle synonyms, paraphrasing,   
          or spelling errors well. Manual adaption / handling was needed in order to becomes resilient against each class
          of problems \citep{StringKernels, SQLizer}.
    \item \textbf{Poor scalability} — With potentially more complex underlying databases, traditional 
          solutions often showed to perform worse. \citeauthor*{GraphMatching} found, that with increasing schema
          complexity more compute was required to resolve the natural language query to a suitable query candidate
          making them less transferable and scalable than initially anticipated \citep{GraphMatching} — ``Evaluating on
          all domains in Freebase would generate a very large number of queries for which denotations would have to be 
          computed ... Our system loads Freebase using Virtuoso and queries it with SPARQL. Virtuoso is slow in dealing with
          millions of queries indexed on the entire Freebase, and is the only reason we did not work with the complete
          Freebase.'' which indicates underlying system design issues with runtime complexity.
\end{enumerate}

These flaws of traditional NL2SQL approaches made it apparent, that a different class of approaches is needed, which 
increase transferability and reduce the brittleness since users are ``unwilling to trade reliable and predictable user 
interfaces for intelligent but unreliable ones'' according to \cite{NLIDBTheory}. Whilst many approaches outlined tractable 
ways to increase user satisfaction and accuracy (like \citeauthor*{Rendezvous} did in \citeyear{Rendezvous} with a 
conversational approach), NLIDBs were and are not considered to be a solved problem.

\subsection{Neural NL2SQL Approaches}

The previously outlined limitations of traditional approaches to solving NL2SQL / implementing NLIDBs pushed the research
branch around neural network application forward to step in and propose new solutions which address the brittleness, 
transferability and scalability concerns addressed with logical programming approaches. Neural approaches showed to yield
significant improvements in terms of transferability and overall accuracy which led to a paradigm shift in this research
field.

\subsubsection{Early Neural Approaches}

In \citeyear{Seq2SQL} \citeauthor*{Seq2SQL} released Seq2SQL which represents a significant breakthrough and leap in NLIDB 
research. Seq2SQL was an early research system that in the field of neural network application and as one of the first papers
to frame the implementation of NLIDBs / NL2SQL Systems as a reinforement learning problem. The system utilized iterative query 
execution in the reward function to improve its accuracy \citep{Seq2SQL}. In the same paper \citeauthor*{Seq2SQL} introduced 
WikiSQL, a training dataset, which enables large scale (in \citeyear{Seq2SQL}) model training.

SQLNet \citep{SQLNet} addressed primarily the order-sensitivitiy trait of Seq2SQL \citep{Seq2SQL} that was prevalent due to being
a derivative approach from sequence-to-sequence approaches. SQLNet diverges from sequence-to-sequence and joins multiple research 
threads, employing a sketch-based query generation. SQLNet breaks down complex queries into smaller (hence more manageable)
sub-queries which can then be individually sketched and refined, yielding a system that outperformed state-of-the-art by 9\% 
to 13\% \citep{SQLNet}.

\citeauthor*{TypeSQL} have introduced \textsc{TypeSQL}, a variation of the SQLNet-approach, in \citeyear{TypeSQL}.
\textsc{TypeSQL}'s primary difference to SQLNet is the encoding of type information for SQL generation. The approach scanned
for entity references and values in natural language and was able to improve performance by 5.5\% over SOTA-Models like
SQLNet whilst requiring significantly less training time, indicating that type information was a useful information for deriving 
accurate SQL queries from user input \citep{TypeSQL}.

\subsubsection{Intermediate Neural Developments}

Later in \citeyear{SyntaxSQLNet} \citeauthor*{SyntaxSQLNet} released SyntaxSQLNet, a followup research to \textsc{TypeSQL},
which represented a slight change in approach and research focus. In direct comparison SyntaxSQLNet focused primarily around
complex query generation using a syntax tree decoder, allowing for longer and more cohesive query generation \citep{SyntaxSQLNet}.
This advancement over \textsc{TypeSQL} allowed more complex queries to be reliably generated, enabling multiple clauses aswell as
nested queries. SyntaxSQLNet was one of the earlier research efforts which utilized Spider instead of WikiSQL (introduced by 
\cite{Seq2SQL}), a large-scale NL2SQL dataset, incorperating 10.181 hand annotated natural language question and alongside
5.693 unique SQL examples that spread across 138 different domains \citep{Spider}. This research led the transition of
comparatively simple, research-grade, neural systems for NLIDBs towards systems which are feasible in the real world.

Building on the above approaches, \citeauthor*{IRNet} have introduced IRNet, a neural network approach using intermediate
representation as a bridge between natural language and SQL in which semantic queries could be expressed. The intermediate
format SemQL (or semantic query language) was utilized to transform and synthesize queries on the actual database schema more
accurately than Seq2SQL. IRNet followed a three phase approach: schema linking between the natural language query and database
layout, synthesis of SemQL as intermediate representation and deterministic conversion of SemQL to SQL. This approach allowed
IRNet to outperform state-of-the-art approaches on the \textsc{Spider} benchmark by 19.5\%, placing IRNet at an overall accuracy
of 46.7\% \citep{IRNet}.

Following IRNet, graph neural networks (GNN) have been explored as alternative architecture by \cite{GNN}, representing the
database schema as a graph and using message passing to model relationships between tables, columns and natural language input.
This approach demonstrated the capability to improve reasoning and query generation capability. \citeauthor{GNN} showed that when 
evaluating against the \textsc{Spider} benchmark GNN outperforms both SyntaxSQLNet (and therefore \textsc{SQLNet} \&
\textsc{TypeSQL}). Although presenting a signficiant advancement over previous state-of-the-art approaches, GNN falls behind in
performance against IRNet by 6\% \citep{IRNet, GNN}.

\subsubsection{Relation-Aware Transformer Approaches}

The release of RAT-SQL (Relation-Aware Transformer for SQL) \cite{RATSQL} represents the most significant leap in research of
neural NL2SQL approaches. RAT-SQL diverged from earlier research through emphasizing the relationship between natural language
and the database schema elements using relation-aware self-attention representing a novel approach for solving \textit{schema 
linking} \citep{RATSQL}.

RAT-SQL's primary innovations was the ability to infer, understand and utilize the relationship between individual tokens in the 
natural language query and link it to the database schema. Thus allowing for reasoning capabilities on the actual database schema
while generating the query.

This architecture yielded a 57.2\% in exact match accuracy when being evaluated on the \textsc{Spider} benchmark, substantially
outperforming comparative approaches like GNN, IRNet and IRNet V2 by 10.5\%, 9.8\% and 8.7\% respectively. Although overall
accuracy improved across all approaches when being paired with BERT (Bidirectional Encoder Representations from Transformers, a 
popular pre-trained lanugage model from Google) the $\delta$ between the indivudual approaches remained relatively steady, leaving
RAT-SQL outperforming state-of-the-art approaches by 5\% to 12.2\% further demonstrating the capability advancement yielded by this 
system \citep{RATSQL}.

\subsubsection{Comparative Analysis of Neural Approaches}

The evolution from early neural approaches to RAT-SQL emphasized the rapid advancements that happened in the research field of 
neural NL2SQL approaches in different dimensions:

\begin{enumerate}
    \item \textbf{Model Complexity} — Given the research progression from early sequence to sequence translation approaches 
          (\textsc{Seq2SQL}) towards sketch based and type augumented and graph based approaches (\textsc{TypeSQL, SQLNet, GNN})
          and syntax tree decoding emphasized by \textsc{SyntaxSQLNet}, neural approaches continously advanced in the complexity of 
          approaches that is required to beat state-of-the-art approaches in contemprorary benchmarks like \textsc{Spider}. RAT-SQL 
          presents one of the late and most complex advancements in the field of neural NL2SQL approaches with its adapted self 
          attention mechanism \citep{Seq2SQL, TypeSQL, GNN, SyntaxSQLNet, RATSQL}.
    \item \textbf{Transferability} — Each of the approaches introduced above represents a succession in terms of their transferability.
          The field of neural NL2SQL approaches significantly improved the ability for NLIDBs to generalize over the underlying database
          schemas. RAT-SQL showed the strongest cross-domain accuracy (that is benchmarked by the \textsc{Spider} benchmark). With
          standard benchmarks emerging it became easier to verify and quantify which approach had the highest transferability as
          \textsc{Spider} specifically had independent development and test datasets, preventing approaches from over-optimizing on
          training data \citep{Spider, RATSQL}.
    \item \textbf{Robustness} — As research systems advanced in complexity and shifted from raw input to output translation (Seq2SQL)
          their robustness steadily increased. Through more approaches like \textsc{SyntaxSQLNet} which utilized structured decoding,
          \textsc{IRNet} which relied on an intermediate representation and \textsc{RAT-SQL} the challenges around \textit{schema linking}
          outlined by \cite{RATSQL} have increasilingly led to more robust systems that can handle rephrasings, spelling mistakes and
          variations in natural language usage far beyond what traditional NL2SQL approaches could accomplish \cite{SyntaxSQLNet, IRNet, RATSQL}.
    \item \textbf{Query Complexity} — The performance on complex queries involving multiple tables, relying on complex aggregations,
          nested structures and joins dramatically improved over the course of the research that happened in this field. Whilst 
          \textsc{IRNet} reresents one of the first singificant advancements when it comes to the ability of neural approaches to handle
          complex queries, \textsc{RAT-SQL} still showed to outperform the intermediate representation approach introduced by \textsc{IRNet}
          by up to 10.5\% \citep{IRNet, RATSQL}.
    \item \textbf{Schema Understanding} — Whilst early approaches like Seq2SQL primarily applied reinforcement learning for end to end
          query generation \citep{Seq2SQL}, later approaches like \textsc{TypeSQL, GNN} and specifically \textsc{RAT-SQL} showed novel and state-of-
          the-art \textit{schema understanding / schema linking} capabilities, yielding the ability to accurately reason about user intent and
          traverse the database schema while generating queries \citep{TypeSQL, GNN, RATSQL}. 
\end{enumerate}

\subsubsection{Limitations of Neural Approaches}

Despite the dramatic \textit{accuracy, transferability} and \textit{robustness} improvements that could be observed with late neural
approaches \citep{IRNet, RATSQL}, neural approaches still suffered from serious shortcommings / unsolved challenges:

\begin{enumerate}
    \item \textbf{Training Data} — Utilizing neural networks these approaches required susbtantially more training data (ie. natural language
          paired with output SQL queries) than traditional systems which required serious efforts of data collection \citep{Spider}.
    \item \textbf{Correctness} — The inherent mismatch between neural networks and formal languages yielded cases where models produced
          invalid SQL code. Approaches like \textsc{SyntaxSQLNet} improved the tried to solve this circumstance by utilizing syntax trees
          during decoding but nonetheless syntactic correctness remained a challenge across future iterations of neural systems. \citep{SyntaxSQLNet}
    \item \textbf{Domain Language} — Despite increased \textit{transferability} characteristics neural approaches still suffered from a
          limited vocabulary and inter-domain understanding of terminology and relation between concepts which made highly domain
          specific natural language queries challenging.
    \item \textbf{Observability} — The black-box nature of neural networks made approaches relying on them, particularly the ones with complex
          architectures, hard do understand / explain in case when neural systems yielded undesirable output. 
\end{enumerate}


The introduction and advancement of early neural NL2SQL approaches led to significant advancements in the research and feasibility of
NLIDBs. The research shift started in this era established the foundations for further and more advanced machine learning approaches
(specifically language model oriented approaches )being researched. Neural approaches showed to significant improvements in performance
when being paired with pre-trained language models \citep{RATSQL} which led to further research on their applicability.

\subsection{Pre-trained Language Models}

The advantages of combining specialized neural networks with general-purpose pre-trained language models
led to a pivotal point in the NL2SQL research field towards focusing increasingly on the application of pre-trained
language models for NLIDBs. Models like BERT or T5 offer noticable performance improvements (especially when it
comes to language understanding) over specialized NL2SQL networks due to training happening on unrestrained amounts
of natural language data, instead of pure NL2SQL datasets which are often fairly limited in size and therfore natural
language use — \textsc{Spider2.0} which is a contemprary NL2SQL benchmark consists of just 632 real-world questions 
\citep{SPIDER2}. Thus PLM-based (or at least augumented) NL2SQL systems can observe dramatic performance improvements
through the language models's ability to understand patterns and identify semantic relationships of natural language
query elements. 

\subsubsection{Early Pre-trained Language Model Adaptations}

The above outlined benefits have led to concrete research efforts focusing on the question whether the sole application
of pre-trained language models could outperform neural state-of-the-art approaches — which often implicitly require
a far more sophisticate architecture when it comes to natural language analysis.

In the time of emerging PLM applicationm \textsc{Grappa} was introduced by \citeauthor*{GRAPPA} in \citeyear{GRAPPA} —
a novel grammar-augmented pre-training approach built on RoBERTa\textsubscript{\tiny{LARGE}} (a derivative model from BERT).
It generates synthetic training data (ie. natural language and sql pairs) using a synchronous context-free grammar (SCFG)
which analyses and identifies patterns in natural language queries that can be used as templates for sythesizing training
data. The specialized pre-training helps \textsc{Grappa} to establish a robust connection between natural-language and
database schema elements, showing significant improvements on existing approaches on multiple contemporary benchmarks like 
\textsc{Spider} and \textsc{WikiSQL} \citep{GRAPPA}.

Several NL2SQL approaches in this era focused on \textit{schema understanding} and \textit{schema linking} — the 
generalizability of PLMs required advanced techniques on ensuring that models both understand the semantic intent
of users when querying and correctly identify database schema elements in natural language queries. Thus improving
semantic accuracy of generated SQL queries. \textsc{StruG} (Structure-Grounded-Pretraining) was introduced in
\citeyear{STRUG} by \citeauthor*{STRUG} and presented a novel pretraining approach that improves model abilities when it comes to
\textit{schema linking}, it separates the problem in three facets: column grounding, value grounding and
column-value mapping. In direct comparison with \textsc{Grappa}, \textsc{StruG} achieves similar performance
while being significantly cheaper to train \citep{STRUG}.

In parallel, \citeauthor*{GAZP} released GAZP (Grounded Adaptation for Zero-shot Executable Semantic Parsing) in
\citeyear{GAZP}. \citeauthor*{GAZP} specifically addressed the challenge of adapting semantic parsers across databases
/ domains which was a apparent problem with neural approaches which had a strong tendency to overfit on benchmark datasets. 
Its novel contribution was the combination of forward semantic parsing with a backward utterance generator which allowed
for data synthesis in unseen environments which could then be used to adapt the semantic parser \citep{GAZP}. This
approach enables a improvement in robustness and accuracy in situations where training and inference environments
differ without requiring manually annotated examples \citep{GAZP}.

\subsubsection{Advanced Pre-trained Language Model Approaches}

Building on earlier foundational research on PLM application for NL2SQL tasks, researchers have developed increasingly complex
systems that leveraged pre-trained language models whilst addressing their limitations when it comes to generating valid SQL.

\citeauthor*{RYANSQL} introduced \textsc{Ryansql} (Recursively Yielding Annotation Network for SQL) in \citeyear{RYANSQL},
which implements a sketch-based approach for decomposing complex SQL generation into multiple smaller problems. \textsc{Ryansql}
transformed nested statements into a set of top-level statements using the Statement Position Code (SPC) technique. This
flattening of structure allowed \textsc{Ryansql} to limit the complexity of the query generation problem whilst maintaining
its ability to answer complex questions by recomposing complex queries from their parts. This approach allowed \textsc{Ryansql}
to achieve 58.2\% accuracy on the \textsc{Spider} benchmark, representing a 3.2\% improvement over contemporary state-of-the-art
approaches at the time \citep{RYANSQL}. The sketch-based approach makes \textsc{Ryansql} a PLM-augumented successor of \textsc{SQLNet}
which was a early neural approach to employ sketch-based query generation \citep{RYANSQL, SQLNet}.

A significant advancement in terms of execution accuracy was reached with the application of T5-Models for NL2SQL tasks. T5
(Text-to-Text Transfer Transformer) Models have proven themselves as well-suited for for query generation — T5-3B for NL2SQL
yielded 71.4\% execution accuracy and thus presented a breakthrough in this domain of research \citep{T2SQL-LLM-Bench-3}.
This established a new baseline for PLM-based approaches and demonstrated that general-purpose language models could not
only compete but outperform specialized architectures by far when properly fine-tuned \citep{T2SQL-LLM-Bench-3}.

Following the adavancements through T5, \citeauthor*{GRAPHIX} introduced GRAPHIX-T5 in \citeyear{GRAPHIX}, which combined
the T5 PLMs with a further graph-aware layers for NL2SQL tasks. This architecture could leverage both pre-trained knowledge
of T5 models aswell as the database schema structure during inference. GRAPHIX-T5 constructs a schema graph where nodes
represent tables and columns and edges represent relationships between them, such as foreign keys or columns association.
This architecture allows the model to deeply understand relationships and the layout of the database schema \citep{GRAPHIX}. 
GRAPHIX-T5 outperformed standard T5 models significantly, with GRAPHIX-T5\textsubscript{\tiny{LARGE}} showing 6.6\% increase
in execution accuracy over T5\textsubscript{\tiny{LARGE}}. When both GRAPHIX and the baseline T5 models were combined with
\textsc{Picard} (a novel constrained decoding mechanism) absolute $\delta$ between them jumped to 7.6\% (81.0\% in absolute
numbers), evaluated on \textsc{Spider-Dev} \citep{GRAPHIX}.

In parallel, \citeauthor*{RESDSQL} proposed \textsc{Resdsql} in \citeyear{RESDSQL}, which proposed to decouple
\textit{schema linking} and \textit{skeleton parsing}. This addressed the typical challenges sequence-to-sequence models
faced when simultaneously trying to link both schema elements and generate the query skeleton (e.g. 
\texttt{SELECT <columns> FROM <table>}). \textsc{Resdsql} further employed a ranking approach to filter schema elements
before passing them to the model for query generation, which reduced noise (when working with large database schemas)
and enabled passing only the most relevant parts. This twofold approach allowed \textsc{Resdsql} to achieve state-of-the-art
performance when being evaluated on \textsc{Spider}, outperforming GRAPHIX-T5\textsubscript{\tiny{3B}}-PICARD by 0.8\% in
execution accuracy. When combined with \textsc{NatSQL} (a contemporary intermediate representation approach introduced
by \cite{NATSQL}) absolute improvement over GRAPHIX-T5\textsubscript{\tiny{3B}}-PICARD jumped to 3.1\% emphasizing the
robustness gain decoupeled architectures have over model-oriented approaches.

These advancement showed rapid improvements over earlier methods — far surpassing neural approaches — through advanced
mechanisms when it comes to schema understanding and query generation. The wide language understanding inherited from
PLM-basemodels further strengthens robustness and shows effectiveness through a large gain on the \textsc{Spider}
benchmark. Collectively these approaches represent a leap in NL2SQL research, emphasizing their usability potential
and real-world feasibility. This era primed the research field for the transition towards large language model adoption.

\subsubsection{Constrained Decoding and Ranking Techniques}

A major challenge in NL2SQL research is making sure that model generated queries are not just semantically accurate
but also syntactically valid queries and thus executable. To address this issue \citeauthor*{PICARD} released \textsc{Picard}
(Parsing Incrementally for Constrained Auto-Regressive Decoding) in \citeyear{PICARD}, a constrained decoding mechanism
for language models which utilizes the SQL grammar and constrained decoding mechanisms to incrementally parse the generate SQL,
rejecting invalid tokens based on the grammar. \textsc{Picard} showed to significantly improve the performance of pre-trained
language models (like T5 or BERT) when it comes to NL2SQL tasks, lifting them from mid-level to state-of-the-art solutions
on the \textsc{Spider} benchmark \citep{PICARD}.

\textsc{Picard} operates as a incremental parser during model output decoding of pre-trained language models and continuously
evaluates the probability of each token. Instead of just passing model outputs to a database for execution \textsc{Picard}
incrementally parses and validates the generated SQL, rejecting tokens if neeeded thus significantly improving the valid
output accuracy (sometimes referred to as VA) of language models. This approach is addressing a significant issue associated
with pre-trained language models — while they outperform in natural language understanding and reasoning, they often lack
SQL grammer knowledge and tend to generate queries that are not executable due to ther unconstrained output space \citep{PICARD}.

The above introduces RESDSQL built ontop of \textsc{Picard}'s foundations and used a ranking-enhanced framework for input
encoding. These two approaches represent a unqiue class of approaches that utilize input and output constraining in order
to increase the performance characteristics of pre-trained language models \citep{RESDSQL}.

\subsubsection{Advantages of PLM Approaches}

PLM approaches to NL2SQL tasks have yielded significant performance improvements for the NL2SQL domain and represent a
leap in NLIDB-research. They primed the research field towards using language models which led to a transition towards
large language models in the following years. Namely PLM approaches brought a series of upsides with them:

\begin{enumerate}
    \item \textbf{Compute Efficiency} — PLMs like \textsc{RESDSQL} achieve high accuracy (up to 84.1\% on \textsc{Spider}
        depending on variants) wilst using far fewer parameters than conteporary LLMs, making them significantly more efficient
        and therefore reduce hardware requirements for their deployment \citep{RESDSQL}.
    \item \textbf{Transferability} — Approaches like \textsc{Grappa} and \textsc{StruG} can incorporate domain-specific understanding
        of natural language, table structures and SQL syntax during pre-training which addressed one of the primary issues with neural
        approaches \citep{GRAPPA, STRUG}.
    \item \textbf{Vocabulary} — PLMs offer a larger vocabulary due to the vast amounts of training data available. This enables them to
        handle a wide variety of natural language patterns which addresses the benchmark-overfitting tendency of neural approaches which
        primarily trained on the development sets of contemporary benchmarks.
\end{enumerate}

\subsubsection{Limitations of PLM Approaches}

Although representing the state-of-the-art at the time, PLMs introduce a class of problems which are associated with their
non-NL2SQL associated nature. There have been an array of approaches to mitigate these shortcommings but nonetheless they
must be considered when using a PLM-based approach to NL2SQL:

\begin{enumerate}

    \item \textbf{Fine-tuning Requirements} — Most PLMs require substantial domain-specific, or at least NL2SQL specific,
          fine-tuning, limiting a straight forward adaptation to new domains or databases. Although being significantly
          more efficient than LLM-based approaches the potential need for initial fine-tuning represent a significant
          computational resource burden. Furthermore when not using synthetic data generation (e.g. \textsc{Grappa})
          annotated datasets of training data are needed to achieve appropiate performance characteristics \citep{GRAPPA}.
    \item \textbf{Wide Input \& Output Space} — Due to the general nature of PLMs their input and output space is often
          far larger than needed 
          NL2SQL tasks. ``Large pre-trained language models for textual data have an unconstrained output space; at each
          decoding step, they can produce any of 10,000s of sub-word tokens'' \citep{PICARD}. This applies to both the
          input and output token space, therefore multiple approaches have been researched which focus on constraining
          these to the subset needed for NL2SQL tasks. Namely GRAPHIX-T5 and \textsc{Picard} have proposed potential
          (and promising) solutions to this issue \citep{GRAPHIX, PICARD}.
    \item \textbf{Limited Schema Awareness} — Due to being general purpose, and non-NL2SQL optimized, PLMs tend to
          incorporate limited amounts of schema awareness when being applied out of the box for NL2SQL tasks. Multiple
          research efforts focused on improving this situation, most notably \textsc{Resdsql} and GRAPHIX-T5 tried to
          improve the schema linking \& awareness of PLMs \citep{RESDSQL, GRAPHIX}, nonetheless the non-specialized nature
          of PLMs prevents NL2SQL being part of the fundamental model architecture.
\end{enumerate}

These characteristics positioned PLMs as powerful but comparatively resource-intensive solutions for NL2SQL (especially
in direct comparison with neural approaches), ultimately yielding the research domain to transition toward exploring
Large Language Model approaches that promise even greater flexibility in adaptation and potentially superior handling
of complex queries through advanced in-context learning approaches.

\subsubsection{Comparison with Large Language Models}

The research on applying pre-trained language models for NL2SQL tasks primed the field for the transition towards LLM usage. While
PLMs like T5 and BERT range from millions to a few billion parameters, prevalent LLMs such as GPT-3 and GPT-4 operate at
significantly larger scales, ranging from a few billions to hundred of billions parameters. The scale of LLMs enables in-context
learning techniques that enable significantly easier and cheaper transferability of NL2SQL systems across domains \citep{DAIL-SQL}.
The $\delta$ of deployment, inference and training requirements of these two approaches are significant due to the size difference
in models, which transfer to hardware requirements and therefore cost. While PLMs can require extensive fine-tuning on domain-specific
data which may aswell be resource intensive \citep{GRAPHIX, RESDSQL, GRAPPA, STRUG}, LLMs transfer the cost to the inference environment,
where model modificants are less impactful, due to the extensive pre-training that took place. Approaches like \textsc{DinSQL} show
that with the application of LLMs the engineering challenges around model instruction gained relevance while model training became
less of a central problem to solve \citep{DINSQL}.

\subsection{Large Language Models}

The emergence of LLMs such as GPT-3, GPT-4, and Claude fundamentally transformed the landscape of NL2SQL research. Early experiments 
with LLMs for NL2SQL tasks showed state-of-the-art capabilities in comparsion with contemporary PLM approaches \citep{DAIL-SQL}.
\cite{T2SQL-LLM-Bench-3} demonstrated that \textsc{Codex} (a contemporary model based on GPT-3), without any fine-tuning efforts,
could achieve competitive performance on \textsc{Spider}, outperforming many state-of-the-art approaches that required extensive
training. This breakthrough challenged the contemporary assumption that further specialization of model architectures would yield
increases in NL2SQL performance (e.g. \textsc{Graphix-T5}) \citep{GRAPHIX}.

\subsubsection{In-Context Learning}

In-Context Learning (ICL) is a foundational approach for leveraging the ability of LLMs to utilize larger context windows for
inference than traditional PLMs. Typical context windows of state-of-the-art LLMs can reach up to hundred thousands of tokens.
This characteristic of LLMs enabled researchers to utilize this context window to provide examples of accurate NL2SQL translation
instead of applying parameter updates. This paradigm shift has made developing NL2SQL systems significantly more accessible.

The fundamental principle of few-shot learning for NL2SQL involves providing the LLM with a small number of example pairs
of natural language and their corresponding SQL representation. These examples can benefit the model's understanding of
mapping between natural language and SQL syntax. This essentially builds on top of prior research like \textsc{Grappa}
and \textsc{StruG}, but applying these examples at inference time, rather than training time. Although this increases
the inference cost of such a system, the upsides lie primarily in the flexibility of such an approach — database content
/ prior usage of the system can be dynamically utilized, rather than requiring retraining.

Example selection strategies showed to have a considerable impact on ICL performance. \cite{DAIL-SQL} evaluated
various example selection methods like \textit{Random}, \textit{Question Similarity Selection (QTS)}, 
\textit{Masked Question Similarity Selection (MQS)}, and \textit{Query Similarity Selection (QRS)}. \citeauthor*{DAIL-SQL}
propose a novel strategy to select, organize and present ICL examples to LLMs. DAIL-SQL utilizes both question and
query similarity, masking domain-specific words and prioritizing examples that exceed a similarity threshold of $\tau$
\citep[p.~5]{DAIL-SQL}. DAIL-SQL encodes examples as question-SQL-pairs without the respective schema to improve
token efficiency. Using a Code Representation Prompt (CR) for question and schema encoding yielded DAIL-SQL to achieve
state-of-the-art 86.6\% execution accuracy on \textsc{Spider}.

The comparison between zero-shot and few-shot performance reveals the accuracy gain potential through supplying examples
to models in the inference context. While contemporary LLMs (such as GPT-4) have demonstrate impressive zero-shot performance
(achieving 72.3\% execution accuarcy on benchmarks like \textsc{Spider}) \citep[Table 1, p.~8]{DAIL-SQL}, few-shot learning 
still shows to substantially improve model performance. \cite{DAIL-SQL} shows that even one-shot learning boosts GPT-4's
execution accuracy to 80.2\%, representing a 7.9\% increase, while five-shot learning reaches 82.4\% \citep[Table 2, p.~8]{DAIL-SQL}. 

Especially with complex queryies which can involve multiple tables, nested queries and complex joins, zero-shot approaches
often dramatically underperform $k$-shot ones. NL2SQL approaches that dont supply examples to the model during inference time
fail more frequently to generate semantically accurate SQL queries \citep{DAIL-SQL}. Notable is the leap in exact match ratio
measured by the \textsc{Spider} benchmark — jumping from 22.1\% for GPT-4 using zero-shot to 71.9\% with five-shot. The
results presented by \cite{DAIL-SQL} show signficant correlation between $k$ and the execution accuracy of $k$-shot approaches.

This effectiveness has established ICL approaches as a standard technique applied in LLM-based NL2SQL approaches. Contemporary
approaches like \textsc{XiYan-SQL, Chase-SQL and Din-SQL} all utilize variations of ICL to achieve state-of-the-art results
\citep{XiYan, CHASE, DINSQL}.

\subsubsection{Self-Correction and Iterative Refinement}

\cite{DINSQL} proposed DIN-SQL as an innovative approach to NLIDBs that rely on LLMs. DIN-SQL decomposes complex queries into
sub-parts and utilizes in-context-learning and self-correction during the generation phase. Compared to DAIL-SQL which relies
on example selection during the in-context-learning phase, DIN-SQL focuses on a refinement loop that allows the model to self-
correct errors it made during the initial generation phase — thus the model can repair schema linking, syntactic or semantic
errors. By explicitly instructing the LLM to review its work against a specfic schema, the user input and potential database
errors, DIN-SQL achieves a high execution accuracy on \textsc{Spider} with 85.3\%. Therefore DIN-SQL outperforms contemporary
approaches but is surpassed by by DAIL-SQL by 1.3\% \citep{DINSQL, DAIL-SQL}. Furthermore DIN-SQL makes observations on the
impact that the self-correction prompt can steer results significantly — \citeauthor{DINSQL} found that using \textit{generic
self-correction} (ie. assuming the query contains errors) lowers the execution accuracy by 4.2\% on \textsc{Spider} compared to
\textit{gentle self-correction} (ie. assuming nothing about the validity of the query). It was noted that the impact of the
self-correction mechanism relies on the model size, with smaller models performing better with \textit{generic self-correction}
and larger models performing better with \textit{gentle self-correction} \citep{DINSQL}. The self-correcting nature of DIN-SQL
represents a diversion from DAIL-SQL's emphasis on input optimization towards output refinement. \citeauthor{DINSQL} demonstrate
how structured introspection can play a significant role in enhancing LLM performance for formal language generation tasks.

Building upon DIN-SQL's self-correction module, \cite{MAGIC} proposed MAGIC (Multi-Agent Guideline for In-Context Text-to-SQL),
which further advances the self-correction mechanism through harnessing a set of specialized agents to automate the self-correction
prompt engineering \citep{MAGIC}. MAGIC consists of a manager agent, a correction agent and a feedback agent that collaboratively
refine LLM instructions during the refinement loop. Further MAGIC derives common faliure patterns of the initial query generation
phase from training data, allowing it to efficiently spot the most common mistakes that the model makes at generation time. This
approach represent a further advancement on \citeauthor{DINSQL}'s DIN-SQL, effectively supersetting the \textit{generic} and
\textit{gentle} correction mechanisms through an intelligent, self-adapting one \citep{MAGIC}. The autogenerated guidelines from
MAGIC yield 85.6\% execution accuracy on the \textsc{Spider} development set — representing a 5.31\% improvement over DIN-SQL's
human written correction guidelines. These results emphasize that optimized self-correction mechanisms have the ability to
significantly drive up overall system performance of NLIDBs \citep{MAGIC}.

While DIN-SQL and MAGIC focus on automated self-correction in single-turn settings, \cite{CoE-SQL} introduced the concept of
Chain-of-Editions (CoE-SQL), which addresses the unique challenges of multi-turn conversational NLIDBs. Conversational interfaces
for NL2SQL systems enable human-in-the-loop refinement. Interactive information seeking from the user has shown to be an
effective way to drive overall accuracy of the system and improve user satisfaction \citep{NALIR}. Rather than approaching
each query independently, CoE-SQL recognizes that in a conversational context, successive SQL queries usuallly require only
small and incremental modifications of the previous queries. Interactive user input is an effective measure for dealing with
ambiguous natural language queries \citep{CoE-SQL, UnnaturalQueryLanguage, NALIR}.

\subsubsection{Candidate Selection Frameworks}

Contemporary NL2SQL approaches have increasingly emphasized on the generation of query candidates and their selectiion as a
promising architecture. Candidate selection strategies have shown significant performance improvements on challenging benchmarks.
These approach acknowledge the inherent difficulty of generating perfect SQL queries in one attempt / using one generation mechanism,
even with capable LLMs and modern self-correction mechanisms.

\cite{CHASE} introduced CHASE-SQL in \citeyear{CHASE}, a framework that leverages multiple reasoning paths to generate multiple
query candidates. After the initial generation phase CHASE selects the most promising solution to the natural query input.
CHASE-SQL harnesses three different generation strategies: A divide-and-conquer approach which breaks down complex natural
language queries into multiple sub tasks that can be individually tackled, a chain-of-thought based generation approach which
inspects execution plans of SQL queries and a schema-aware generation of synthetic examples that can be used for in-context
learning \citep{CHASE}. These different generation mechanisms produce a set of query candidates that each have different
characteristics. For candidate selection CHASE harnesses a fine-tuned LLM that can do binary selection of candidates.
\citeauthor{CHASE} have demostrated that their query selection approach is more robust than apparent alternatives and yields
state-of-the-art performance with 73\% execution accuracy on \textsc{Bird} and 87.6\% on \textsc{Spider} \citep{CHASE}.

Conceptually similar work has been done by \citeauthor*{XiYan} with \textsc{XiYan-SQL} which is architected as multi-generation
ensemble strategy with better schema representation. \textsc{XiYan-SQL} integrated in-context learning alongside supervised
fine-tuning approaches to generate query candidates \citep{XiYan}. A key contribution of \cite{XiYan} is their M-Schema
representation of database schemas, which improves the models schema awareness and reduces frequent schema linking errors.
\textsc{XiYan-SQL} enhances accuracy by utilizing multiple different strategies that have complementary characteristics
during query generation to enhance the robustness of the overall system. The query generation stage utilizes both a
fine-tuned SQL generation model aswell as ICL strategies to achieve a breadth of candidate coverage. Following to the query
generation stage a self-correction stage (referred to as \textit{refinement} stage by \citeauthor{XiYan}) is utilized
to correct common errors. Lastly a selection model is used to choose the most accurate candidate that was produced
during the generation stage \citep{XiYan}. Through this sophisticated and diverse architecture \textsc{XiYan-SQL} was
able to achieve impressive results across contemporary NL2SQL benchmarks — achieving 89.65\% execution accuracy on
\textsc{Spider} and 73.34\% on \textsc{Bird}, which renders \textsc{XiYan-SQL} state-of-the-art \citep{XiYan}.

Both CHASE and \textsc{XiYan-SQL} show that diversifing candidate generation and training specialized models for candidate
selection yield state-of-the-art execution accuracy which significantly outperforms single-path generation approaches.
The success of these two approaches indicates that for increasingly complex NL2SQL tasks (such as \textsc{Spider2.0}),
the capability to generate multiple valid interpretations of the natural language query is an important stepping stone
to achieving meaningful execution accuracy. Both CHASE and \textsc{XiYan-SQL} rely on specialized candidate selection
models which renders these approaches to combine the strengths of LLMs when it comes to language understanding and
transferability with the robustness of specialized model architectures for candidate ranking and selection.

% Maybe: Agent specialization and collaboration patterns
% Maybe: Voting and selection mechanisms for query ranking

\subsubsection{Retrieval-Augumented Generation}

Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing NL2SQL systems by integrating
external knowledge retrieval with the generative capabilities of LLMs. This technique is seen in all above introduced
papers in varying forms \citep{XiYan, CHASE, CoE-SQL, MAGIC, DINSQL, DAIL-SQL}. The most prevalent form of RAG in
NL2SQL is the encoding of the database schema into the LLM prompt for in-context-learning. This allows LLMs to be
aware of the table structures and names, foreign key relationships, primary keys etc. \cite{XiYan} proposed the
serialization of database schemas in the \textsc{M-Schema} format, a semi structured, text-based schema description.
\cite{DAIL-SQL} proposed to encode the schema using a Code Representation Prompt (CR) which refers to the encoding of
the raw SQL statements need to construct the schema. \cite{XiYan} provided an ablation study for the M-Schema which
yielded questionable results on the optimization of this approach. While the M-Schema format yielded best results when
\textsc{XiYan-SQL} was combined with GPT-4o or Claude 3.5 Sonnet, it performed worse than alternatives on DeepSeek
and Gemini models \citep{XiYan}.

A relevant optimization technique for RAG is the selection of a schema subset before encoding the schema for the model.
\textsc{Resdsql} was one of the earlier approaches to explore subset-encoding of database schemas with \cite{RetAug}
building on top of this \citep{RetAug, RESDSQL}. \cite{RetAug} introduced \textsc{ASTReS} which dynamically
retrieves database schemas and uses abstract syntax trees (ASTs) to select optimal few-shot examples for ICL. 
By pruning the ASTs down to the most relevant subset, \textsc{ASTReS} achieved the highest at-the-time (\citeyear{RetAug})
execution accuracy on \textsc{Spider} with 86.6\% indicating that subset-encoding is a sensible optimization mechanism.
\textsc{ASTReS} was combined with GRAPHIX-T5 in order to achieve this result \citep{RetAug}.

The impact of RAG when NL2SQL systems face large and complex database schemas has been particularly significant. Traditional
approaches struggle when database schemas contain hundreds or thousands of tables and columns, as the complete schema may not
fit within model context windows depending on their size. RAG-based systems address this by dynamically retrieving only the most
relevant portions of the schema based on the natural language query. The technique of subset-encoding becomes especially relevant
in enterprise environments where database schemas can be extremely large and complex. Recent benchmarks like \textsc{Spider2.0}
emphasize enterprise environments and show that existing solutions underperform in those scenarios, often reaching single digit
execution accuracy.

The work of \citeauthor{RetAug} indicates that prefiltering of the environment of language models is an effective
and promising technique that has the ability to reduce computational requirements of contemporary NL2SQL systems.
As introduced above, recent state-of-the-art systems often utilize closed-source models like Gemini, GPT-4(o), Claude 3.5/3.7
Sonnet etc which often come with massive parameter sizes (reaching hundreds of billions of parameters). \textsc{ASTReS}
demonstrated that efficient schema retrieval mechanisms enable smaller models (e.g. GRAPHIX-T5) to achieve state-of-the-art
performance against LLM based approaches like DAIL-SQL \citep{DAIL-SQL, RetAug}.

\subsubsection{Specialized LLMs and Fine-tuning}

While general-purpose LLMs demonstrated state-of-the-art natural language understanding capabilities, the research
domain of NLIDBs increasingly explored the potential of fine-tuned LLMs for NL2SQL tasks, which offer a promising
tradeof between natural language understanding (ie. breadth of the model) and concrete SQL generation capabilities
(ie. depth of the model). Multiple research works have been done on the fine-tuning of language models which
yielded a series of dedicated models for optimized SQL generation.

A significant limitation of many LLM-based NL2SQL solutions is their dependency on proprietary and closed-source
LLMs like GPT-4(o), Claude and Gemini. Whilst they are useful for initially proving the potential of LLMs on
contemporary benchmarks, this dependency introduces significant concerns related to data-privacy, high-side use 
(e.g. in classified environments), transparency of data-flow and deployment costs. To address these challenges
\cite{CodeS} have introduced \textsc{CodeS} in \citeyear{CodeS}, a series of of open-source language models dedicated
for NL2SQL tasks with parameter sizes ranging from 1B to 15B. The \textsc{CodeS} models were evaluated against
contemporary benchmarks like \textsc{Spider} and \textsc{Bird} and showed promising inference results when compared
to their closed-source counterparts. \cite{CodeS} showed that \textsc{CodeS} 7B achieves 85.4\% execution accuracy
on the \textsc{Spider} development set, outperforming both fine-tuning approaches like RESDSQL and GRAPHIX-T5-PICARD
aswell as prompting based methods DIN-SQL+GPT-4 and DAIL-SQL+GPT-4 \citep{CodeS}. The same tendency was observed
on \textsc{Bird} with \textsc{CodeS}-15B achieving 60.37\% execution accuracy, compared to 57.41\% for DAIL-SQL+GPT-4
and 55.90\% for DIN-SQL+GPT4 \citep{CodeS}. This marks a significant advancement in the open-source language model
research area, with \textsc{CodeS} reaching new state-of-the-art performance in \citeyear{CodeS}. While more recent
approaches like \textsc{XiYan-SQL} and CHASE both outperform \textsc{CodeS}, CHASE relies on proprietary models and
\textsc{XiYan-SQL} doesnt provide any information on what base models where used.

\cite{CodeS} addressed serveral critical research challenges in the NL2SQL domain and proved that open-source models
could perform competitively with proprietary models whilst maintaining a significantly smaller parameter footprint
(ie. 7B and 15B) compared to GPT-4 which is a multi-hundred-billion parameter model. This makes it feasibile to deploy
\textsc{CodeS} locally, instead of relying on an enterprise API like OpenAI's one \citep{CodeS}, therefore making it
highly practical for real-world deployments where computation resource are constrained. 

\citeauthor*{OmniSQL} published a follow-up paper in \citeyear{OmniSQL} which introduced the next-generation \textsc{OmniSQL}
models with 7B, 14B and 32B sizes, trained using synthetic data generation. \textsc{OmniSQL} achieves state-of-the-art
performance when compared to alternative LLMs - including both open-source and closed-source competitors. The models
achieve significant execution accuracy improvements on both \textsc{Spider} and \textsc{Bird}, the 7B model reaches 88.9\%
on \textsc{Spider} and 66.1\% execution accuracy on \textsc{Bird} which represents a significant (5\%+) improvement over
comparable alternatives \citep{OmniSQL}. These results were achieved without combining \textsc{OmniSQL} with advanced
in-context-learning, self-correction, retrieval-augumented-generation or candidate-selection techniques, which indicates
that even higher accuracy scores are possible.

\subsection{Benchmarking}

In order to evaluate NL2SQL systems standardized benchmarks like \textsc{Spider} and \textsc{Bird} have emerged. These
benchmarks can measure performance across different approaches and models, enable meaningful ablation studies and are a
useful indicator for the state of the research field. In the past decade significant advancements have been made with
\textsc{Spider} being released in \citeyear{Spider} the first major, widely adopted, benchmark emerged in this field
\citep{Spider}.

\subsubsection{Spider}

\textsc{Spider}, introduced by \citeauthor{Spider} in \citeyear{Spider}, has become the de facto standard benchmark for
evaluating complex and cross-domain Text-to-SQL systems. It consists of 10,181 questions and 5,693 unique SQL queries
spanning 200 databases across 138 domains. Previous benchmarks like lacked complexity and cross-domain distrubtion of
datapoints which prevented the \textit{transferability} of approaches or models to be accounted for in benchmarks.
With \textsc{Spider} the capability to be database agnostic was required to achieve meaningful accuracy scores. Furthermore
\textsc{Spider} was split in training and test sets which contain different database in order to prevent overfitting
models from succeeding. This design specifically tests a model's ability to handle schema linking and generalization
challenges rather than memorizing specific database patterns. \textsc{Spider} evaluates both \textit{exact matching
accuracy} and \textit{execution accuracy}, with contemporary state-of-the-art systems achieving approximately 85-90\%
\textit{execution accuracy} (as of 2025) \citep{Spider, OmniSQL, XiYan, CHASE}.

\subsubsection{Bird}

The \textsc{Bird} benchmark (BIg bench for large-scale database gRounded Text-to-SQLs), released in \citeyear{BIRD},
and addresses the gap between academic benchmarks and real-world applications by focusing on large-scale databases
with actual data content \citep{BIRD}. \textsc{Bird} contains 12,751 text-to-SQL pairs and 95 databases with a total
size of 33.4 GB across 37 professional domains. Unlike \textsc{Spider}, which primarily evaluates against database
schemas with minimal content, \textsc{Bird} emphasizes challenges related to dirty database contents, external
knowledge between natural language questions and database values, and SQL efficiency in massive databases. This
places \textsc{Bird} as a relevant benchmark for real world feasibility of approaches and models. Even state-of-the-art
LLMs like GPT-4 achieve only 54.89\% execution accuracy on \textsc{Bird}, compared to human performance of 92.96\%,
highlighting the significant challenges posed by real-world database scenarios on NLIDBs \citep{BIRD}.

\subsubsection{Spider 2.0}

\textsc{Spider 2.0} which was introduced by \citeauthor*{SPIDER2} in \citeyear{SPIDER2} represents the most recent
advancements of benchmarks for NL2SQL systems. It represents a significant evolution in NL2SQL benchmarking and focuses
primarily on enterprise level database challenges. \textsc{Spider 2.0} is much smaller with only 632 real-world problems
which were derived from enterprise database usecases, but yet represents a meaningful indicator for the complexity of
databases that approaches and models can handle. \textsc{Spider 2.0} goes beyond simple query generation tasks and moves
towards testing the deep understanding of the database, requiring models to understand metadata, SQL dialect documentation
and project-level codebases \citep{SPIDER2}. The tasks contained in \textsc{Spider 2.0} often demand multiple complex SQL
queries often exceeding the 100-line mark and require incorporating a diverse set of database operation from transformation
to analytics. \citeauthor{SPIDER2} further highlights the gap between academic research and enterprise-level environments
with even advanced approaches achieving only 21.3\% on \textsc{Spider2.0} compared to 91.2\% on \textsc{Spider 1.0}
\citep{SPIDER, SPIDER2}.

\subsection{Research Gaps}

\subsubsection{Deployment Gaps}

\newpage

\section{Theoretical Foundations}

\subsection{Problem Decomposition}

\subsection{Requirements}

% Conceptual Design
\subsection{System Design}

\subsubsection{Architecture Design}

\subsection{Technical Implementation Strategies}

\newpage

% Implementation
\section{Implementation}

\subsection{Development Environment and Tools}

\subsection{Integration of the Model}


\subsection{Development of the PostgreSQL Extension}


\subsection{Optimization}

\newpage

% Evaluation
\section{Evaluation}

% https://github.com/petavue/NL2SQL-Benchmark/blob/main/results/Report/NL2SQL%20Benchmark%20Report.pdf

\subsection{Test Environment and Methodology}

\subsection{Performance Tests}
\subsubsection{Latency}
\subsubsection{Throughput}
\subsubsection{Scalability}

\subsection{Use Cases}
\subsubsection{Natural Language Queries}
\subsubsection{Text Generation Within the Database}
\subsubsection{Semantic Search and Text Classification}

\subsection{Ablation Study}

\subsection{Comparison with Alternative Approaches}

\newpage


% Discussion
\section{Discussion}

\subsection{Interpretation of Results}
\subsection{Limitations of the Implementation}
\subsection{Ethical and Data Privacy Considerations}
\subsection{Potential Future Developments}

\newpage

% Summary and Outlook
\section{Summary and Outlook}

\subsection{Summary of Results}
\subsection{Addressing the Research Questions}
\subsection{Outlook for Future Research and Development}

\newpage

\newpage

\appendix
\section*{Appendix}
\subsection*{Installation Guide}
\subsection*{API Documentation}
\subsection*{Code Examples}
\subsection*{Test Data and Results}

\bibliographystyle{apacite}
\bibliography{references}

\end{document}