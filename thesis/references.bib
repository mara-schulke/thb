% Foundations

@article{NLIDBs,
    author = {Androutsopoulos, Ion and Ritchie, Graeme D. and Thanisch, Peter},
    title = {Natural Language Interfaces to Databases - An Introduction},
    journal = {CoRR},
    volume = {cmp-lg/9503016},
    year = {1995},
    url = {http://arxiv.org/abs/cmp-lg/9503016},
    abstract = {This paper is an introduction to natural language interfaces to databases (NLIDBs). A brief overview of the history of NLIDBs is first given. Some advantages and disadvantages of NLIDBs are then discussed, comparing NLIDBs to formal query languages, form-based interfaces, and graphical interfaces. An introduction to some of the linguistic problems NLIDBs have to confront follows, for the benefit of readers less familiar with computational linguistics. The discussion then moves on to NLIDB architectures, portability issues, restricted natural language input systems (including menu-based NLIDBs), and NLIDBs with reasoning capabilities. Some less explored areas of NLIDB research are then presented, namely database updates, meta-knowledge questions, temporal questions, and multi-modal NLIDBs. The paper ends with reflections on the current state of the art.},
    timestamp = {Mon, 13 Aug 2018 16:45:57 +0200},
    biburl = {https://dblp.org/rec/journals/corr/cmp-lg-9503016.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NLIDBTheory,
    author = {Popescu, Ana-Maria and Etzioni, Oren and Kautz, Henry},
    title = {Towards a theory of natural language interfaces to databases},
    year = {2003},
    isbn = {1581135866},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/604045.604070},
    doi = {10.1145/604045.604070},
    abstract = {The need for Natural Language Interfaces to databases (NLIs) has become increasingly acute as more and more people access information through their web browsers, PDAs, and cell phones. Yet NLIs are only usable if they map natural language questions to SQL queries correctly. As Schneiderman and Norman have argued, people are unwilling to trade reliable and predictable user interfaces for intelligent but unreliable ones. In this paper, we introduce a theoretical framework for reliable NLIs, which is the foundation for the fully implemented Precise NLI. We prove that, for a broad class of semantically tractable natural language questions, Precise is guaranteed to map each question to the corresponding SQL query. We report on experiments testing Precise on several hundred questions drawn from user studies over three benchmark databases. We find that over 80\% of the questions are semantically tractable questions, which Precise answers correctly. Precise automatically recognizes the 20\% of questions that it cannot handle, and requests a paraphrase. Finally, we show that Precise compares favorably with Mooney's learning NLI and with Microsoft's English Query product},
    booktitle = {Proceedings of the 8th International Conference on Intelligent User Interfaces},
    pages = {149–157},
    numpages = {9},
    keywords = {database, natural language interface, reliability},
    location = {Miami, Florida, USA},
    series = {IUI '03}
}

% Foundations - Benchmarking

@article{ImprovingT2SQLEvaluation,
    author = {Catherine Finegan{-}Dollak and Jonathan K. Kummerfeld and Li Zhang and Karthik Ramanathan and Sesh Sadasivam and Rui Zhang and Dragomir R. Radev},
    title = {Improving Text-to-SQL Evaluation Methodology},
    journal = {CoRR},
    volume = {abs/1806.09029},
    year = {2018},
    url = {http://arxiv.org/abs/1806.09029},
    eprinttype = {arXiv},
    eprint = {1806.09029},
    abstract = {To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.},
    timestamp = {Fri, 19 Nov 2021 15:41:24 +0100},
    biburl = {https://dblp.org/rec/journals/corr/abs-1806-09029.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Foundations - Datasets

@article{SPIDER,
    author = {Tao Yu and Rui Zhang and Kai Yang and Michihiro Yasunaga and Dongxu Wang and Zifan Li and James Ma and Irene Li and Qingning Yao and Shanelle Roman and Zilin Zhang and Dragomir R. Radev},
    title = {Spider: {A} Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task},
    journal = {CoRR},
    volume = {abs/1809.08887},
    year = {2018},
    url = {http://arxiv.org/abs/1809.08887},
    eprinttype = {arXiv},
    eprint = {1809.08887},
    abstract = {We present Spider, a large-scale, complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task where different complex SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and the exact same programs in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 12.4% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research.},
    timestamp = {Sun, 02 Oct 2022 15:31:53 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-1809-08887.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Foundations - LLMs

@misc{Atlas,
    title = {Atlas: Few-shot Learning with Retrieval Augmented Language Models}, 
    author = {Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
    year = {2022},
    eprint = {2208.03299},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2208.03299}, 
    abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.}
}

% Foundations - NLP

@inproceedings{ILPParsing,
    author = {Zelle, John M. and Mooney, Raymond J.},
    title = {Learning to parse database queries using inductive logic programming},
    year = {1996},
    isbn = {026251091X},
    publisher = {AAAI Press},
    abstract = {This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application.},
    booktitle = {Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2},
    pages = {1050–1055},
    numpages = {6},
    location = {Portland, Oregon},
    series = {AAAI'96}
}


@inproceedings{ILPParsing2,
    author = {Tang, Lappoon R. and Mooney, Raymond J.},
    title = {Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing},
    year = {2001},
    isbn = {3540425365},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    abstract = {In this paper, we explored a learning approach which combines different learning methods in inductive logic programming (ILP) to allow a learner to produce more expressive hypotheses than that of each individual learner. Such a learning approach may be useful when the performance of the task depends on solving a large amount of classification problems and each has its own characteristics which may or may not fit a particular learning method. The task of semantic parser acquisition in two different domains was attempted and preliminary results demonstrated that such an approach is promising.},
    booktitle = {Proceedings of the 12th European Conference on Machine Learning},
    pages = {466–477},
    numpages = {12},
    series = {EMCL '01}
}

% Foundations - Techniques

@article{Seq2SQL,
    author = {Victor Zhong and Caiming Xiong and Richard Socher},
    title = {Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},
    journal = {CoRR},
    volume = {abs/1709.00103},
    year = {2017},
    url = {http://arxiv.org/abs/1709.00103},
    eprinttype = {arXiv},
    eprint = {1709.00103},
    abstract={A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.},
    timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-1709-00103.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NALIR,
    author = {Li, Fei and Jagadish, Hosagrahar V},
    title = {NaLIR: an interactive natural language interface for querying relational databases},
    year = {2014},
    isbn = {9781450323765},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2588555.2594519},
    doi = {10.1145/2588555.2594519},
    abstract = {In this demo, we present NaLIR, a generic interactive natural language interface for querying relational databases. NaLIR can accept a logically complex English language sentence as query input. This query is first translated into a SQL query, which may include aggregation, nesting, and various types of joins, among other things, and then evaluated against an RDBMS. In this demonstration, we show that NaLIR, while far from being able to pass the Turing test, is perfectly usable in practice, and able to handle even quite complex queries in a variety of application domains. In addition, we also demonstrate how carefully designed interactive communication can avoid misinterpretation with minimum user burden.},
    booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
    pages = {709–712},
    numpages = {4},
    keywords = {usability, relational database, natural language interface},
    location = {Snowbird, Utah, USA},
    series = {SIGMOD '14}
}

@article{SQLizer,
    author = {Yaghmazadeh, Navid and Wang, Yuepeng and Dillig, Isil and Dillig, Thomas},
    title = {SQLizer: query synthesis from natural language},
    year = {2017},
    issue_date = {October 2017},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {1},
    number = {OOPSLA},
    url = {https://doi.org/10.1145/3133887},
    doi = {10.1145/3133887},
    abstract = {This paper presents a new technique for automatically synthesizing SQL queries from natural language (NL). At the core of our technique is a new NL-based program synthesis methodology that combines semantic parsing techniques from the NLP community with type-directed program synthesis and automated program repair. Starting with a program sketch obtained using standard parsing techniques, our approach involves an iterative refinement loop that alternates between probabilistic type inhabitation and automated sketch repair. We use the proposed idea to build an end-to-end system called SQLIZER that can synthesize SQL queries from natural language. Our method is fully automated, works for any database without requiring additional customization, and does not require users to know the underlying database schema. We evaluate our approach on over 450 natural language queries concerning three different databases, namely MAS, IMDB, and YELP. Our experiments show that the desired query is ranked within the top 5 candidates in close to 90\% of the cases and that SQLIZER outperforms NALIR, a state-of-the-art tool that won a best paper award at VLDB'14.},
    journal = {Proc. ACM Program. Lang.},
    month = oct,
    articleno = {63},
    numpages = {26},
    keywords = {Relational Databases, Programming by Natural Languages, Program Synthesis}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Parsing

@book{Lunar,
    title={The Lunar Sciences Natural Language Information System: Final Report},
    author = {Woods, W. A. and Kaplan, R. and Nash-Webber, B.},
    year={1972},
    publisher={Bolt, Beranek and Newman, Inc.},
    address={Cambridge, Massachusetts}
}

@article{Ladder,
    author = {Hendrix, Gary G. and Sacerdoti, Earl D. and Sagalowicz, Daniel and Slocum, Jonathan},
    title = {Developing a natural language interface to complex data},
    year = {1978},
    issue_date = {June 1978},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {3},
    number = {2},
    issn = {0362-5915},
    url = {https://doi.org/10.1145/320251.320253},
    doi = {10.1145/320251.320253},
    abstract = {Aspects of an intelligent interface that provides natural language access to a large body of data distributed over a computer network are described. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. A pragmatic approach to language access that has proved useful for building interfaces to databases is described and illustrated by examples. Special language features that increase system usability, such as spelling correction, processing of incomplete inputs, and run-time system personalization, are also discussed. The language system is contrasted with other work in applied natural language processing, and the system's limitations are analyzed.},
    journal = {ACM Trans. Database Syst.},
    month = jun,
    pages = {105–147},
    numpages = {43},
    keywords = {semantic grammar, run-time personalization, natural language, intelligent interface, human engineering, database access}
}

@inproceedings{Rendezvous,
    added-at = {2019-08-02T00:00:00.000+0200},
    author = {Codd, E. F.},
    biburl = {https://www.bibsonomy.org/bibtex/23f3967203f75a0382f9ca1710abee49b/dblp},
    booktitle = {IFIP Working Conference Data Base Management},
    cdrom = {DS/DS1974/P179.pdf},
    crossref = {conf/ds/1974},
    editor = {Klimbie, J. W. and Koffeman, K. L.},
    interhash = {c7a3ebd8f4e75b9add319a25e9ed42c9},
    intrahash = {3f3967203f75a0382f9ca1710abee49b},
    isbn = {0-7204-2809-2},
    keywords = {dblp},
    month = {January},
    note = {IBM Research Report RJ 1333, San Jose, California},
    pages = {179-200},
    publisher = {North-Holland},
    timestamp = {2019-08-03T11:37:33.000+0200},
    title = {Seven Steps to Rendezvous with the Casual User.},
    url = {http://dblp.uni-trier.de/db/conf/ds/dbm74.html#Codd74},
    year = 1974
}

@inproceedings{StringKernels,
    title = "Using String-Kernels for Learning Semantic Parsers",
    author = "Kate, Rohit J. and Mooney, Raymond J.",
    editor = "Calzolari, Nicoletta and Cardie, Claire and Isabelle, Pierre",
    booktitle = "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P06-1115/",
    doi = "10.3115/1220175.1220290",
    pages = "913--920",
    abstract = {We present a new approach for mapping natural language sentences to their formal meaning representations using stringkernel-based classifiers. Our system learns these classifiers for every production in the formal language grammar. Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers. Our experiments on two realworld data sets show that this approach compares favorably to other existing systems and is particularly robust to noise.}
}

@article{GraphMatching,
    title = "Large-scale Semantic Parsing without Question-Answer Pairs",
    author = "Reddy, Siva and Lapata, Mirella and Steedman, Mark",
    editor = "Lin, Dekang and Collins, Michael and Lee, Lillian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q14-1030/",
    doi = "10.1162/tacl_a_00190",
    pages = "377--392",
    abstract = "In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the Free917 and WebQuestions benchmark datasets show our semantic parser improves over the state of the art."
}

@inproceedings{UnnaturalQueryLanguage,
    author = {Montgomery, Christine A.},
    title = {Is natural language an unnatural query language?},
    year = {1972},
    isbn = {9781450374927},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/800194.805902},
    doi = {10.1145/800194.805902},
    abstract = {The complex system of form/meaning correlations comprising a natural language presents a considerable challenge for automated interpretation. In order to deal with such complexity, attempts at automated interpretation of natural language queries have typically concentrated on limited subsets of natural language. However, such subsets are inevitably ill-defined in some way, adding another class of interpretive problems to the well-known ones of syntactic ambiguity and ungrammatical strings. While the other problems are practically resolvable in some sense, the ability of an automated system to resolve the many syntactic ambiguities of natural language is a function of the interpretive power of the linguistic model upon which the system is based. Although previous models have lacked the interpretive power to deal with natural language, a novel approach integrating the treatment of form and meaning may provide an effective basis for handling natural language as an instrument of communication in automated systems.},
    booktitle = {Proceedings of the ACM Annual Conference - Volume 2},
    pages = {1075–1078},
    numpages = {4},
    keywords = {Syntax, Semantics, Query languages, Natural language processing},
    location = {Boston, Massachusetts, USA},
    series = {ACM '72}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Neural Network Era

@inproceedings{IRNet,
    title = "Towards Complex Text-to-{SQL} in Cross-Domain Database with Intermediate Representation",
    author = "Guo, Jiaqi  and Zhan, Zecheng  and Gao, Yan  and Xiao, Yan and Lou, Jian-Guang  and Liu, Ting  and Zhang, Dongmei",
    editor = "Korhonen, Anna  and Traum, David  and M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1444/",
    doi = "10.18653/v1/P19-1444",
    pages = "4524--4535",
    abstract = "We present a neural approach called IRNet for complex and cross-domain Text-to-SQL. IRNet aims to address two challenges: 1) the mismatch between intents expressed in natural language (NL) and the implementation details in SQL; 2) the challenge in predicting columns caused by the large number of out-of-domain words. Instead of end-to-end synthesizing a SQL query, IRNet decomposes the synthesis process into three phases. In the first phase, IRNet performs a schema linking over a question and a database schema. Then, IRNet adopts a grammar-based neural model to synthesize a SemQL query which is an intermediate representation that we design to bridge NL and SQL. Finally, IRNet deterministically infers a SQL query from the synthesized SemQL query with domain knowledge. On the challenging Text-to-SQL benchmark Spider, IRNet achieves 46.7{\%} accuracy, obtaining 19.5{\%} absolute improvement over previous state-of-the-art approaches. At the time of writing, IRNet achieves the first position on the Spider leaderboard."
}

@article{SQLNet,
  author = {Xiaojun Xu and Chang Liu and Dawn Song},
  title = {SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning},
  journal = {CoRR},
  volume = {abs/1711.04436},
  year = {2017},
  url = {http://arxiv.org/abs/1711.04436},
  eprinttype = {arXiv},
  eprint = {1711.04436},
  timestamp = {Mon, 22 Jul 2019 13:37:30 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-1711-04436.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {Synthesizing SQL queries from natural language is a long-standing open problem and has been attracting considerable interest recently. Toward solving the problem, the de facto approach is to employ a sequence-to-sequence-style model. Such an approach will necessarily require the SQL queries to be serialized. Since the same SQL query may have multiple equivalent serializations, training a sequence-to-sequence-style model is sensitive to the choice from one of them. This phenomenon is documented as the "order-matters" problem. Existing state-of-the-art approaches rely on reinforcement learning to reward the decoder when it generates any of the equivalent serializations. However, we observe that the improvement from reinforcement learning is limited. In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally solve this problem by avoiding the sequence-to-sequence structure when the order does not matter. In particular, we employ a sketch-based approach where the sketch contains a dependency graph so that one prediction can be done by taking into consideration only the previous predictions that it depends on. In addition, we propose a sequence-to-set model as well as the column attention mechanism to synthesize the query based on the sketch. By combining all these novel techniques, we show that SQLNet can outperform the prior art by 9\% to 13\% on the WikiSQL task.}
}

@inproceedings{TypeSQL,
    title = "{T}ype{SQL}: Knowledge-Based Type-Aware Neural Text-to-{SQL} Generation",
    author = "Yu, Tao and Li, Zifan and Zhang, Zilin and Zhang, Rui and Radev, Dragomir",
    editor = "Walker, Marilyn and Ji, Heng and Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2093/",
    doi = "10.18653/v1/N18-2093",
    pages = "588--594",
    abstract = "Interacting with relational databases through natural language helps users with any background easily query and analyze a vast amount of data. This requires a system that understands users' questions and converts them to SQL queries automatically. In this paper, we present a novel approach TypeSQL which formats the problem as a slot filling task in a more reasonable way. In addition, TypeSQL utilizes type information to better understand rare entities and numbers in the questions. We experiment this idea on the WikiSQL dataset and outperform the prior art by 6{\%} in much shorter time. We also show that accessing the content of databases can significantly improve the performance when users' queries are not well-formed. TypeSQL can reach 82.6{\%} accuracy, a 17.5{\%} absolute improvement compared to the previous content-sensitive model."
}

@inproceedings{SyntaxSQLNet,
    title = "{S}yntax{SQLN}et: Syntax Tree Networks for Complex and Cross-Domain Text-to-{SQL} Task",
    author = "Yu, Tao and Yasunaga, Michihiro and Yang, Kai and Zhang, Rui and Wang, Dongxu and Li, Zifan and Radev, Dragomir",
    editor = "Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1193/",
    doi = "10.18653/v1/D18-1193",
    pages = "1653--1663",
    abstract = "Most existing studies in text-to-SQL tasks do not require generating complex SQL queries with multiple clauses or sub-queries, and generalizing to new, unseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address the complex and cross-domain text-to-SQL generation task. SyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL generation path history and table-aware column attention encoders. We evaluate SyntaxSQLNet on a new large-scale text-to-SQL corpus containing databases with multiple tables and complex SQL queries containing multiple SQL clauses and nested queries. We use a database split setting where databases in the test set are unseen during training. Experimental results show that SyntaxSQLNet can handle a significantly greater number of complex SQL examples than prior work, outperforming the previous state-of-the-art model by 9.5{\%} in exact matching accuracy. To our knowledge, we are the first to study this complex text-to-SQL task. Our task and models with the latest updates are available at \url{https://yale-lily.github.io/seq2sql/spider}."
}

@inproceedings{GNN,
    title = "Representing Schema Structure with Graph Neural Networks for Text-to-{SQL} Parsing",
    author = "Bogin, Ben and Berant, Jonathan and Gardner, Matt",
    editor = "Korhonen, Anna and Traum, David and M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1448/",
    doi = "10.18653/v1/P19-1448",
    pages = "4560--4565",
    abstract = "Research on parsing language to SQL has largely ignored the structure of the database (DB) schema, either because the DB was very simple, or because it was observed at both training and test time. In spider, a recently-released text-to-SQL dataset, new and complex DBs are given at test time, and so the structure of the DB schema can inform the predicted SQL query. In this paper, we present an encoder-decoder semantic parser, where the structure of the DB schema is encoded with a graph neural network, and this representation is later used at both encoding and decoding time. Evaluation shows that encoding the schema structure improves our parser accuracy from 33.8{\%} to 39.4{\%}, dramatically above the current state of the art, which is at 19.7{\%}."
}

@inproceedings{RATSQL,
    title = "{RAT-SQL}: Relation-Aware Schema Encoding and Linking for Text-to-{SQL} Parsers",
    author = "Wang, Bailin and Shin, Richard and Liu, Xiaodong and Polozov, Oleksandr and Richardson, Matthew",
    editor = "Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.677/",
    doi = "10.18653/v1/2020.acl-main.677",
    pages = "7567--7578",
    abstract = "When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2{\%}, surpassing its best counterparts by 8.7{\%} absolute improvement. Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6{\%} on the Spider leaderboard. In addition, we observe qualitative improvements in the model`s understanding of schema linking and alignment. Our implementation will be open-sourced at \url{https://github.com/Microsoft/rat-sql}."
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Recent 

% Recent - Benchmarks

@misc{BIRD,
    title={Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs}, 
    author={Jinyang Li and Binyuan Hui and Ge Qu and Jiaxi Yang and Binhua Li and Bowen Li and Bailin Wang and Bowen Qin and Rongyu Cao and Ruiying Geng and Nan Huo and Xuanhe Zhou and Chenhao Ma and Guoliang Li and Kevin C. C. Chang and Fei Huang and Reynold Cheng and Yongbin Li},
    year={2023},
    eprint={2305.03111},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2305.03111},
    abstract={Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. Besides, we also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research.}
}

@misc{T2SQL-LLM-Bench,
    title={Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation}, 
    author={Dawei Gao and Haibin Wang and Yaliang Li and Xiuyu Sun and Yichen Qian and Bolin Ding and Jingren Zhou},
    year={2023},
    eprint={2308.15363},
    archivePrefix={arXiv},
    primaryClass={cs.DB},
    url={https://arxiv.org/abs/2308.15363},
    abstract={Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications.}
}

@misc{T2SQL-LLM-Bench-2,
    title={Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation}, 
    author={Bin Zhang and Yuxiao Ye and Guoqing Du and Xiaoru Hu and Zhishuai Li and Sun Yang and Chi Harold Liu and Rui Zhao and Ziyue Li and Hangyu Mao},
    year={2024},
    eprint={2403.02951},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2403.02951},
    abstract={Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions. To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL this http URL study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer valuable insights for enhancing the development of LLM-based Text-to-SQL systems.}
}

@misc{T2SQL-LLM-Bench-3,
    title={Evaluating the Text-to-SQL Capabilities of Large Language Models}, 
    author={Nitarshan Rajkumar and Raymond Li and Dzmitry Bahdanau},
    year={2022},
    eprint={2204.00498},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2204.00498},
    abstract={We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.}
}

@misc{LLM-SQL,
    title={Evaluating SQL Understanding in Large Language Models}, 
    author={Ananya Rahaman and Anny Zheng and Mostafa Milani and Fei Chiang and Rachel Pottinger},
    year={2024},
    eprint={2410.10680},
    archivePrefix={arXiv},
    primaryClass={cs.DB},
    url={https://arxiv.org/abs/2410.10680},
    abstract={The rise of large language models (LLMs) has significantly impacted various domains, including natural language processing (NLP) and image generation, by making complex computational tasks more accessible. While LLMs demonstrate impressive generative capabilities, there is an ongoing debate about their level of "understanding," particularly in structured domains like SQL. In this paper, we evaluate the extent to which LLMs "understand" SQL by testing them on a series of key SQL tasks. These tasks, such as syntax error detection, missing token identification, query performance prediction, query equivalence checking, and query explanation, assess the models' proficiency in recognition, context awareness, semantics, and coherence, which are essential skills for SQL understanding. We generate labeled datasets from well-known workloads, and evaluate the latest LLMs, focusing on how query complexity and syntactic features influence performance. Our results indicate that while GPT4 excels at tasks requiring recognition and context, all models struggle with deeper semantic understanding and coherence, especially in query equivalence and performance estimation, revealing the limitations of current LLMs in achieving full SQL comprehension.},
}

@misc{SPIDER2,
    title={Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows}, 
    author={Fangyu Lei and Jixuan Chen and Yuxiao Ye and Ruisheng Cao and Dongchan Shin and Hongjin Su and Zhaoqing Suo and Hongcheng Gao and Wenjing Hu and Pengcheng Yin and Victor Zhong and Caiming Xiong and Ruoxi Sun and Qian Liu and Sida Wang and Tao Yu},
    year={2025},
    eprint={2411.07763},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2411.07763},
    abstract={Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics. We introduce Spider 2.0, an evaluation framework comprising 632 real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake. We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding 100 lines, which goes far beyond traditional text-to-SQL challenges. Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 21.3% of the tasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation -- especially in prior text-to-SQL benchmarks -- they require significant improvement in order to achieve adequate performance for real-world enterprise usage. Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings.},
}

@inproceedings{NL2SQLUnsolved,
    author={Avrilia Floratou and Fotis Psallidas and Fuheng Zhao and Shaleen Deep and Gunther Hagleither and Wangda Tan and Joyce Cahoon and Rana Alotaibi and Jordan Henkel and Abhik Singla and Alex Van Grootel and Brandon Chow and Kai Deng and Katherine Lin and Marcos Campos and K. Venkatesh Emani and Vivek Pandit and Victor Shnayder and Wenjing Wang and Carlo Curino},
    title={NL2SQL is a solved problem... Not!},
    year={2024},
    cdate={1704067200000},
    url={https://www.cidrdb.org/cidr2024/papers/p74-floratou.pdf},
    booktitle={CIDR},
    crossref={conf/cidr/2024},
    abstract={The development of natural language (NL) interfaces for databases has been notably shaped by the rise of Large Language Models (LLMs), which provide an easy way to automate the translation of NL queries into structured SQL queries. While LLMs bring valuable technical advancements, this paper stresses that achieving Enterprise-Grade NL2SQL is still far from being resolved, necessitating extensive novel research in various domains. We present insights from two competing teams dedicated to delivering reliable enterprise-grade NL2SQL technology, shedding light on challenges faced in real-world applications, including handling complex schemata, dealing with ambiguity in natural language statements, and incorporating it in our benchmarking methodologies and responsible AI considerations. While this paper may raise more questions than it answers, its aim is to act as a catalyst for a fruitful discussion on the topic. Additionally, it provides a practical pathway for the community to develop enterprise-grade NL2SQL solutions.}
}

% Recent - LLM - Models

@misc{ModelDomainAdaption,
    title={Domain Adaptation of a State of the Art Text-to-SQL Model: Lessons Learned and Challenges Found}, 
    author={Irene Manotas and Octavian Popescu and Ngoc Phuoc An Vo and Vadim Sheinin},
    year={2023},
    eprint={2312.05448},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2312.05448},
    abstract={There are many recent advanced developments for the Text-to-SQL task, where the Picard model is one of the the top performing models as measured by the Spider dataset competition. However, bringing Text-to-SQL systems to realistic use-cases through domain adaptation remains a tough challenge. We analyze how well the base T5 Language Model and Picard perform on query structures different from the Spider dataset, we fine-tuned the base model on the Spider data and on independent databases (DB). To avoid accessing the DB content online during inference, we also present an alternative way to disambiguate the values in an input question using a rule-based approach that relies on an intermediate representation of the semantic concepts of an input question. In our results we show in what cases T5 and Picard can deliver good performance, we share the lessons learned, and discuss current domain adaptation challenges.},
}

@misc{CodeS,
    title={CodeS: Towards Building Open-source Language Models for Text-to-SQL}, 
    author={Haoyang Li and Jing Zhang and Hanbing Liu and Ju Fan and Xiaokang Zhang and Jun Zhu and Renjie Wei and Hongyan Pan and Cuiping Li and Hong Chen},
    year={2024},
    eprint={2402.16347},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2402.16347},
    abstract={Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique. We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and this http URL, as well as two real-world datasets created for financial and academic applications. The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks.},
}

% Recent - LLM

@article{PICARD,
    author = {Torsten Scholak and Nathan Schucher and Dzmitry Bahdanau},
    title = {{PICARD:} Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models},
    journal = {CoRR},
    volume = {abs/2109.05093},
    year = {2021},
    url = {https://arxiv.org/abs/2109.05093},
    eprinttype = {arXiv},
    eprint = {2109.05093},
    abstract={Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code and trained models available at this https URL), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.},
    timestamp = {Tue, 21 Sep 2021 17:46:04 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2109-05093.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}

@misc{PromptingTechniques,
    title={How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings}, 
    author={Shuaichen Chang and Eric Fosler-Lussier},
    year={2023},
    eprint={2305.11853},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2305.11853},
    abstract={Large language models (LLMs) with in-context learning have demonstrated remarkable capability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and intermediate reasoning steps to enhance the performance of LLMs. However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across various settings and provide insights into prompt constructions for future text-to-SQL studies.},
}

@misc{DBGPT,
    title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, 
    author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},
    year={2024},
    eprint={2312.17449},
    archivePrefix={arXiv},
    primaryClass={cs.DB},
    url={https://arxiv.org/abs/2312.17449},
    abstract={The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software. Database technologies particularly have an important entanglement with LLMs as efficient and intuitive database interactions are paramount. In this paper, we present DB-GPT, a revolutionary and production-ready project that integrates LLMs with traditional database systems to enhance user experience and accessibility. DB-GPT is designed to understand natural language queries, provide context-aware responses, and generate complex SQL queries with high accuracy, making it an indispensable tool for users ranging from novice to expert. The core innovation in DB-GPT lies in its private LLM technology, which is fine-tuned on domain-specific corpora to maintain user privacy and ensure data security while offering the benefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, which includes a novel retrieval augmented generation (RAG) knowledge system, an adaptive learning mechanism to continuously improve performance based on user feedback and a service-oriented multi-model framework (SMMF) with powerful data-driven agents. Our extensive experiments and user studies confirm that DB-GPT represents a paradigm shift in database interactions, offering a more natural, efficient, and secure way to engage with data repositories. The paper concludes with a discussion of the implications of DB-GPT framework on the future of human-database interaction and outlines potential avenues for further enhancements and applications in the field. The project code is available at this https URL. Experience DB-GPT for yourself by installing it with the instructions this https URL and view a concise 10-minute video at this https URL.},
}

% Recent - NLP

@article{GRAPPA,
    author = {Tao Yu and Chien{-}Sheng Wu and Xi Victoria Lin and Bailin Wang and Yi Chern Tan and Xinyi Yang and Dragomir R. Radev and Richard Socher and Caiming Xiong},
    title = {GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing},
    journal = {CoRR},
    volume = {abs/2009.13845},
    year = {2020},
    url = {https://arxiv.org/abs/2009.13845},
    eprinttype = {arXiv},
    eprint = {2009.13845},
    timestamp = {Fri, 27 Dec 2024 10:33:44 +0100},
    biburl = {https://dblp.org/rec/journals/corr/abs-2009-13845.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    abstract={We present GraPPa, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG) induced from existing text-to-SQL datasets. We pre-train our model on the synthetic data using a novel text-schema linking objective that predicts the syntactic role of a table field in the SQL for each question-SQL pair. To maintain the model's ability to represent real-world data, we also include masked language modeling (MLM) over several existing table-and-language datasets to regularize the pre-training process. On four popular fully supervised and weakly supervised table semantic parsing benchmarks, GraPPa significantly outperforms RoBERTa-large as the feature representation layers and establishes new state-of-the-art results on all of them.},
}

@article{STRUG,
    author = {Xiang Deng and Ahmed Hassan Awadallah and Christopher Meek and Oleksandr Polozov and Huan Sun and Matthew Richardson},
    title = {Structure-Grounded Pretraining for Text-to-SQL},
    journal = {CoRR},
    volume = {abs/2010.12773},
    year = {2020},
    url = {https://arxiv.org/abs/2010.12773},
    eprinttype = {arXiv},
    eprint = {2010.12773},
    timestamp = {Wed, 31 May 2023 19:02:38 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2010-12773.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    abstract={Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (StruG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel prediction tasks: column grounding, value grounding and column-value mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing text-to-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERT-LARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. The Spider-Realistic dataset is available at this https URL.},
}

% Recent - Techniques

@misc{RetAug,
      title={Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning}, 
      author={Zhili Shen and Pavlos Vougiouklis and Chenxin Diao and Kaustubh Vyas and Yuanyi Ji and Jeff Z. Pan},
      year={2024},
      eprint={2407.03227},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.03227},
      abstract={We focus on Text-to-SQL semantic parsing from the perspective of retrieval-augmented generation. Motivated by challenges related to the size of commercial database schemata and the deployability of business intelligence solutions, we propose ASTReS that dynamically retrieves input database information and uses abstract syntax trees to select few-shot examples for in-context learning.
Furthermore, we investigate the extent to which an in-parallel semantic parser can be leveraged for generating approximated versions of the expected SQL queries, to support our retrieval. We take this approach to the extreme--we adapt a model consisting of less than 500M parameters, to act as an extremely efficient approximator, enhancing it with the ability to process schemata in a parallelised manner. We apply ASTReS to monolingual and cross-lingual benchmarks for semantic parsing, showing improvements over state-of-the-art baselines. Comprehensive experiments highlight the contribution of modules involved in this retrieval-augmented generation setting, revealing interesting directions for future work.},
}

@inproceedings{10.5555/3618408.3618843,
    author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
    title = {PAL: program-aided language models},
    year = {2023},
    publisher = {JMLR.org},
    abstract = {Large language models (LLMs) have demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought", which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and others. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using CODEX achieves state-of-the-art few-shot accuracy on GSM8K, surpassing PaLM-540B which uses chain-of-thought by absolute 15\% top-1.},
    booktitle = {Proceedings of the 40th International Conference on Machine Learning},
    articleno = {435},
    numpages = {36},
    location = {Honolulu, Hawaii, USA},
    series = {ICML'23}
}

@inproceedings{10.5555/3666122.3667699,
    author = {Pourreza, Mohammadreza and Rafiei, Davood},
    title = {DIN-SQL: decomposed in-context learning of text-to-SQL with self-correction},
    year = {2023},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {There is currently a significant gap between the performance of fine-tuned models and prompting approaches using Large Language Models (LLMs) on the challenging task of text-to-SQL, as evaluated on datasets such as Spider. To improve the performance of LLMs in the reasoning process, we study how decomposing the task into smaller sub-tasks can be effective. In particular, we show that breaking down the generation problem into sub-problems and feeding the solutions of those sub-problems into LLMs can be an effective approach for significantly improving their performance. Our experiments with three LLMs show that this approach consistently improves their simple few-shot performance by roughly 10\%, pushing the accuracy of LLMs towards SOTA or surpassing it. On the holdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9 and the new SOTA at the time of this writing using our approach is 85.3. Our approach with in-context learning beats many heavily fine-tuned models by at least 5\%. Additionally, when evaluated on the BIRD benchmark, our approach achieved an execution accuracy of 55.9\%, setting a new SOTA on its holdout test set.},
    booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
    articleno = {1577},
    numpages = {10},
    location = {New Orleans, LA, USA},
    series = {NIPS '23}
}

@misc{CoE-SQL,
    title={CoE-SQL: In-Context Learning for Multi-Turn Text-to-SQL with Chain-of-Editions}, 
    author={Hanchong Zhang and Ruisheng Cao and Hongshen Xu and Lu Chen and Kai Yu},
    year={2024},
    eprint={2405.02712},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2405.02712},
    abstract={Recently, Large Language Models (LLMs) have been demonstrated to possess impressive capabilities in a variety of domains and tasks. We investigate the issue of prompt design in the multi-turn text-to-SQL task and attempt to enhance the LLMs' reasoning capacity when generating SQL queries. In the conversational context, the current SQL query can be modified from the preceding SQL query with only a few operations due to the context dependency. We introduce our method called CoE-SQL which can prompt LLMs to generate the SQL query based on the previously generated SQL query with an edition chain. We also conduct extensive ablation studies to determine the optimal configuration of our approach. Our approach outperforms different in-context learning baselines stably and achieves state-of-the-art performances on two benchmarks SParC and CoSQL using LLMs, which is also competitive to the SOTA fine-tuned models.},
}

@misc{MAGIC,
    title={MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL}, 
    author={Arian Askari and Christian Poelitz and Xinye Tang},
    year={2024},
    eprint={2406.12692},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2406.12692},
    abstract={Self-correction in text-to-SQL is the process of prompting large language model (LLM) to revise its previously incorrectly generated SQL, and commonly relies on manually crafted self-correction guidelines by human experts that are not only labor-intensive to produce but also limited by the human ability in identifying all potential error patterns in LLM responses. We introduce MAGIC, a novel multi-agent method that automates the creation of the self-correction guideline. MAGIC uses three specialized agents: a manager, a correction, and a feedback agent. These agents collaborate on the failures of an LLM-based method on the training set to iteratively generate and refine a self-correction guideline tailored to LLM mistakes, mirroring human processes but without human involvement. Our extensive experiments show that MAGIC's guideline outperforms expert human's created ones. We empirically find out that the guideline produced by MAGIC enhances the interpretability of the corrections made, providing insights in analyzing the reason behind the failures and successes of LLMs in self-correction. All agent interactions are publicly available at this https URL.},
}

@misc{CHASE,
    title={CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL}, 
    author={Mohammadreza Pourreza and Hailong Li and Ruoxi Sun and Yeounoh Chung and Shayan Talaei and Gaurav Tarlok Kakkar and Yu Gan and Amin Saberi and Fatma Ozcan and Sercan O. Arik},
    year={2024},
    eprint={2410.01943},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2410.01943},
    abstract={In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs' intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test this http URL identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0% and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission).},
}

@misc{XiYan,
    title={A Preview of XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL}, 
    author={Yingqi Gao and Yifu Liu and Xiaoxia Li and Xiaorong Shi and Yin Zhu and Yiming Wang and Shiqi Li and Wei Li and Yuntao Hong and Zhiling Luo and Jinyang Gao and Liyu Mou and Yu Li},
    year={2025},
    eprint={2411.08599},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2411.08599},
    abstract={To tackle the challenges of large language model performance in natural language to SQL tasks, we introduce XiYan-SQL, an innovative framework that employs a multi-generator ensemble strategy to improve candidate generation. We introduce M-Schema, a semi-structured schema representation method designed to enhance the understanding of database structures. To enhance the quality and diversity of generated candidate SQL queries, XiYan-SQL integrates the significant potential of in-context learning (ICL) with the precise control of supervised fine-tuning. On one hand, we propose a series of training strategies to fine-tune models to generate high-quality candidates with diverse preferences. On the other hand, we implement the ICL approach with an example selection method based on named entity recognition to prevent overemphasis on entities. The refiner optimizes each candidate by correcting logical or syntactical errors. To address the challenge of identifying the best candidate, we fine-tune a selection model to distinguish nuances of candidate SQL queries. The experimental results on multiple dialect datasets demonstrate the robustness of XiYan-SQL in addressing challenges across different scenarios. Overall, our proposed XiYan-SQL achieves the state-of-the-art execution accuracy of 75.63% on Bird benchmark, 89.65% on the Spider test set, 69.86% on SQL-Eval, 41.20% on NL2GQL. The proposed framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods.},
}