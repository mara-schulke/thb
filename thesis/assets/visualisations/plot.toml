# Plot configurations for thesis visualizations

[[plots]]
type = "benchmark"
benchmark = "spider-dev"
metric = "execution-accuracy"
metric_label = "Execution Accuracy"
output = "spider-dev.svg"

[[plots]]
type = "benchmark"
benchmark = "spider-test"
metric = "execution-accuracy"
metric_label = "Execution Accuracy"
output = "spider-test.svg"

[[plots]]
type = "benchmark"
benchmark = "bird-dev"
metric = "execution-accuracy"
metric_label = "Execution Accuracy"
output = "bird-dev.svg"

[[plots]]
type = "metric_comparison"
metric = "execution-accuracy"
metric_label = "Execution Accuracy"
output = "comparison.svg"

[[plots]]
type = "relative_improvement"
baseline_key = "natural-baseline"
comparison_keys = ["natural-zeroshot", "natural-full-syn", "natural-full-train", "natural-full-ground"]
metric = "execution-accuracy"
title = "Relative Performance Improvement Over Baseline"
output = "relative-improvement.svg"

[[plots]]
type = "metric_comparison_bars"
metric = "execution-accuracy"
metric_label = "Execution Accuracy"
include_sota = false
output = "comparison-bars-natural.svg"

[[plots]]
type = "sota_comparison"
natural_configs = ["natural-baseline", "natural-full-ground"]
sota_configs = ["omnisql-7b-ext", "gpt-4o"]
metric = "execution-accuracy"
title = "Performance Comparison: Natural vs SOTA"
output = "sota-comparison.svg"

[[plots]]
type = "example_source"
source_keys = ["natural-full-syn", "natural-full-train", "natural-full-ground"]
metric = "execution-accuracy"
title = "Impact of Example Source on Performance"
output = "example-source-comparison.svg"
