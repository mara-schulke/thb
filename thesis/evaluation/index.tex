\section{Evaluation}

The evaluation phase of \Natural~builds upon the benchmarking
infrastructure outlined in section~\ref{impl:benchmark}. This section is
evaluating the performance metrics recorded while running \Natural in
different configurations on two prevalent benchmarking datasets:
\Spider and \Bird. A representative baseline is introduced,
aiming to mirror the base-model performance. Subsequently an ablation study is
performed to understand the performance impact of individual pipeline components.

\subsection{Methodology}

The following sections document the experimentation environment, outline
tradeoffs and constraints influencing the performed evaluations.

\subsubsection{Experimentation Environment}

All benchmarks are performed on a consumer-grade linux system, as research- or
enterprise-grade hardware was not available. Only a subset of commonly used
datasets and configurations have been therefore measured, as a single benchmark
executions of \Spider~and~\Bird take between 12-36 hours. 

\paragraph{Hardware Configuration}

The host system used for benchmarking has a NVIDIA RTX 3090 GPU with 24GB VRAM
available, an AMD 9990X3D CPU with 12-cores up to 5.5GHz, 64GB of RAM at
6400mt/s and 1TB of SSD storage at a read speed of 7450 MB/s and a write speed
of 6900 MB/s.

\paragraph{Software Stack}

The software stack running on the system used for benchmarking is NixOS (25.05)
with a recent linux kernel version (6.17.7). CUDA 13.0 is being used for
inference on GPUs and Rust 1.91 for compiling
\Natural. The models used during benchmarking are \textsc{OmniSql 7B
Q8\_0} and \textsc{Qwen3 Embedding 8B Q4\_K\_M} for generation and embedding
respectively.

\subsubsection{Benchmark Datasets}

\Spider and \Bird were used as benchmarking datasets as they are prevalently
used for the evaluation of NL2SQL systems and therefore enable cross study
evaluations (see section \ref{lit:benchmarks:spider} and
\ref{lit:benchmarks:bird}). The complex \Spider~2.0 version of \Spider~was
excluded from this evaluation due to the lack of time and hardware required to
develop a competitive approach (see section \ref{lit:benchmarks:spider2}).
Comparable research publishes \textsc{Execution Accuracy} and \textsc{Exact
Match} metrics on these benchmarks which enables relative and absolute
performance comparisons of \textsc{Natural} with other studies.

\subsubsection{Evaluation Metrics}

The chosen set of evaluation metrics used is a mixture of semantic metrics
(\EA, \EM) and functional metrics (\ER, \CL) to gain understanding on system
behavior and real world applicability.

\paragraph{Execution Accuracy (\EA)}

Execution Accuracy (or \EA) is the primary success metric of
NL2SQL systems as it represents the semantic accuracy of the SQL queries
produced NL2SQL systems. Execution accuracy is computed by determining whether
the rows in the candidate results are a permutation of the rows returned by
ground truth results. 

For a query $q_g$ with candidate results $R_c$ and ground truth results $R_g$, 
where $R_c, R_g \subseteq \mathbb{V}^{n \times m}$ (sets of $n$ rows with $m$ columns 
of values $\mathbb{V}$), semantically accurate execution is defined as:

\begin{equation}
    semanticeq(R_c, R_g, q_g) = \mathbb{1}\left[\exists \pi \in \Pi_m : 
    \begin{cases}
        R_c = \pi(R_g) & \text{if } ordered(q_g) \\
        R_c \equiv_{\text{multiset}} \pi(R_g) & \text{otherwise}
    \end{cases}
    \right]
\end{equation}

where $\Pi_m$ is the set of valid column permutations (those preserving column
value sets), $\pi(R_g)$ applies permutation $\pi$ to reorder columns in each
row of $R_g$, $\equiv_{\text{multiset}}$ denotes multiset equality (allowing
row reordering) and $ordered(q_g)$ is determining if the ground truth query
$q_g$ contains an \texttt{ORDER BY} clause.

In order to compute the \EA on \Spider and \Bird the official evaluation suite
is used respectively to ensure comparability across externally reported
measurements and locally observed measurements.

\paragraph{Exact Match (\EM)}

Exact Match (or \EM) measures the syntactic equivalence between
candidate and ground truth SQL queries after normalization (ie, whitespace
trimming, casing adjustments etc). Unlike execution accuracy which validates
semantic correctness through result comparison, exact match determines whether
the generated query structurally matches the reference query. \EM acts as a
lower bound for execution accuracy as queries must be identical which is
therefore more restrictive as semantically equivalent queries with different
syntax (ie, using a different join order) are marked as incorrect.

\paragraph{Error Rate (\ER)}

Error Rate (or \ER) describes the system reliability by measuring the
frequency of SQL generation failures that prevent query execution. \ER is
reported in failures per hundred queries and monitors system reliability
(schema violations, syntax errors, out-of-memory errors etc).

\paragraph{Candidate Latency (\CL)}

Candidate Latency (or \CL) measures the end-to-end execution
time of a system from natural language input to SQL candidate output, reported
in seconds. This metric captures the compound runtime of all pipeline
components ($\sigma, \phi, \pi, \rho, \nu$) and reflects real-world system
responsiveness. Latency increases with pipeline complexity, particularly when
example selection and self refinement are applied, thus representing relative
computational cost.

\subsubsection{Baselines}

To adequately determine the performance impact of methods and components
applied in \Natural a baseline helps to measure relative performance
gains or losses compared to existing systems. Therefore two sets of baseline
systems are introduced: Internal baselines (measuring the system without
crucial components) and external baselines (eg, proprietary system
performance and raw model performance).

\paragraph{Internal Baselines}

Introducing two internal baselines subsequently allows for a brief ablation
study, where the impact of different pipeline configurations and different
sampling datasets can be measured to isolate the contribution of each
configuration. The \textit{Baseline} configuration of \Natural is using
only the inference logic ($\pi$) without all other pipeline components,
representing the performance contribution of \textsc{OmniSQL} as closely as
possible. Additionally the \textit{Zero-Shot} configuration of \Natural
is introduced and measures the performance of \Natural with all
pipeline components activated but without any examples available to use during
in-context learning.

\paragraph{External Baselines}

Using published results from other systems, \Natural can be briefly
compared against existing systems with similar capabilities highlighting
relative performance improvements or losses. Notably external baselines largely
rely on unverified data from other papers. Given that \Natural is
largely based on \OmniSQL, GPT-4 and \OmniSQL are the primary
external systems are taken into account as baselines.

\subsubsection{Vector Databases}

Three different vector databases are introduced per benchmark. These differ in
the datasets using during sampling:

\paragraph{Synthetic}

The \textit{Synthetic} vector databases are sampled from the
\textsc{SynSQL-2.5m} dataset introduced by \citeauthor{OmniSQL} in
\citeyear{OmniSQL}. The \textsc{SynSQL-2.5m} dataset is largely unrelated to
\textsc{Spider} and \textsc{Bird} but covers a wide array of domains through
its LLM-based synthetic data generation approach.

\paragraph{Train}

The \textit{Train} vector databases are sampled from the respective training
splits from \textsc{Spider} and \textsc{Bird} \citep{Spider, BIRD}. The train
splits include similar style of SQL queries and similar domains but different
databases and different questions.

\paragraph{Ground}

The \textit{Ground} vector databases are sampled from the respective splits
used during evaluation (dev and test) from \textsc{Spider} and \textsc{Bird}
\citep{Spider, BIRD}. These allow \textsc{Natural} to reference ``previously
used'' answers as examples during generation indicating the potential
upper-limit of performance during a self-learning deployment which has access
to past conversations.

\subsection{Benchmark Results}

This section is presenting the measured performance across all \textsc{Natural}
configurations and benchmark datasets. Internal ablations are compared
(Baseline, Zero-Shot, as well as different vector DBs) against external prior
art (GPT-4, \OmniSQL). Subsequently performance characteristics and improvement
trends are quantitatively and qalitatively analyzed.

\subsubsection{Overview}

The overall benchmarking results are presented in
Table~\ref{tab:eval:overall-results} and visualized in
Figures~\ref{fig:eval:execution-accuracy-overview}
and~\ref{fig:eval:exact-match-overview}, which displays the the performance for
all system configurations across the \textsc{Spider} and \textsc{Bird}
datasets, the execution accuracy ($\mathbb{EA}$), exact match ($\mathbb{EM}$),
error rate ($\mathbb{ER}$) and candidate latency
($\mathbb{CL}$) metrics.

\begin{table}[ht]
    \centering
    \hspace*{-1.25mm}
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
        \toprule
        \textbf{System}     & \multicolumn{4}{c|}{\textbf{Spider (dev)}}                       & \multicolumn{4}{c|}{\textbf{Spider (test)}}                       & \multicolumn{4}{c}{\textbf{BIRD (dev)}}                            \\
        \midrule                                                                                                                                                 
                            & \EA             & \EM            & \ER           & \CL           & \EA             & \EM          & \ER              & \CL            & \EA           & \EM              & \ER            & \CL           \\
        \midrule
        \multicolumn{13}{c}{\textbf{Closed-Source LLMs}} \\                                                                                                                                                                          
        \midrule                                                                                                                                                                                                                     
        GPT-4o-mini*        & 71.0            & -              & -             & -             & 83.7           & -             & -                & -              & 61.5          & -                & -              & -             \\
        GPT-4-Turbo*        & 72.2            & -              & -             & -             & 84.2           & -             & -                & -              & 63.6          & -                & -              & -             \\
        GPT-4o*             & 70.7            & -              & -             & -             & 84.9           & -             & -                & -              & 64.0          & -                & -              & -             \\
        \midrule
        \multicolumn{13}{c}{\textbf{Open-Source LLMs}} \\
        \midrule
        OmniSQL-7B*         & 81.6            & -              & -             & -             & 89.8           & -             & -                & -              & 66.1          & -                & -              & -             \\
        OmniSQL-14B*        & 82.0            & -              & -             & -             & 88.3           & -             & -                & -              & 65.9          & -                & -              & -             \\
        OmniSQL-32B*        & 80.9            & -              & -             & -             & 89.8           & -             & -                & -              & 67.0          & -                & -              & -             \\
        OmniSQL-7B-gguf     & 79.0            & 33.0           & 0.1           & \textbf{6.6s} & 79.0           & 35.7          & 0.3              & \textbf{6.7s}  & 38.0          & 0.03             & 0.8            & \textbf{8.2s} \\
        \midrule
        \multicolumn{13}{c}{\textbf{Pipelines}} \\                                                                                                                                                                          
        \midrule
        \Natural~(Baseline)  & 77.9           & 30.3           & \textbf{0.0}  & 7.3s          & 77.0           & 32.5          & \textbf{0.0}     & 7.3s           & 32.4          & 0.03             & \textbf{0.3}   & 9.3s          \\
        \Natural~(Zero-Shot) & 78.7           & 27.1           & 0.2           & 14.0s         & 77.5           & 32.7          & 0.4              & 15.3s          & 34.0          & 0.03             & 0.4            & 18.7s         \\
        \Natural~(Syn)       & \textbf{81.0}  & 41.2           & 0.3           & 16.3s         & 79.6           & 38.7          & 0.4              & 15.4s          & \textbf{53.8} & \textbf{18.8}    & 0.2            & 43.7s         \\
        \Natural~(Train)     & 80.4           & \textbf{42.2}  & 0.6           & 16.2s         & \textbf{81.4}  & \textbf{43.9} & 0.3              & 15.8s          & 48.4          & 13.2             & 0.3            & 33.7s         \\
        \Natural~(Ground)**  & 84.2           & 42.7           & 0.7           & 16.1s         & 82.8           & 39.5          & 0.3              & 16.0s          & 55.8          & 19.3             & 0.2            & 43.8s         \\
        \bottomrule
    \end{tabular}
    \caption{
        Comprehensive benchmark results across all systems and datasets.
        \EA~and \EM~are reported as percentages. \ER~is reported in failures
        per hundred queries and \CL~is reported in seconds. Systems marked with
        * are external benchmarks with unverified results from published
        papers. Systems marked with ** had access to ground truth data during
        inference, illustrating the upper bound achievable. Bold values
        indicate best verified performance, excluding systems with access to
        ground truth. \citep{OmniSQL}
    }
    \label{tab:eval:overall-results}
\end{table}

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/execution-accuracy-overview.svg}
  \caption{
    Execution accuracy overview across all systems and benchmarks. The
    \textit{Syn} configuration achieves the highest verified performance on
    \Spider~(dev) at 81.0\% and \Bird~(dev) at 53.8\%, while \textit{Train} leads
    on \Spider~(test) at 81.4\%.
  }
  \label{fig:eval:execution-accuracy-overview}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/exact-match-overview.svg}
  \caption{
    Exact match overview across all systems and benchmarks. ICL configurations
    demonstrate substantial improvements over baselines, with \textit{Train}
    achieving 43.9\% on \Spider~(test), the highest verified exact match rate
    across all benchmarks.
  }
  \label{fig:eval:exact-match-overview}
\end{figure}

\subsubsection{Baseline Performance Analysis}

One notable insight during benchmarking was the apparent gap between the
published and the locally measured performance of \OmniSQL~7B (using GGUF with the F16
weights) and the reported accuracy metrics results from \cite{OmniSQL}.
As shown in Figure~\ref{fig:eval:omnisql-gap}, the $\Delta$ in performance
between the local measurement and the official values ranges from 2.6\% on
\Spider~(dev) to 11.8\% on \Spider~(test) and 28.1\% on \Bird~(dev). The
section performance gap section (\ref{sec:eval:performance-gap}) analyzes this
gap in detail.

The \textit{Baseline} configuration of \Natural~further showed degradation of
performance compared to the OmniSQL-7B-gguf system configuration. Using the
\textit{Baseline} configuration, \Natural~only uses the inference logic and
prompt template for ICL ($\pi$) without other pipeline components, achieving
77.9\% \EA~on \Spider~(dev), 77.0\% on \Spider~(test), and 32.4\% on \Bird
(dev). These results further fall below the published \textsc{OmniSQL-7B}
performance of 81.6\%, 89.8\%, and 66.1\% respectively, representing gaps of
3.7, 12.8, and 33.7 percentage points. This continued degradation of
performance indicates that a misaligned prompt template (ie, divergent from
training phase) harms performance if not paired with counter measures such as
ICL, self-correction or majority voting. These performance numbers mark the
\textit{Baseline} configuration the worst performing variant of \Natural~on
\EA~and \EM~across \Spider~(dev and test) and \Bird~(dev).

\subsubsection{Spider Results}

The results of the \Spider~benchmarks demonstrate consistent performance
characteristics of \Natural~and \OmniSQL~across the development and test splits
(detailed breakdown in Appendix~\ref{sec:appendix:benchmark-results-spider}).
The OmniSQL-7B-gguf system outperforms the two \Natural~baseline configurations
\textit{Baseline} and \textit{Zero-Shot} in \EA~and \EM by 2.1\% and 0.3\% on
the development split and by 2.0\% and 1.5\% on the test split respectively.

The full pipeline with the \textit{Zero-Shot} configuration (all components
active but no examples) hence only yields marginal improvements (+0.8pp on dev,
+0.5pp on test) over \textit{Baseline}, suggesting that schema subsetting and
query refinement contribute minimally without example-based guidance. Notably,
as demonstrated in Figure~\ref{fig:eval:candidate-latency}, the \CL of
\textit{Zero-Shot} increases by 6.7s on dev and 8.0s on test over
\textit{Baseline}, doubling the system latency without a significant
improvement in system performance.

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/candidate-latency-spider.svg}
  \caption{
    Candidate latency across system configurations on \Spider~benchmarks. The
    \textit{Zero-Shot} configuration shows doubled latency (14.0s on dev, 15.3s
    on test) compared to \textit{Baseline} (7.3s) without commensurate
    performance gains, while ICL configurations incur additional overhead from
    example retrieval.
  }
  \label{fig:eval:candidate-latency}
\end{figure}

Introducing vector database access to the system configurations demonstrates
substantial improvement in performance. The two configurations without ground
truth examples showed a significant improvement in both \EA~and \EM:
\textit{Syn} outperformed all other system variants and baselines with a
accuracy improvement of 3.1\% over \textit{Baseline} and 2.0\% over
OmniSQL-7B-gguf on \Spider~(dev). On \Spider~(test) the \textit{Train}
configuration outperformed \textit{Syn} by 1.8\% in \EA. For \EM~both system
configurations show a steep improvement over the three baselines with largest
\EM~deltas being observed between \textit{Zero-Shot} and \textit{Train} with
+15.1\% on \Spider~(dev) and between \textit{Baseline} and \textit{Train} with
+21.4\% on \Spider~(test). These double digit improvements in accuracy point
towards the conclusion that in-context learning is an effective mechanism for
further improving the performance of already fine-tuned models. The performance
of the \textit{Ground} configuration shows the theoretical upper limits in
\EA~achievable on top of \OmniSQL~with example selection algorithms with 84.2\%
on \Spider~(dev) and 82.8\% on \Spider~(test)
(see~\ref{sec:appendix:benchmark-results-spider}).

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/baseline-improvement-ea.svg}
  \caption{
    Execution accuracy improvement over the \textit{Baseline} configuration
    across all benchmarks. The \textit{Zero-Shot} configuration yields marginal
    gains ($\leq$1.6pp), while ICL-enabled configurations show substantial
    improvements peaking at +21.4pp (\textit{Syn} on \Bird~dev). The stark
    contrast between \Spider~and \Bird~gains highlights that ICL impact scales
    with query and schema complexity.
  }
  \label{fig:eval:baseline-improvement-ea}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/error-rates.svg}
  \caption{
      Error rates across system configurations. All configurations
      maintain error rates below 0.7 failures per hundred queries, indicating
      that failures are predominantly semantic (incorrect results) rather than
      syntactic or runtime related (malformed SQL, out of memory errors).
  }
  \label{fig:eval:error-rates}
\end{figure}

Error rates remain consistently low across all configurations (<0.7 failures
per hundred queries), see ~\ref{fig:eval:error-rates}, indicating that failures
are predominantly semantic (incorrect results) rather than syntactic (malformed
SQL), which validates the robustness of \OmniSQL's SQL generation capabilities.

\subsubsection{BIRD Results}

Measurements on the \Bird benchmark, which is designed to represent real-world
scenarios with imperfect data, external knowledge requirements and hard to
reason about database schemas yields significantly lower absolute performance
numbers but shows fundamental improvements through ICL based systems.
While the \textit{Baseline} configuration of \Natural~only achieves 32.4\%
\EA~(less than half the accuracy than on \Spider~with 77.9\% and 77.0\% for dev
and test splits respectively). This poor baseline performance confirms \Bird
being a significantly more challenging benchmark. While the \textit{Zero-Shot}
configuration shows incremental improvements over the baseline with a relative
improvement of +1.6pp in \EA~(34.0\% absolute), the overall performance shows
to be behind the reported numbers of \OmniSQL~(7B) and GPT-4 by \cite{OmniSQL}
which were reported to had an \EA of 66.1\% and 61.5\% respectively. The
locally reproduced measurement of \OmniSQL~(7B-gguf) yielded a drop in \EA~of
-28.1pp compared to the officially reported \EA~yielding 38.0\% in absolute
accuracy.

Both the \textit{Syn} and \textit{Train} configurations of \Natural~outperform
all baselines. \textit{Syn} shows an improvement of +21.4pp over
\textit{Baseline} and +15.8pp over \OmniSQL~(7B-gguf), achieving 53.8\% \EA,
the best local measured system excluding \textit{Ground}. \textit{Train} yields
improvements of +16.0pp over \textit{Baseline} and +10.4pp over \OmniSQL~(7B-gguf).

This shows a stark contrast with \textsc{Spider}, where synthetic examples
provided only +3.1pp improvement on dev and +2.6pp improvement on train.
The effectiveness of synthetic examples on \Bird~suggests that the impact of
ICL increases with query and schema complexity. When queries involve complex
joins, nested subqueries, and aggregations, even domain-agnostic examples
provide crucial scaffolding for the generation. The \textit{Train}
configuration of \Natural~underperformed \textit{Syn} by 5.4pp. This may
indicate that \textsc{Bird}'s training examples, while domain-relevant, contain
different structural patterns than the development set, whereas
\textsc{SynSQL-2.5m} contains a broader coverage of different database schema
structures to reference from.

The other metrics \EM, \ER~and \CL~further show a stark difference between
\Spider~and \Bird. Systems which mostly relied on \OmniSQL~(\Natural
\textit{Baseline} and \textit{Zero-Shot} and \OmniSQL~(7B-gguf)) showed the
same 0.03\% in \EM indicating a potential overfitting of \OmniSQL~to the SQL
style used in \Spider. Notably ICL based systems showed a significant
improvement with +13.17pp for \textit{Train} and +18.77pp for \textit{Syn}.
The \CL~metric showed a similar pattern to the \Spider~benchmarks with the \CL
increasing with pipeline complexity from 9.3s for \textit{Baseline} to 43.8s
for \textit{Ground}.

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/candidate-latency-bird.svg}
  \caption{
    Candidate latency across system configurations on the \Bird~benchmark.
    Latencies are substantially higher than \Spider~equivalents, with
    \textit{Syn} and \textit{Ground} reaching 43.7s and 43.8s respectively,
    driven by the larger schemas, longer prompts and more complex query
    generation required by \Bird.
  }
  \label{fig:eval:candidate-latency-bird}
\end{figure}

Lastly the \textit{Ground} configuration establishes an upper bound of
55.8\% \EA, representing a +23.4pp improvement over \textit{Baseline} (with a
+72\% relative gain). Despite these substantial relative improvements within
the recorded benchmarks a significant gap of -10.3pp and -8.2pp remains when
compared to the published \OmniSQL~(7B) performance with 66.1\% and GPT-4o
performance with 64.0\%. This stark difference between local replication and
officially reported performance metrics is yet unclarified. Detailed per-metric
breakdowns for \Bird~(dev) are provided in
Appendix~\ref{fig:appendix:bird-dev-ea} and~\ref{fig:appendix:bird-dev-em}.

\subsection{Performance Characteristics}

To assess the performance of NL2SQL systems holistically metrics beyond
semantic accuracy have to be taken into account. The two performance
characteristics \ER~and \CL~provide insight into system reliability and their
real-world applicability. These functional metrics complement accuracy by
revealing operational constraints (such as hardware requirements or
reliability) that affect deployment feasibility.

\subsubsection{Error Rate Analysis}

The error rate results demonstrate great reliability across all benchmarked
systems and datasets. The error rates remain below 0.7\% (fewer than 7 failures
per 1000 queries) across all measured systems, with the \textit{Baseline}
achieving 0.0\% error rate on \Spider~(dev) and 0.3\% on \Bird~(dev). Even
\Natural~configurations with self-refinement, query parsing and ICL maintain
low error rates: \textit{Ground} shows an \ER~of 0.7\% on \textsc{Spider}
(dev), 0.3\% on \textsc{Spider} (test) and 0.2\% on \textsc{Bird} (dev).

This consistency indicates that the \OmniSQL~model generates syntactically
valid, executable SQL with a high reliability, and that pipeline components do
not introduce significant failure modes.

Furthermore these low error rates reveal an important characteristic of
\OmniSQL~and \textsc{Natural}: Errors of these NL2SQL systems are predominantly
semantic (ie, wrong results) rather than syntactic (eg, malformed SQL or schema
violations) or runtime-related.

This distinction matters for production deployments, as semantic errors are
significantly harder to validate and catch than hard system failures. Semantic
accuracy might be further improved through user feedback which can feedback
into ICL. 

\subsubsection{Latency Analysis}

Measuring the \CL~reveals the computational trade-offs that come with the
the \textsc{Natural} pipeline architecture. The \textit{Baseline} configuration
achieves minimal candidate latency (7.3s on \textsc{Spider} dev, 7.3s on test,
9.3s on \textsc{Bird} dev), representing the time required for schema
subsetting and SQL generation without example retrieval or refinement.

The \textit{Zero-Shot} pipeline configuration approximately doubles the end to
end latency to (14.0s, 15.3s, 18.7s respectively) highlighting that the
schema subsetting, self-refinement and voting components contribute to a
significant increase in computational cost. Configurations with example
selection and in-context learning show further latency increases. The
\textit{Train} and \textit{Syn} configurations achieve 15s-17s \CL~on
\textsc{Spider} and 33s-44s on \textsc{Bird}, with the \textit{Syn}
configuration yielding the highest latency (43.7s on \textsc{Bird}).

Overall \OmniSQL~(7B-gguf) consistently showed the lowest \CL~values with
(6.6s, 6.7s and 8.2s) for \Spider~(dev and test) and \Bird~respectively. This is
unsurprising as the \OmniSQL~(7B-gguf) system provides a realistic estimation
for the $\pi$ component of \Natural.

These characteristics can inform potential hardware requirements or deployment
decisions. For interactive applications sub-second response times of these
models are recommended. \Natural~clearly exceeds these thresholds by a factor
of up to 45 on complex databases and questions (eg, \Bird). Thus either
significantly better hardware is required, a simpler configuration needs to be
used or further performance optimizations need to be introduced.

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/latency-accuracy-tradeoff.svg}
  \caption{
    Latency vs.\ execution accuracy trade-off across all verified system
    configurations and benchmarks. Each point represents a
    (configuration, benchmark) pair. The plot illustrates an efficiency
    frontier: moving from \textit{Baseline} to ICL-enabled configurations
    increases accuracy substantially but incurs significant latency costs,
    particularly on \Bird~where the latency penalty reaches 43s. The
    \textit{Zero-Shot} configuration represents an inefficient operating
    point — high latency relative to accuracy gains over \textit{Baseline}.
  }
  \label{fig:eval:latency-accuracy-tradeoff}
\end{figure}

\subsection{Performance Gap}\label{sec:eval:performance-gap}

During baseline validations of the foundational models a critical performance
discrepancy emerged (see~\ref{fig:eval:omnisql-gap}) between the locally
measured performance of \OmniSQL~7B and the published performance by
\citeauthor{OmniSQL}. This section systematically breaks down the magnitude,
potential causes and implications of this gap. 

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/omnisql-difference.svg}
  \caption{
    Performance gap between measured OmniSQL-7B-gguf and reported
    OmniSQL-7B results across benchmarks. The $\Delta$ ranges from 2.6\% on
    \Spider~(dev) to 28.1\% on \Bird~(dev), highlighting differences between
    quantized local deployment and published full-precision results.
  }
  \label{fig:eval:omnisql-gap}
\end{figure}

\subsubsection{Magnitude and Characterization}

Table~\ref{tab:eval:performance-gap} presents the gap between the measured
performance discrepancies across all evaluated benchmarks for the \OmniSQL~7B model.
It is important to note that a GGUF variant was used for local measurements.
The resulting gap varies significantly by dataset, ranging from only 2.6\% on
\Spider~(dev) up to 28.1\% on \Bird~(dev).

\begin{table}[ht]
    \centering
    \begin{tabular}{l|c|c|c}
        \toprule
        \textbf{Benchmark} & \textbf{Published (\EA)} & \textbf{Measured (\EA)} & \textbf{Gap ($\Delta$)} \\
        \midrule
        \Spider~(dev)      & 81.6\%                   & 79.0\%                  & -2.6pp                 \\
        \Spider~(test)     & 89.8\%                   & 79.0\%                  & -10.8pp                \\
        \Bird~(dev)        & 66.1\%                   & 38.0\%                  & -28.1pp                \\
        \bottomrule
    \end{tabular}
    \caption{
        Performance gap between published \OmniSQL~7B results \citep{OmniSQL} and
        local measurements using OmniSQL-7B-gguf (Q8\_0 quantization). Negative
        values indicate underperformance relative to published baselines.
    }
    \label{tab:eval:performance-gap}
\end{table}

The uneven distribution of the measure performance gaps across the three
benchmarks suggests a systematic cause rather than random variation. The gap
correlates strongly with benchmark complexity: \Spider~(dev), the simplest
of the three benchmarks with well-structured schemas and clean data is showing
only a minimal performance degradation of -2.6pp. \Spider~(test) which is not
included in any training dataset, using more diverse domains and more complex
queries is showing a moderate degreation of -10.8pp. \Bird~(dev) which is the
most complex benchmark of the three, featuring larger schemas, dirty data and
external knowledge is showing an extreme degradation of -28.1pp.

This is indicating a performance drop correlating with task difficulty,
suggesting that the locally measured model had impaired reasoning capabilities,
difficulties handling larger schemas or handling external knowledge
requirements.

\subsubsection{Potential Contributing Factors}

Several factors may contribute to this stark gap between the two systems.
This subsection is analyzing potential causes using available evidence.

\paragraph{Model Quantization and Format Differences}

Likely the most apparent difference between the published performance metrics
and the local measurements for \OmniSQL~7B is the model format and precision.
The OmniSQL-7B-gguf system uses the F16 weights but transformed into the GGUF
model format while published results presumably use full-precision models in
the safetensors formats. Prior research has shown that bit-wise quantization
can degrade model accuracy, particularly on complex reasoning tasks on
multi-billion parameter models \citep{Int8AtScale} and it is yet unclear
whether the mere transformation into GGUF harms performance for this model
series. Second, the exact hyperparameters used during evaluation likely
differed between \cite{OmniSQL} and local benchmarks which may influence the
model accuracy in unforeseen ways. Unless the exact same inference environment,
quantization and hyperparameters the performance difference might be caused by
a multitude of subtle differences. Finally, dataset version differences or
preprocessing variations could contribute to divergent results.

The $\Delta$ of 2.6\% on \Spider~(dev) is well within the expected
format-induced degradation ranges, which initially suggests that the model
format alone could explain the simple benchmark discrepancy. However, the
widening performance gaps with increasing benchmark complexity (up to 28.1\% on
\Bird) substantially exceed the typical effects of model format conversions
indicating that other factors might contribute to the observed performance loss.

\paragraph{Inference Implementation Differences}

\citeauthor{OmniSQL} likely used \texttt{vllm} as their inference engine, while
local measurements of \OmniSQL~7B used Rust FFI bindings to llama.cpp. These
engines implement the same mathematical operations but may differ in multiple
dimensions. The engines might differ in implementation when it comes to numeric
operations, the exact attention implementation, sampling algorithms and KV-cache
management. To keep further inference environment divergent as small as possible,
similar (where possible) sampling configurations, prompts and hyperparameters
where used.

\paragraph{Hyperparameter and Configuration Uncertainty}

While the published \OmniSQL~evaluation does not exactly describe all its
inference hyperparameters, some were noted in example code but uncertainty
remains about the concrete configuration. Critical parameters that can widely
influence model performance include temperature, top-p sampling, maximum
generation length, context window size, repetition penalties and stop sequences.

Local measurements used a context size of 32768 and only greedy sampling with
standard stop sequences. Inferring from the results published by
\citeauthor{OmniSQL} greedy sampling was the primary sampling method during
evaluation alongside majority voting. Overall a difference in hyperparameters
and inference engine configuration is possible, but is concluded to be unlikely
given that greedy sampling as a standard and very straightforward algorithm.

\paragraph{Prompt Template Alignment}

While a prompt drift could cause significant performance differences the exact
prompt as supplied by \citeauthor{OmniSQL} was used. Subtle differences in
phrasing, layout or presentation could yield widely diverging results. It is
likely that \OmniSQL~7B was fine-tuned using a specific prompt format, and
deviation from this format can significantly degrade performance.

The local benchmarking implementation used the published prompt template but
likely had a slightly diverging code representation implementation than the
authors of \OmniSQL. The code representation implementation used by the
benchmarking harness is based around a the \texttt{sqlparser} crate in Rust
which includes an implementation for human readable formatting of SQL queries.
While these modifications are subtle, they may inadvertently misalign with the
model's learned expectations on the code representation format, confusing the
generation process.

Further evidence for a possible prompt drift comes from the \textit{Baseline}
configuration results. Even when using only the inference logic ($\pi$) without
additional pipeline components, \Natural~achieved 77.9\% EA on \Spider~(dev)
compared to OmniSQL-7B-gguf's 79.0\%—a 1.1pp degradation despite using the same
model and quantization. This suggests that even \OmniSQL is noticeably
sensitive to prompt template differences affect performance.

The prompt template used in the local benchmarks for \Natural is shown in
Appendix~\ref{appendix:prompt:natural:inference} and for \OmniSQL in
Appendix~\ref{appendix:prompt:omnisql:inference}. \Natural's prompt deviated
from the presumed original format through the addition of similarity scores for
in-context learning examples, more explicit instructions and the likely
modified schema representation.

\subsubsection{Implications for Evaluation Validity}

Due to time and resource constraints no exhaustive systematic evaluation of
different quantization schemes, inference engines and prompt templates
was performed. This significantly limits the ability to draw
definite conclusions on the exact root cause for the performance drift between
the published performance numbers and the locally measured ones.

\paragraph{Impact on Absolute Performance Claims}

The substantial gap between the published vs reproduced baselines undermines
the confidence in \textit{absolute} performance gains. The \textit{relative}
performance gains are excellent (eg, \textit{Syn} achieving 53.8\% vs.
\OmniSQL~7B-gguf achieving 38.0\% on \Bird) but given that no clear absolute
improvement could be measured (eg, \textit{Syn} achieving 53.8\% vs \OmniSQL~7B
achieving 66.1\% on \Bird). Furthermore the comparisons with other external
systems become unclear if local measurements face a significant gap in
compared to the published performance numbers.

Thus this thesis avoids strong claims about absolute performance levels and
instead focuses on the relative improvements achieved on local NL2SQL systems
which is consistent with the evaluation environment and results.

\paragraph{Reliability of Relative Comparisons}

Most notably, the performance gap between \OmniSQL~7B and \OmniSQL~7B-gguf does
not infect the validity results measured between different variants of \Natural
and between \Natural and \OmniSQL. All \Natural~configurations and
\OmniSQL~7B-gguf have been evaluated using the same evaluation framework and
are therefore strictly comparable. \Natural~showed strong performance
improvements across increasingly complex pipeline configurations. \textit{Syn}
showed an improvement of +21.4pp pver \textit{Baseline} on \Bird which was
reliably measurable regardless of the absolute baseline uncertainty.

Furthermore component ablations in the evaluation provide valid evidence for
understanding the relative contributions of different pipeline components
(minimal gain without ICL vs. substantial gain with ICL). Therefore the
conclusion holds true that example selection is the most impactful component
and that components exhibit synergistic rather than additive effects remain
valid although the conclusion is only backed by same-environment evaluations
instead of cross-study evaluations.

\paragraph{Broader Implications for LLM-based Research}

The challenges encountered in this thesis reflect broader, systemic issues in
LLM-based NL2SQL research. Without accessibility to the same hardware, models,
specifications and evaluation frameworks a reliable cross-study comparison
becomes close to unachievable. Intra-study comparisons remain valid but require
significant time and hardware resources to achieve. This suggests that the
% TODO: Add a specific recommendation for a standardized evaluation protocol or
% reference implementation
NL2SQL research community would benefit from standardization of inference
frameworks, comprehensive documentation of all hyperparameters and
configurations used as well as a release of exact prompts, preprocessing code
and scripts used during evaluation. Furthermore could multiple independent
reproductions increase the confidence in claimed results. 

\subsubsection{Recommendations for Future Work}

Based on this analysis several recommendations for further research in this
area can be made:

\paragraph{Systematic Quantization Study}

A dedicated investigation of the impact of model quantization on NL2SQL
performance could provide guidance for better understanding the quantization to
performance tradeoffs. This would allow for empirical decision making when
deploying choosing model sizes and quantization into resource constrained
environments. Prior research exists on the general impact of quantization on
model performance, but having concrete datapoints specific to NL2SQL would
clarify the observed behavior \citep{Int8AtScale}. This study should evaluate
state of the art NL2SQL systems across multiple quantization levels (F16, Q8,
Q6, Q4 and Q3) on all major NL2SQL benchmarks like \Spider, \Spider~2.0 and
\Bird~measuring both the accuracy and latency implications of different
configurations. Furthermore an analysis could reveal correlation in performance
behavior between quantization levels and query and schema complexity.

\paragraph{Cross-Framework Validation}

To isolate inference engine effects and subtle differences implied through
different model formats the same model should be evaluated across multiple
frameworks. This study could include PyTorch and Transformers, llama.cpp, vLLM
and ONNX and evaluate state of the art NL2SQL models in varying sizes against
prevalent benchmarks. Consistent results across the frameworks would further
increase the confidence in reproducibility of results where discrepancies would
highlight discrepancies in implementation-specific details of algorithms and
prompting.

\paragraph{Schema and Example Presentation}

While research on schema and example presentation exist within the NL2SQL
community it is not exhaustive and includes evaluations of different classes of
schema and query presentation \citep{DAIL-SQL, XiYan}. Further research in this
field could highlight differences introduced through subtle changes in
presentation of examples or schemas and recommend mitigation techniques like
identifying the most performant presentation formats or fine-tuning base models
to a specific presentation format.

\subsubsection{Summary}

Overall the gap of 28.1pp on \Bird~between the published \OmniSQL~7B results
(66.1\% \EA) and local measurements (38.0\% \EA) represents a significant
challenge for the absolute performance interpretation from this evaluation.
While strong incremental performance gains could be observed (eg, \textit{Syn}
achieving 53.8\% vs. \OmniSQL~7B-gguf achieving 38.0\% on \Bird) by applying
the techniques outlined in this thesis (schema aware example selection,
self-refinement, majority voting) no clear statement can be made on the
absolute performance of \Natural. Multiple factors are possibly contributing to
the observed gap which include model quantization, inference implementation,
hyperparameter uncertainty, prompt template drifting and potential dataset or
evaluation script version differences.

Despite these limitations, the gap does \textit{not} invalidate the core
contributions of this thesis. The relative comparisons within the consistent
local evaluation environment remain reliable, therefore enabling valid, but
thesis-local conclusions.
