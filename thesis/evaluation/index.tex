\newcommand{\EA}{$\mathbb{EA}$}
\newcommand{\EM}{$\mathbb{EM}$}
\newcommand{\ER}{$\mathbb{ER}$}
\newcommand{\CL}{$\mathbb{CL}$}
\newcommand{\Natural}{\textsc{Natural}}
\newcommand{\OmniSQL}{\textsc{OmniSql}}
\newcommand{\Spider}{\textsc{Spider}}
\newcommand{\Bird}{\textsc{Bird}}

\section{Evaluation}

The evaluation phase of \Natural builds upon the benchmarking
infrastructure outlined in section~\ref{impl:benchmark}. This section is
evaluating benchmarking data gathered while running \Natural in
different configurations on two prevalent benchmarking datasets:
\Spider and \Bird. A representative baseline is introduced,
aiming to mirror the base-model performance. Subsequently an ablation study is
performed to understand the performance impact of individual pipeline components.

\subsection{Experimental Methodology}

\subsubsection{Test Environment}

The tests are performed on a consumer-grade linux system, as research- or
enterprise-grade hardware was not available. Thus only a subset of datasets and
configurations have been measured, as a single benchmark executions on prevalent
datasets takes 12-36 hours.

\paragraph{Hardware Configuration}

The system used for benchmarking has a NVIDIA RTX 3090 GPU with 24GB VRAM
available, an AMD 9990X3D CPU with 12-cores up to 5.5GHz, 64GB of RAM at
6400mt/s and 1TB of SSD storage at a read speed of 7450 MB/s and a write speed
of 6900 MB/s.

\paragraph{Software Stack}

The software stack running on the system used for benchmarking is NixOS (25.05)
with a recent linux kernel version (6.17.7). CUDA 13.0 is being used for
inference on GPUs and a rust compiler version is 1.91 for compiling
\Natural. The models used during benchmarking are \textsc{OmniSql 7B
Q8\_0} and \textsc{Qwen3 Embedding 8B Q4\_K\_M} for generation and embedding
respectively.

\subsubsection{Benchmark Datasets}

The benchmarking datasets used are the \Spider and \Bird
testdatasets that are prevalently used for evaluation of NL2SQL systems.
Comparable research has frequently published performance metrics
(\textsc{Execution Accuracy} and \textsc{Exact Match}) on these benchmarks
which makes the performance of \textsc{Natural} comparable to other works.

\paragraph{Spider}

\textsc{Spider} is likely the most prevalently used benchmark for NL2SQL
systems in contemporary research. The test dataset is comprised of 10,181
questions and 5693 corresponding SQL queries spread across 200 databases
spanning 138 domains. Questions in \Spider are categorized by
difficulty (easy, medium, hard, extra hard). Evaluations on \Spider
refer to the test dataset.

\paragraph{Bird}

\Bird is another widely used benchmark that is comprised of 12,751
question answer pairs spread across 95 databases in 37 domains. It aims to be
more representative of real world scenarios with external knowledge
requirements and is generally considered harder than \Spider.

\subsubsection{Evaluation Metrics}

The chosen set of evaluation metrics used is a mixture of semantic metrics
(\EA, \EM) and functional metrics (\ER, \CL) to gather a hollistic picture of
system behavior and real world applicablility.

\paragraph{Execution Accuracy (\EA)}

Execution Accuracy (or \EA) is the primary success metric of
NL2SQL systems as it represents the semantic accuracy of the SQL queries
produced NL2SQL systems. Execution accuracy is computed by determining whether
the rows in the candidate results are a permutation of the rows returned by
ground truth results. 

For a query $q_g$ with candidate results $R_c$ and ground truth results $R_g$, 
where $R_c, R_g \subseteq \mathbb{V}^{n \times m}$ (sets of $n$ rows with $m$ columns 
of values $\mathbb{V}$), semantically accurate execution is defined as:

\begin{equation}
    semanticeq(R_c, R_g, q_g) = \mathbb{1}\left[\exists \pi \in \Pi_m : 
    \begin{cases}
        R_c = \pi(R_g) & \text{if } ordered(q_g) \\
        R_c \equiv_{\text{multiset}} \pi(R_g) & \text{otherwise}
    \end{cases}
    \right]
\end{equation}

where $\Pi_m$ is the set of valid column permutations (those preserving column
value sets), $\pi(R_g)$ applies permutation $\pi$ to reorder columns in each
row of $R_g$, $\equiv_{\text{multiset}}$ denotes multiset equality (allowing
row reordering) and $ordered(q_g)$ is determining if the ground truth query
$q_g$ contains an \texttt{ORDER BY} clause.

In order to compute the \EA on \Spider and \Bird the official evaluation suite
is used respectively to ensure comparability across externally reported
measurements and locally observed measurements.

\paragraph{Exact Match (\EM)}

Exact Match (or \EM) measures the syntactic equivalence between
candidate and ground truth SQL queries after normalization (ie, whitespace
trimming, casing adjustments etc). Unlike execution accuracy which validates
semantic correctness through result comparison, exact match determines whether
the generated query structurally matches the reference query. \EM acts as a
lower bound for execution accuracy as queries must be identical which is
therefore more restrictive as semantically equivalent queries with different
syntax (ie, using a different join order) are marked as incorrect.

\paragraph{Error Rate (\ER)}

Error Rate (or \ER) describes the system reliability by measuring the
frequency of SQL generation failures that prevent query execution. \ER is
reported in failures per hundred queries and monitors system reliability
(schema violations, syntax errors, out-of-memory errors etc).

\paragraph{Candidate Latency (\CL)}

Candidate Latency (or \CL) measures the end-to-end execution
time of a system from natural language input to SQL candidate output, reported
in seconds. This metric captures the compound runtime of all pipeline
components ($\sigma, \phi, \pi, \rho, \nu$) and reflects real-world system
responsiveness. Latency increases with pipeline complexity, particularly when
example selection and self refinement are applied, thus representing relative
computational cost.

\subsubsection{Baselines}

To adequately determine the performance impact of methods and components
applied in \Natural a baseline helps to measure relative performance
gains or losses compared to existing systems. Therefore two sets of baseline
systems are introduced: Internal baselines (measuring the system without
crucial components) and external baselines (eg, proprietary system
performance and raw model performance).

\paragraph{Internal Baselines}

Introducing two internal baselines subsequently allows for a brief ablation
study, where the impact of different pipeline configurations and different
sampling datasets can be measured to isolate the contribution of each
configuration. The \textit{Baseline} configuration of \Natural is using
only the inference logic ($\pi$) without all other pipeline components,
representing the performance contribution of \textsc{OmniSQL} as closely as
possible. Additonally the \textit{Zero-Shot} configuration of \Natural
is introduced and measures the performance of \Natural with all
pipeline components activated but without any examples available to use during
in-context learning.

\paragraph{External Baselines}

Using published results from other systems, \Natural can be briefly
compared against existing systems with similar capabilities highlighting
relative performance improvements or losses. Notably external baselines largely
rely on unverified data from other papers. Given that \Natural is
largely based on \OmniSQL, GPT-4 and \OmniSQL are the primary
external systems are taken into account as baselines.

\subsubsection{Vector Databases}

Three different vector databases are introduced per benchmark. These differ in
the datasets using during sampling:

\paragraph{Synthetic}

The \textit{Synthetic} vector databases are sampled from the
\textsc{SynSQL-2.5m} dataset introduced by \citeauthor{OmniSQL} in
\citeyear{OmniSQL}. The \textsc{SynSQL-2.5m} dataset is largely unrelated to
\textsc{Spider} and \textsc{Bird} but covers a wide array of domains through
its LLM-based synthetic data generation approach.

\paragraph{Train}

The \textit{Train} vector databases are sampled from the respective training
splits from \textsc{Spider} and \textsc{Bird} \citep{Spider, BIRD}. The train
splits include similar style of SQL queries and similar domains but different
databases and different questions.

\paragraph{Ground}

The \textit{Ground} vector databases are sampled from the respective splits
used during evaluation (dev and test) from \textsc{Spider} and \textsc{Bird}
\citep{Spider, BIRD}. These allow \textsc{Natural} to reference ``previously
used'' answers as examples during generation indicating the potential
upper-limit of performance during a self-learning deployment which has access
to past conversations.

\subsection{Benchmark Results}

This section is presenting the measured performance across all \textsc{Natural}
configurations and benchmark datasets. Internal ablations are compared
(Baseline, Zero-Shot, as well as different vector DBs) against external prior
art (GPT-4, \OmniSQL). Subsequently performance characteristics and improvement
trends are quantitatively and qalitatively analzed.

\subsubsection{Overview}

The overall benchmarking results are presented in
Table~\ref{tab:eval:overall-results}, which displays the the performance for
all system configurations across the \textsc{Spider} and \textsc{Bird}
datasets, the execution accuracy ($\mathbb{EA}$), exact match ($\mathbb{EM}$),
error rate ($\mathbb{ER}$) and candidate latency
($\mathbb{CL}$) metrics.

\begin{table}[ht]
    \centering
    \footnotesize
    \hspace*{-2.5mm}
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
        \toprule
        \textbf{System}     & \multicolumn{4}{c|}{\textbf{Spider (dev)}}                       & \multicolumn{4}{c|}{\textbf{Spider (test)}}                       & \multicolumn{4}{c}{\textbf{BIRD (dev)}}                            \\
        \midrule                                                                                                                                                 
                            & \EA             & \EM            & \ER           & \CL           & \EA             & \EM          & \ER              & \CL            & \EA           & \EM              & \ER            & \CL           \\
        \midrule
        \multicolumn{13}{c}{\textbf{Closed-Source LLMs}} \\                                                                                                                                                                          
        \midrule                                                                                                                                                                                                                     
        GPT-4o-mini*        & 71.0            & -              & -             & -             & 83.7           & -             & -                & -              & 61.5          & -                & ?              & -             \\
        GPT-4-Turbo*        & 72.2            & -              & -             & -             & 84.2           & -             & -                & -              & 63.6          & -                & ?              & -             \\
        GPT-4o*             & 70.7            & -              & -             & -             & 84.9           & -             & -                & -              & 64.0          & -                & ?              & -             \\
        \midrule
        \multicolumn{13}{c}{\textbf{Open-Source LLMs}} \\                                                                                                                                                                            
        \midrule
        OmniSQL-7B*         & 81.6            & -              & -             & -             & 89.8           & -             & -                & -              & 66.1          & -                & ?              & -             \\
        OmniSQL-14B*        & 82.0            & -              & -             & -             & 88.3           & -             & -                & -              & 65.9          & -                & ?              & -             \\
        OmniSQL-32B*        & 80.9            & -              & -             & -             & 89.8           & -             & -                & -              & 67.0          & -                & ?              & -             \\
        OmniSQL-7B-gguf     & 79.0            & 33.0           & 0.1           & \textbf{6.6s} & 79.0           & 35.7          & 0.3              & \textbf{6.7s}  & 38.0          & 0.03             & 0.8            & \textbf{8.2s} \\
        \midrule
        \multicolumn{13}{c}{\textbf{Pipelines}} \\                                                                                                                                                                          
        \midrule
        \Natural~(Baseline)  & 77.9           & 30.3           & \textbf{0.0}  & 7.3s          & 77.0           & 32.5          & \textbf{0.0}     & 7.3s           & 32.4          & 0.03             & \textbf{0.3}   & 9.3s          \\
        \Natural~(Zero-Shot) & 78.7           & 27.1           & 0.2           & 14.0s         & 77.5           & 32.7          & 0.4              & 15.3s          & 34.0          & 0.03             & 0.4            & 18.7s         \\
        \Natural~(Syn)       & \textbf{81.0}  & 41.2           & 0.3           & 16.3s         & 79.6           & 38.7          & 0.4              & 15.4s          & \textbf{53.8} & \textbf{18.8}    & 0.2            & 43.7s         \\
        \Natural~(Train)     & 80.4           & \textbf{42.2}  & 0.6           & 16.2s         & \textbf{81.4}  & \textbf{43.9} & 0.3              & 15.8s          & 48.4          & 13.2             & 0.3            & 33.7s         \\
        \Natural~(Ground)**  & 84.2           & 42.7           & 0.7           & 16.1s         & 82.8           & 39.5          & 0.3              & 16.0s          & 55.8          & 19.3             & 0.2            & 43.8s         \\
        \bottomrule
    \end{tabular}
    \caption{
        Comprehensive benchmark results across all systems and datasets.
        \EA~and \EM~are reported as percentages. \ER~is reported in failures
        per hundred queries and \CL~is reported in seconds. Systems marked with
        * are external benchmarks with unverified results from published
        papers. Systems marked with ** had access to ground truth data during
        inference, illustrating the upper bound achievable. Bold values
        indicate best verified performance, excluding systems with access to
        ground truth. \citep{OmniSQL}
    }
    \label{tab:eval:overall-results}
\end{table}

\subsubsection{Baseline Performance Analysis}

One notable insight during benchmarking was the apparent gap between the
and the locally measured performance of \OmniSQL~7B (using GGUF with the F16
quantization) and the reported accuracy metrics results from \cite{OmniSQL}.
The $\delta$ in performance between the local measurement and the official
values ranges from 2.6\% on \Spider~(dev) to 11.8\% on \Spider~(test) and
28.1\% on \Bird~(dev).

Several factors may contribute to this stark gap between the two systems.
First, model quantization will come at a non-negligible cost and different
quantization variants come with different performance characteristics. The
OmniSQL-7B-gguf system uses the F16 weights but transformed into the gguf
model format while published results presumably use full-precision models in
the safetensor formats. Prior research has shown that bit-wise quantization
can degrade model accuracy, particularly on complex reasoning tasks on
multi-billion paramater models \citep{Int8AtScale}. Second, the exact
hyperparameters used during evaluation likely differed between
\cite{OmniSQL} and local benchmarks which may influence the model accuracy in
unforseen ways. Unless the exact same inference environment, quantization and
hyperparameters the performance difference might be caused by a multitude of
subtle differences. Finally, dataset version differences or preprocessing
variations could contribute to divergent results.

The \textit{Baseline} configuration of \Natural~further showed degredation of
performance compared to the OmniSQL-7B-gguf system configuration. Using the
\textit{Baseline} configuration, \Natural~only uses the inference logic and
prompt template for ICL ($\pi$) without other pipeline components, achieving
77.9\% \EA~on \Spider~(dev), 77.0\% on \Spider~(test), and 32.4\% on \Bird
(dev). These results further fall below the published \textsc{OmniSQL-7B}
performance of 81.6\%, 89.8\%, and 66.1\% respectively, representing gaps of
3.7, 12.8, and 33.7 percentage points. This continued degredation of
performance indicates that a misaligned prompt template (ie, divergent from
training phase) harms performance if not paired with counter measures such as
ICL, self-correction or majority voting. These performance numbers mark the
\textit{Baseline} configuration the worst performing variant of \Natural~on
\EA~and \EM~across \Spider~(dev and test) and \Bird~(dev).

\subsubsection{Spider Results}

The results of the \Spider~benchmarks demonstrate consistent performance
characteristics of \Natural~and \OmniSQL~across the development and test splits.
The OmniSQL-7B-gguf system outperforms the two \Natural~baseline configurations
\textit{Baseline} and \textit{Zero-Shot} in \EA~and \EM by 2.1\% and 0.3\% on
the development split and by 2.0\% and 1.5\% on the test split respectively.

The full pipeline with the \textit{Zero-Shot} configuration (all components
active but no examples) hence only yields marginal improvements (+0.8pp on dev,
+0.5pp on test) over \textit{Baseline}, suggesting that schema subsetting and
query refinement contribute minimally without example-based guidance. Notably
the \CL of \textit{Zero-Shot} increases by 6.7s on dev and 8.0s on test over
\texit{Baseline}, doubling the system latency without a significant improvement
in system performance.

Intruducing the vector databases to the system configurations demonstrates
substantial improvement in performance characteristics. The two configurations
without ground truth examples showed a significant improvement in both \EA~and
\EM: \textit{Syn} outperformed all other system variants and baselines with a
accuracy improvement of 3.1\% over \textit{Baseline} and 2.0\% over
OmniSQL-7B-gguf on \Spider~(dev). On \Spider~(test) the \textit{Train}
configuration outperformed \textit{Syn} on

