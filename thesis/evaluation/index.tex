% Reference: https://github.com/petavue/NL2SQL-Benchmark/blob/main/results/Report/NL2SQL%20Benchmark%20Report.pdf

\section{Evaluation}

The evaluation phase of \textsc{Natural} builds upon the benchmarking
infrastructure outlined in section~\ref{impl:benchmark}. This section is
evaluating benchmarking data gathered while running \textsc{Natural} in
different configurations on two prevalent benchmarking datasets:
\textsc{Spider} and \textsc{Bird}. A representative baseline is introduced,
aiming to mirror the base-model performance. Subsequently an ablation study is
performed to understand the performance impact of individual pipeline components.

\subsection{Experimental Methodology}

% TODO: Introduction paragraph explaining the systematic evaluation approach

\subsubsection{Test Environment}

The tests are performed on a consumer-grade linux system, as research- or
enterprise-grade hardware was not available. Thus only a subset of datasets and
configurations have been measured, as a single benchmark executions on prevalent
datasets takes 12-36 hours.

\paragraph{Hardware Configuration}

The system used for benchmarking has a NVIDIA RTX 3090 GPU with 24GB VRAM
available, an AMD 9990X3D CPU with 12-cores up to 5.5GHz, 64GB of RAM at
6400mt/s and 1TB of SSD storage at a read speed of 7450 MB/s and a write speed
of 6900 MB/s.

\paragraph{Software Stack}

The software stack running on the system used for benchmarking is NixOS (25.05)
with a recent linux kernel version (6.17.7). CUDA 13.0 is being used for
inference on GPUs and a rust compiler version is 1.91 for compiling
\textsc{Natural}. The models used during benchmarking are \textsc{OmniSQL 7B
Q8\_0} and \textsc{Qwen3 Embedding 8B Q4\_K\_M} for generation and embedding
respectively.

\subsubsection{Benchmark Datasets}

The benchmarking datasets used are the \textsc{Spider} and \textsc{Bird}
testdatasets that are prevalently used for evaluation of NL2SQL systems.
Comparable research has frequently published performance metrics
(\textsc{Execution Accuracy} and \textsc{Exact Match}) on these benchmarks
which makes the performance of \textsc{Natural} comparable to other works.

\paragraph{Spider}

\textsc{Spider} is likely the most prevalently used benchmark for NL2SQL
systems in contemporary research. The test dataset is comprised of 10,181
questions and 5693 corresponding SQL queries spread across 200 databases
spanning 138 domains. Questions in \textsc{Spider} are categorized by
difficulty (easy, medium, hard, extra hard). Evaluations on \textsc{Spider}
refer to the test dataset.

\paragraph{Bird}

\textsc{Bird} is another widely used benchmark that is comprised of 12,751
question answer pairs spread across 95 databases in 37 domains. It aims to be
more representative of real world scenarios with external knowledge
requirements and is generally considered harder than \textsc{Spider}.

\subsubsection{Evaluation Metrics}

\paragraph{Execution Accuracy (EA)}

\textsc{Execution Accuracy} (or EA) is the primary success metric of NL2SQL systems
as it represents the semantic accuracy of the SQL queries produced NL2SQL
systems. Execution accuracy is computed by determining whether the rows in the
candidate results are a permutation of the rows returned by ground truth
results. 

For a query $q_g$ with candidate results $R_c$ and ground truth results $R_g$, 
where $R_c, R_g \subseteq \mathbb{V}^{n \times m}$ (sets of $n$ rows with $m$ columns 
of values $\mathbb{V}$), semantically accurate execution is defined as:

\begin{equation}
    semanticeq(R_c, R_g, q_g) = \mathbbm{1}\left[\exists \pi \in \Pi_m : 
    \begin{cases}
        R_c = \pi(R_g) & \text{if } ordered(q_g) \\
        R_c \equiv_{\text{multiset}} \pi(R_g) & \text{otherwise}
    \end{cases}
    \right]
\end{equation}

where $\Pi_m$ is the set of valid column permutations (those preserving column
value sets), $\pi(R_g)$ applies permutation $\pi$ to reorder columns in each
row of $R_g$, $\equiv_{\text{multiset}}$ denotes multiset equality (allowing
row reordering) and $ordered(q_g)$ is determining if the ground truth query
$q_g$ contains an \texttt{ORDER BY} clause.

\paragraph{Exact Match (EM)}

% TODO: Define exact match
% - Definition: whether SQL query exactly matches ground truth
% - Calculation method
% - Limitations and interpretation

\textsc{Exact Match}..

\paragraph{Performance Metrics}

Measuring real world applicablility, the metric \textsc{Latency} is introduced 
for measuring the compound pipeline runtime, thus measuring NL query to final
SQL candidate.

% TODO:
% - End-to-end latency (time from NL query to SQL result)
% - Per-component latency (σ, φ, π, ρ, ν timings)
% - Resource utilization (GPU memory, CPU usage)

\subsubsection{Baseline Systems and Comparisons}

To determine the effect of approaches introduced by \textsc{Natural} two
a set of baselines is introduced. The baselines are split into two groups,
internal baselines (measuring the system without crucial components) and
external baselines (eg, State-of-the-Art system performance).

\paragraph{Internal Baselines}

Two primary internal baselines are introduced:

\begin{enumerate}
    \item \textbf{Model Baseline} — Using only the inference logic for
        zero-shot accuracy, aiming to represent \textsc{OmniSQL} performance as
        closely as possible.
    \item \textbf{Zero-Shot Baseline} — A full \textsc{Natural} pipeline but
        without any examples available for the in-context learning inference
        phase.
\end{enumerate}

These baselines aim to measure both the actual performance of the NL2SQL model
(\textsc{OmniSQL}) and the full \textsc{Natural} pipeline without in-context
learning.

\paragraph{State-of-the-Art Comparisons}

Two primary external baselines are taking into account:

\begin{enumerate}
    \item \textbf{GPT-4} — A proprietary baseline for advanced model
        capabilities without finetuning.
    \item \textbf{OmniSQL} — The open source baseline model series used by
        \textsc{Natural}.
\end{enumerate}

\subsection{Benchmark Results}

\subsubsection{Spider Results}

\paragraph{Overall Performance}

% TODO: Present overall EA and EM scores
% - Table: Overall Spider dev set results
% - Comparison to baseline
% - Comparison to SOTA from literature

\paragraph{Performance by Difficulty Level}

% TODO: Break down results by Spider difficulty classification
% - Table: EA/EM by Easy/Medium/Hard/Extra Hard
% - Analysis of performance degradation with difficulty
% - Discussion of which difficulty levels benefit most from system components

\paragraph{Performance by Database}

% TODO: Present per-database analysis
% - Identify databases where system excels
% - Identify databases where system struggles
% - Analyze characteristics that correlate with performance

\paragraph{Performance by SQL Complexity}

% TODO: Analyze results by SQL pattern complexity
% - Simple SELECT queries
% - JOIN operations
% - Nested subqueries
% - Aggregations and GROUP BY
% - Set operations (UNION, INTERSECT, EXCEPT)
% - Complex predicates

\paragraph{Learning Curve Analysis}

% TODO: Analyze impact of training example quantity
% - How does performance change with k-shot examples?
% - Diminishing returns analysis
% - Optimal k value determination

\subsubsection{Bird Results}

\paragraph{Overall Performance}

% TODO: Present overall Bird results
% - Table: Overall Bird dev set results
% - Comparison to baseline
% - Comparison to SOTA from literature (GPT-4: 54.89%, human: 92.96%)

\paragraph{Domain-Specific Performance}

% TODO: Break down by Bird's 37 domains
% - Best-performing domains
% - Worst-performing domains
% - Domain characteristics that impact performance

\paragraph{Dirty Data Handling}

% TODO: Analyze performance on Bird's dirty data cases
% - How does system handle real-world data quality issues?
% - Error patterns related to data quality
% - Comparison to clean data performance

\paragraph{External Knowledge Requirements}

% TODO: Analyze cases requiring external knowledge
% - Identify queries needing domain knowledge
% - System performance on these cases
% - Limitations of pure schema-based approaches

\subsubsection{Comparison with State-of-the-Art}

\paragraph{Quantitative Comparison}

% TODO: Create comprehensive comparison table
% - Table: System vs GPT-4, DIN-SQL, DAIL-SQL, C3, etc.
% - Both Spider and Bird results
% - Note differences in model architecture and hardware

\paragraph{Open-Source vs Proprietary Models}

% TODO: Discuss open-source (OmniSQL 7B) vs proprietary (GPT-4) trade-offs
% - Performance gap analysis
% - Cost and accessibility advantages
% - Privacy and control benefits
% - Deployment feasibility differences

\paragraph{Qualitative Comparison}

% TODO: Beyond metrics, discuss systemic differences
% - System architecture differences
% - Component strategies (how others handle schema subsetting, refinement, etc.)
% - Integration and deployment characteristics

\subsection{Performance Characteristics}

% TODO: Introduction paragraph on performance evaluation methodology

\subsubsection{Latency Analysis}

\paragraph{Per-Component Timing Breakdown}

% TODO: Present timing for each pipeline component
% - Table: Average latency for σ, φ, π, ρ, ν
% - Distribution statistics (median, p50, p95, p99)
% - Identify bottlenecks

\paragraph{End-to-End Query Response Time}

% TODO: Present total pipeline latency
% - Distribution of total response times
% - Comparison to acceptable thresholds (what's "interactive"?)
% - Impact of query complexity on latency

\paragraph{Latency vs Accuracy Trade-offs}

% TODO: Analyze speed-accuracy Pareto frontier
% - Can we reduce latency by limiting refinement iterations?
% - Can we reduce latency by using fewer candidates for voting?
% - Configurable performance profiles (fast mode vs accurate mode)

\paragraph{Bottleneck Identification and Optimization Opportunities}

% TODO: Identify where time is spent
% - Model inference time (largest component?)
% - Embedding search time
% - Graph kernel computation time
% - Database execution time (for refinement)
% - Recommendations for optimization

\subsubsection{Resource Utilization}

\paragraph{GPU Memory Usage}
% TODO: Analyze VRAM consumption
% - Peak memory usage during inference
% - Memory required for model loading
% - Impact of schema size on memory (large schemas → CUDA OOM)
% - Memory optimization strategies employed

\paragraph{CPU and System Memory}
% TODO: Analyze CPU usage patterns
% - CPU utilization during pipeline execution
% - RAM consumption for embeddings, candidate storage
% - I/O patterns (database access, vector index reads)

\paragraph{Disk I/O and Vector Index Performance}
% TODO: Analyze storage subsystem impact
% - SQLite-vec index size
% - Read performance for similarity search
% - Impact of index size on query latency

\paragraph{Power Consumption and Thermal Characteristics}
% TODO: If measured, discuss energy efficiency
% - GPU power draw during inference
% - Comparison to CPU-only approaches
% - Sustainability considerations

\subsection{Qualitative Analysis}

% TODO: Introduction paragraph explaining value of qualitative evaluation beyond metrics

\subsubsection{Case Studies}

\paragraph{Success Cases}
% TODO: Present 3-5 exemplar success cases
% - Full trace: NL query → schema → examples → generated SQL → execution results
% - Analysis of what made these queries succeed
% - Demonstrate system capabilities at their best
% - Show component contributions in action

\paragraph{Failure Cases}
% TODO: Present 3-5 exemplar failure cases
% - Full trace showing where system failed
% - Root cause analysis: schema linking error? Semantic error? Syntactic error?
% - What component(s) failed?
% - Could failure be prevented with different design choices?

\paragraph{Edge Cases and Boundary Conditions}
% TODO: Present interesting edge cases
% - Very ambiguous queries
% - Queries requiring domain knowledge
% - Queries with multiple valid interpretations
% - Queries at the limit of SQL expressiveness
% - How system behaves at boundaries

\newpage
