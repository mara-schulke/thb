\section{Evaluation}

The evaluation phase of \textsc{Natural} builds upon the benchmarking
infrastructure outlined in section~\ref{impl:benchmark}. This section is
evaluating benchmarking data gathered while running \textsc{Natural} in
different configurations on two prevalent benchmarking datasets:
\textsc{Spider} and \textsc{Bird}. A representative baseline is introduced,
aiming to mirror the base-model performance. Subsequently an ablation study is
performed to understand the performance impact of individual pipeline components.

\subsection{Experimental Methodology}

% TODO: Introduction paragraph explaining the systematic evaluation approach

\subsubsection{Test Environment}

The tests are performed on a consumer-grade linux system, as research- or
enterprise-grade hardware was not available. Thus only a subset of datasets and
configurations have been measured, as a single benchmark executions on prevalent
datasets takes 12-36 hours.

\paragraph{Hardware Configuration}

The system used for benchmarking has a NVIDIA RTX 3090 GPU with 24GB VRAM
available, an AMD 9990X3D CPU with 12-cores up to 5.5GHz, 64GB of RAM at
6400mt/s and 1TB of SSD storage at a read speed of 7450 MB/s and a write speed
of 6900 MB/s.

\paragraph{Software Stack}

The software stack running on the system used for benchmarking is NixOS (25.05)
with a recent linux kernel version (6.17.7). CUDA 13.0 is being used for
inference on GPUs and a rust compiler version is 1.91 for compiling
\textsc{Natural}. The models used during benchmarking are \textsc{OmniSQL 7B
Q8\_0} and \textsc{Qwen3 Embedding 8B Q4\_K\_M} for generation and embedding
respectively.

\subsubsection{Benchmark Datasets}

The benchmarking datasets used are the \textsc{Spider} and \textsc{Bird}
testdatasets that are prevalently used for evaluation of NL2SQL systems.
Comparable research has frequently published performance metrics
(\textsc{Execution Accuracy} and \textsc{Exact Match}) on these benchmarks
which makes the performance of \textsc{Natural} comparable to other works.

\paragraph{Spider}

\textsc{Spider} is likely the most prevalently used benchmark for NL2SQL
systems in contemporary research. The test dataset is comprised of 10,181
questions and 5693 corresponding SQL queries spread across 200 databases
spanning 138 domains. Questions in \textsc{Spider} are categorized by
difficulty (easy, medium, hard, extra hard). Evaluations on \textsc{Spider}
refer to the test dataset.

\paragraph{Bird}

\textsc{Bird} is another widely used benchmark that is comprised of 12,751
question answer pairs spread across 95 databases in 37 domains. It aims to be
more representative of real world scenarios with external knowledge
requirements and is generally considered harder than \textsc{Spider}.

\subsubsection{Evaluation Metrics}

The chosen set of evaluation metrics used is a mixture of semantic metrics
($\mathbb{EA}$, $\mathbb{EM}$) and functional metrics ($\mathbb{ER}$,
$\mathbb{CL}$) to gather a hollistic picture of system behavior and real world
applicablility.

\paragraph{Execution Accuracy ($\mathbb{EA}$)}

\textsc{Execution Accuracy} (or $\mathbb{EA}$) is the primary success metric of
NL2SQL systems as it represents the semantic accuracy of the SQL queries
produced NL2SQL systems. Execution accuracy is computed by determining whether
the rows in the candidate results are a permutation of the rows returned by
ground truth results. 

For a query $q_g$ with candidate results $R_c$ and ground truth results $R_g$, 
where $R_c, R_g \subseteq \mathbb{V}^{n \times m}$ (sets of $n$ rows with $m$ columns 
of values $\mathbb{V}$), semantically accurate execution is defined as:

\begin{equation}
    semanticeq(R_c, R_g, q_g) = \mathbb{1}\left[\exists \pi \in \Pi_m : 
    \begin{cases}
        R_c = \pi(R_g) & \text{if } ordered(q_g) \\
        R_c \equiv_{\text{multiset}} \pi(R_g) & \text{otherwise}
    \end{cases}
    \right]
\end{equation}

where $\Pi_m$ is the set of valid column permutations (those preserving column
value sets), $\pi(R_g)$ applies permutation $\pi$ to reorder columns in each
row of $R_g$, $\equiv_{\text{multiset}}$ denotes multiset equality (allowing
row reordering) and $ordered(q_g)$ is determining if the ground truth query
$q_g$ contains an \texttt{ORDER BY} clause.

In order to compute the $\mathbb{EA}$ on \textsc{Spider} and \textsc{Bird}
the official evaluation suite is used respectively to ensure comparability
across externally reported measurements and locally observed measurements.

\paragraph{Exact Match ($\mathbb{EM}$)}

\textsc{Exact Match} (or $\mathbb{EM}$) measures the syntactic equivalence
between candidate and ground truth SQL queries after normalization (ie,
whitespace trimming, casing adjustments etc). Unlike execution accuracy which
validates semantic correctness through result comparison, exact
match determines whether the generated query structurally matches the reference
query. $\mathbb{EM}$ acts as a lower bound for execution accuracy as queries
must be identical which is therefore more restrictive as semantically
equivalent queries with different syntax (ie, using a different join order) are
marked as incorrect.

\paragraph{Error Rate ($\mathbb{ER}$)}

\textsc{Error Rate} (or $\mathbb{ER}$) describes the system reliability by
measuring the frequency of SQL generation failures that prevent query execution.
$\mathbb{ER}$ is reported in failures per hundred queries and monitors system
reliability (schema violations, syntax errors, out-of-memory errors etc).

\paragraph{Candidate Latency ($\mathbb{CL}$)}

\textsc{Candidate Latency} (or $\mathbb{CL}$) measures the end-to-end execution
time of a system from natural language input to SQL candidate output, reported
in seconds. This metric captures the compound runtime of all pipeline
components ($\sigma, \phi, \pi, \rho, \nu$) and reflects real-world system
responsiveness. Latency increases with pipeline complexity, particularly when
example selection and self refinement are applied, thus representing relative
computational cost.

\subsubsection{Baselines}

To adequately determine the performance impact of methods and components
applied in \textsc{Natural} a baseline helps to measure relative performance
gains or losses compared to existing systems. Therefore two sets of baseline
systems are introduced: Internal baselines (measuring the system without
crucial components) and external baselines (eg, proprietary system
performance and raw model performance).

\paragraph{Internal Baselines}

Introducing two internal baselines subsequently allows for a brief ablation
study, where the impact of different pipeline configurations and different
sampling datasets can be measured to isolate the contribution of each
configuration. The \textit{Baseline} configuration of \textsc{Natural} is using
only the inference logic ($\pi$) without all other pipeline components,
representing the performance contribution of \textsc{OmniSQL} as closely as
possible. Additonally the \textit{Zero-Shot} configuration of \textsc{Natural}
is introduced and measures the performance of \textsc{Natural} with all
pipeline components activated but without any examples available to use during
in-context learning.

\paragraph{External Baselines}

Using published results from other systems, \textsc{Natural} can be briefly
compared against existing systems with similar capabilities highlighting
relative performance improvements or losses. Notably external baselines largely
rely on unverified data from other papers. Given that \textsc{Natural} is
largely based on \textsc{OmniSQL}, GPT-4 and \textsc{OmniSQL} are the primary
external systems are taken into account as baselines.

\subsubsection{Vector Databases}

Three different vector databases are introduced per benchmark. These differ in
the datasets using during sampling:

\paragraph{Synthetic}

The \textit{Synthetic} vector databases are sampled from the
\textsc{SynSQL-2.5m} dataset introduced by \citeauthor{OmniSQL} in
\citeyear{OmniSQL}. The \textsc{SynSQL-2.5m} dataset is largely unrelated to
\textsc{Spider} and \textsc{Bird} but covers a wide array of domains through
its LLM-based synthetic data generation approach.

\paragraph{Train}

The \textit{Train} vector databases are sampled from the respective training
splits from \textsc{Spider} and \textsc{Bird} \citep{Spider, BIRD}. The train
splits include similar style of SQL queries and similar domains but different
databases and different questions.

\paragraph{Ground}

The \textit{Ground} vector databases are sampled from the respective splits
used during evaluation (dev and test) from \textsc{Spider} and \textsc{Bird}
\citep{Spider, BIRD}. These allow \textsc{Natural} to reference ``previously
used'' answers as examples during generation indicating the potential
upper-limit of performance during a self-learning deployment which has access
to past conversations.

\subsection{Benchmark Results}

This section is presenting the measured performance across all \textsc{Natural}
configurations and benchmark datasets. Internal ablations are compared
(Baseline, Zero-Shot, as well as different vector DBs) against external prior
art (GPT-4, OmniSQL). Subsequently performance characteristics and improvement
trends are quantitatively and qalitatively analzed.

\subsubsection{Overview}

The overall benchmarking results are presented in
Table~\ref{tab:eval:overall-results}, which displays the the performance for
all system configurations across the \textsc{Spider} and \textsc{Bird}
datasets, the execution accuracy ($\mathbb{EA}$), exact match ($\mathbb{EM}$),
error rate ($\mathbb{ER}$) and candidate latency
($\mathbb{CL}$) metrics.

\begin{table}[ht]
    \centering
    \footnotesize
    \hspace*{-2.5mm}
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
        \toprule
        \textbf{System}     & \multicolumn{4}{c|}{\textbf{Spider (dev)}}                       & \multicolumn{4}{c|}{\textbf{Spider (test)}}                     & \multicolumn{4}{c}{\textbf{BIRD (dev)}}                          \\
        \midrule                                                                                                                                                 
                            & $\mathbb{EA}$  & $\mathbb{EM}$  & $\mathbb{ER}$ & $\mathbb{CL}$ & $\mathbb{EA}$ & $\mathbb{EM}$  & $\mathbb{ER}$    & $\mathbb{CL}$  & $\mathbb{EA}$ & $\mathbb{EM}$    & $\mathbb{ER}$  & $\mathbb{CL}$ \\
        \midrule
        \multicolumn{13}{c}{\textbf{Closed-Source LLMs}} \\                                                                                                                                                                          
        \midrule                                                                                                                                                                                                                     
        GPT-4o-mini*        & 71.0           & -              & -             & -             & 83.7           & -             & -                & -              & 61.5          & -                & ?              & -             \\
        GPT-4-Turbo*        & 72.2           & -              & -             & -             & 84.2           & -             & -                & -              & 63.6          & -                & ?              & -             \\
        GPT-4o*             & 70.7           & -              & -             & -             & 84.9           & -             & -                & -              & 64.0          & -                & ?              & -             \\
        \midrule
        \multicolumn{13}{c}{\textbf{Open-Source LLMs}} \\                                                                                                                                                                            
        \midrule
        OmniSQL-7B*         & 81.6           & -              & -             & -             & 89.8           & -             & -                & -              & 66.1          & -                & ?              & -             \\
        OmniSQL-14B*        & 82.0           & -              & -             & -             & 88.3           & -             & -                & -              & 65.9          & -                & ?              & -             \\
        OmniSQL-32B*        & 80.9           & -              & -             & -             & 89.8           & -             & -                & -              & 67.0          & -                & ?              & -             \\
        OmniSQL-7B-gguf     & 79.0           & 33.0           & 0.1           & \textbf{6.6s} & 79.0           & 35.7          & 0.3              & \textbf{6.7s}  & 38.0          & 0.03             & 0.8            & \textbf{8.2s} \\
        \midrule
        \multicolumn{13}{c}{\textbf{Pipelines}} \\                                                                                                                                                                          
        \midrule
        Natural (Baseline)  & 77.9           & 30.3           & \textbf{0.0}  & 7.3s          & 77.0           & 32.5          & \textbf{0.0}     & 7.3s           & 32.4          & 0.03             & \textbf{0.3}   & 9.3s          \\
        Natural (Zero-Shot) & 78.7           & 27.1           & 0.2           & 14.0s         & 77.5           & 32.7          & 0.4              & 15.3s          & 34.0          & 0.03             & 0.4            & 18.7s         \\
        Natural (Syn)       & \textbf{81.0}  & 41.2           & 0.3           & 16.3s         & 79.6           & 38.7          & 0.4              & 15.4s          & \textbf{53.8} & \textbf{18.8}    & 0.2            & 43.7s         \\
        Natural (Train)     & 80.4           & \textbf{42.2}  & 0.6           & 16.2s         & \textbf{81.4}  & \textbf{43.9} & 0.3              & 15.8s          & 48.4          & 13.2             & 0.3            & 33.7s         \\
        Natural (Ground)**  & 84.2           & 42.7           & 0.7           & 16.1s         & 82.8           & 39.5          & 0.3              & 16.0s          & 55.8          & 19.3             & 0.2            & 43.8s         \\
        \bottomrule
    \end{tabular}
    \caption{
        Comprehensive benchmark results across all systems and datasets. $\mathbb{EA}$
        (Execution Accuracy) and $\mathbb{EM}$ (Exact Match) are reported as
        percentages. $\mathbb{ER}$ (Error Rate) is reported in failures per
        hundred queries and $\mathbb{CL}$ (Candidate Latency) is reported in
        seconds. Systems marked with * are external benchmarks with unverified
        results from published papers. Systems marked with ** had access to
        ground truth data during inference, illustrating the upper bound
        achievable. Bold values indicate best verified performance, excluding
        systems with access to ground truth.
        \citep{OmniSQL}
    }
    \label{tab:eval:overall-results}
\end{table}

%\subsubsection{Spider Results}

%\paragraph{Overall Performance}

%% TODO: Present overall EA and EM scores
%% - Table: Overall Spider dev set results
%% - Comparison to baseline
%% - Comparison to SOTA from literature

%\paragraph{Performance by Difficulty Level}

%% TODO: Break down results by Spider difficulty classification
%% - Table: EA/EM by Easy/Medium/Hard/Extra Hard
%% - Analysis of performance degradation with difficulty
%% - Discussion of which difficulty levels benefit most from system components

%\paragraph{Performance by Database}

%% TODO: Present per-database analysis
%% - Identify databases where system excels
%% - Identify databases where system struggles
%% - Analyze characteristics that correlate with performance

%\paragraph{Performance by SQL Complexity}

%% TODO: Analyze results by SQL pattern complexity
%% - Simple SELECT queries
%% - JOIN operations
%% - Nested subqueries
%% - Aggregations and GROUP BY
%% - Set operations (UNION, INTERSECT, EXCEPT)
%% - Complex predicates

%\paragraph{Learning Curve Analysis}

%% TODO: Analyze impact of training example quantity
%% - How does performance change with k-shot examples?
%% - Diminishing returns analysis
% - Optimal k value determination

%\subsubsection{Bird Results}

%\paragraph{Overall Performance}

%% TODO: Present overall Bird results
%% - Table: Overall Bird dev set results
%% - Comparison to baseline
%% - Comparison to SOTA from literature (GPT-4: 54.89%, human: 92.96%)

%\paragraph{Domain-Specific Performance}

%% TODO: Break down by Bird's 37 domains
%% - Best-performing domains
%% - Worst-performing domains
%% - Domain characteristics that impact performance

%\paragraph{Dirty Data Handling}

%% TODO: Analyze performance on Bird's dirty data cases
%% - How does system handle real-world data quality issues?
%% - Error patterns related to data quality
%% - Comparison to clean data performance

%\paragraph{External Knowledge Requirements}

%% TODO: Analyze cases requiring external knowledge
%% - Identify queries needing domain knowledge
%% - System performance on these cases
%% - Limitations of pure schema-based approaches

%\subsubsection{Comparison with State-of-the-Art}

%\paragraph{Quantitative Comparison}

%Figure~\ref{fig:eval:sota-landscape} presents the comprehensive SOTA landscape,
%positioning Natural within the current state-of-the-art systems across all
%benchmarks.

%\begin{figure}[h]
    %\centering
    %\includesvg[width=\textwidth]{assets/out/sota-landscape.svg}
    %\caption{SOTA landscape comparison showing Natural's performance relative
        %to state-of-the-art systems (OmniSQL variants and GPT-4 models) across
        %Spider and BIRD benchmarks. Unverified external results are marked with striped
        %patterns.}
    %\label{fig:eval:sota-landscape}
%\end{figure}

%\paragraph{Open-Source vs Proprietary Models}

%% TODO: Discuss open-source (OmniSQL 7B) vs proprietary (GPT-4) trade-offs
%% - Performance gap analysis
%% - Cost and accessibility advantages
%% - Privacy and control benefits
%% - Deployment feasibility differences

%\paragraph{Qualitative Comparison}

%% TODO: Beyond metrics, discuss systemic differences
%% - System architecture differences
%% - Component strategies (how others handle schema subsetting, refinement, etc.)
%% - Integration and deployment characteristics

\subsubsection{Baseline Performance Analysis}

\paragraph{OmniSQL Performance Gap}

A critical aspect of this evaluation is understanding the performance gap
between our measured OmniSQL 7B (GGUF) baseline and the reported OmniSQL results.
Figure~\ref{fig:eval:omnisql-difference} shows the difference between our
measured performance using OmniSQL 7B and the reported values from the OmniSQL
paper.

%\begin{figure}[h]
    %\centering
    %\includesvg[width=\textwidth]{assets/out/omnisql-difference.svg}
    %\caption{Performance gap between measured OmniSQL-7B-gguf and reported
        %OmniSQL-7B results across benchmarks. Positive values indicate higher
        %reported performance, negative values indicate higher measured performance.}
    %\label{fig:eval:omnisql-difference}
%\end{figure}

\paragraph{Cross-Benchmark Performance Comparison}

Figures~\ref{fig:eval:baseline-gap-spider-dev},
\ref{fig:eval:baseline-gap-spider-test}, and
\ref{fig:eval:baseline-gap-bird-dev} present per-benchmark performance
comparisons between OmniSQL (Quantized) and the OmniSQL (Unquantized) variant series.

\subsection{Qualitative Analysis}

% TODO: Introduction paragraph explaining value of qualitative evaluation beyond metrics

\subsubsection{Case Studies}

\paragraph{Success Cases}
% TODO: Present 3-5 exemplar success cases
% - Full trace: NL query → schema → examples → generated SQL → execution results
% - Analysis of what made these queries succeed
% - Demonstrate system capabilities at their best
% - Show component contributions in action

\paragraph{Failure Cases}
% TODO: Present 3-5 exemplar failure cases
% - Full trace showing where system failed
% - Root cause analysis: schema linking error? Semantic error? Syntactic error?
% - What component(s) failed?
% - Could failure be prevented with different design choices?

\paragraph{Edge Cases and Boundary Conditions}
% TODO: Present interesting edge cases
% - Very ambiguous queries
% - Queries requiring domain knowledge
% - Queries with multiple valid interpretations
% - Queries at the limit of SQL expressiveness
% - How system behaves at boundaries

\newpage
