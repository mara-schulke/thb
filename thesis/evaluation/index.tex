\newcommand{\EA}{$\mathbb{EA}$}
\newcommand{\EM}{$\mathbb{EM}$}
\newcommand{\ER}{$\mathbb{ER}$}
\newcommand{\CL}{$\mathbb{CL}$}
\newcommand{\Natural}{\textsc{Natural}}
\newcommand{\OmniSQL}{\textsc{OmniSql}}
\newcommand{\Spider}{\textsc{Spider}}
\newcommand{\Bird}{\textsc{Bird}}

\section{Evaluation}

The evaluation phase of \Natural builds upon the benchmarking
infrastructure outlined in section~\ref{impl:benchmark}. This section is
evaluating benchmarking data gathered while running \Natural in
different configurations on two prevalent benchmarking datasets:
\Spider and \Bird. A representative baseline is introduced,
aiming to mirror the base-model performance. Subsequently an ablation study is
performed to understand the performance impact of individual pipeline components.

\subsection{Experimental Methodology}

\subsubsection{Test Environment}

The tests are performed on a consumer-grade linux system, as research- or
enterprise-grade hardware was not available. Thus only a subset of datasets and
configurations have been measured, as a single benchmark executions on prevalent
datasets takes 12-36 hours.

\paragraph{Hardware Configuration}

The system used for benchmarking has a NVIDIA RTX 3090 GPU with 24GB VRAM
available, an AMD 9990X3D CPU with 12-cores up to 5.5GHz, 64GB of RAM at
6400mt/s and 1TB of SSD storage at a read speed of 7450 MB/s and a write speed
of 6900 MB/s.

\paragraph{Software Stack}

The software stack running on the system used for benchmarking is NixOS (25.05)
with a recent linux kernel version (6.17.7). CUDA 13.0 is being used for
inference on GPUs and a rust compiler version is 1.91 for compiling
\Natural. The models used during benchmarking are \textsc{OmniSql 7B
Q8\_0} and \textsc{Qwen3 Embedding 8B Q4\_K\_M} for generation and embedding
respectively.

\subsubsection{Benchmark Datasets}

The benchmarking datasets used are the \Spider and \Bird
testdatasets that are prevalently used for evaluation of NL2SQL systems.
Comparable research has frequently published performance metrics
(\textsc{Execution Accuracy} and \textsc{Exact Match}) on these benchmarks
which makes the performance of \textsc{Natural} comparable to other works.

\paragraph{Spider}

\textsc{Spider} is likely the most prevalently used benchmark for NL2SQL
systems in contemporary research. The test dataset is comprised of 10,181
questions and 5693 corresponding SQL queries spread across 200 databases
spanning 138 domains. Questions in \Spider are categorized by
difficulty (easy, medium, hard, extra hard). Evaluations on \Spider
refer to the test dataset.

\paragraph{Bird}

\Bird is another widely used benchmark that is comprised of 12,751
question answer pairs spread across 95 databases in 37 domains. It aims to be
more representative of real world scenarios with external knowledge
requirements and is generally considered harder than \Spider.

\subsubsection{Evaluation Metrics}

The chosen set of evaluation metrics used is a mixture of semantic metrics
(\EA, \EM) and functional metrics (\ER, \CL) to gather a hollistic picture of
system behavior and real world applicablility.

\paragraph{Execution Accuracy (\EA)}

Execution Accuracy (or \EA) is the primary success metric of
NL2SQL systems as it represents the semantic accuracy of the SQL queries
produced NL2SQL systems. Execution accuracy is computed by determining whether
the rows in the candidate results are a permutation of the rows returned by
ground truth results. 

For a query $q_g$ with candidate results $R_c$ and ground truth results $R_g$, 
where $R_c, R_g \subseteq \mathbb{V}^{n \times m}$ (sets of $n$ rows with $m$ columns 
of values $\mathbb{V}$), semantically accurate execution is defined as:

\begin{equation}
    semanticeq(R_c, R_g, q_g) = \mathbb{1}\left[\exists \pi \in \Pi_m : 
    \begin{cases}
        R_c = \pi(R_g) & \text{if } ordered(q_g) \\
        R_c \equiv_{\text{multiset}} \pi(R_g) & \text{otherwise}
    \end{cases}
    \right]
\end{equation}

where $\Pi_m$ is the set of valid column permutations (those preserving column
value sets), $\pi(R_g)$ applies permutation $\pi$ to reorder columns in each
row of $R_g$, $\equiv_{\text{multiset}}$ denotes multiset equality (allowing
row reordering) and $ordered(q_g)$ is determining if the ground truth query
$q_g$ contains an \texttt{ORDER BY} clause.

In order to compute the \EA on \Spider and \Bird the official evaluation suite
is used respectively to ensure comparability across externally reported
measurements and locally observed measurements.

\paragraph{Exact Match (\EM)}

Exact Match (or \EM) measures the syntactic equivalence between
candidate and ground truth SQL queries after normalization (ie, whitespace
trimming, casing adjustments etc). Unlike execution accuracy which validates
semantic correctness through result comparison, exact match determines whether
the generated query structurally matches the reference query. \EM acts as a
lower bound for execution accuracy as queries must be identical which is
therefore more restrictive as semantically equivalent queries with different
syntax (ie, using a different join order) are marked as incorrect.

\paragraph{Error Rate (\ER)}

Error Rate (or \ER) describes the system reliability by measuring the
frequency of SQL generation failures that prevent query execution. \ER is
reported in failures per hundred queries and monitors system reliability
(schema violations, syntax errors, out-of-memory errors etc).

\paragraph{Candidate Latency (\CL)}

Candidate Latency (or \CL) measures the end-to-end execution
time of a system from natural language input to SQL candidate output, reported
in seconds. This metric captures the compound runtime of all pipeline
components ($\sigma, \phi, \pi, \rho, \nu$) and reflects real-world system
responsiveness. Latency increases with pipeline complexity, particularly when
example selection and self refinement are applied, thus representing relative
computational cost.

\subsubsection{Baselines}

To adequately determine the performance impact of methods and components
applied in \Natural a baseline helps to measure relative performance
gains or losses compared to existing systems. Therefore two sets of baseline
systems are introduced: Internal baselines (measuring the system without
crucial components) and external baselines (eg, proprietary system
performance and raw model performance).

\paragraph{Internal Baselines}

Introducing two internal baselines subsequently allows for a brief ablation
study, where the impact of different pipeline configurations and different
sampling datasets can be measured to isolate the contribution of each
configuration. The \textit{Baseline} configuration of \Natural is using
only the inference logic ($\pi$) without all other pipeline components,
representing the performance contribution of \textsc{OmniSQL} as closely as
possible. Additonally the \textit{Zero-Shot} configuration of \Natural
is introduced and measures the performance of \Natural with all
pipeline components activated but without any examples available to use during
in-context learning.

\paragraph{External Baselines}

Using published results from other systems, \Natural can be briefly
compared against existing systems with similar capabilities highlighting
relative performance improvements or losses. Notably external baselines largely
rely on unverified data from other papers. Given that \Natural is
largely based on \OmniSQL, GPT-4 and \OmniSQL are the primary
external systems are taken into account as baselines.

\subsubsection{Vector Databases}

Three different vector databases are introduced per benchmark. These differ in
the datasets using during sampling:

\paragraph{Synthetic}

The \textit{Synthetic} vector databases are sampled from the
\textsc{SynSQL-2.5m} dataset introduced by \citeauthor{OmniSQL} in
\citeyear{OmniSQL}. The \textsc{SynSQL-2.5m} dataset is largely unrelated to
\textsc{Spider} and \textsc{Bird} but covers a wide array of domains through
its LLM-based synthetic data generation approach.

\paragraph{Train}

The \textit{Train} vector databases are sampled from the respective training
splits from \textsc{Spider} and \textsc{Bird} \citep{Spider, BIRD}. The train
splits include similar style of SQL queries and similar domains but different
databases and different questions.

\paragraph{Ground}

The \textit{Ground} vector databases are sampled from the respective splits
used during evaluation (dev and test) from \textsc{Spider} and \textsc{Bird}
\citep{Spider, BIRD}. These allow \textsc{Natural} to reference ``previously
used'' answers as examples during generation indicating the potential
upper-limit of performance during a self-learning deployment which has access
to past conversations.

\subsection{Benchmark Results}

This section is presenting the measured performance across all \textsc{Natural}
configurations and benchmark datasets. Internal ablations are compared
(Baseline, Zero-Shot, as well as different vector DBs) against external prior
art (GPT-4, \OmniSQL). Subsequently performance characteristics and improvement
trends are quantitatively and qalitatively analyzed.

\subsubsection{Overview}

The overall benchmarking results are presented in
Table~\ref{tab:eval:overall-results} and visualized in
Figures~\ref{fig:eval:execution-accuracy-overview}
and~\ref{fig:eval:exact-match-overview}, which displays the the performance for
all system configurations across the \textsc{Spider} and \textsc{Bird}
datasets, the execution accuracy ($\mathbb{EA}$), exact match ($\mathbb{EM}$),
error rate ($\mathbb{ER}$) and candidate latency
($\mathbb{CL}$) metrics.

\begin{table}[ht]
    \centering
    \hspace*{-1.25mm}
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
        \toprule
        \textbf{System}     & \multicolumn{4}{c|}{\textbf{Spider (dev)}}                       & \multicolumn{4}{c|}{\textbf{Spider (test)}}                       & \multicolumn{4}{c}{\textbf{BIRD (dev)}}                            \\
        \midrule                                                                                                                                                 
                            & \EA             & \EM            & \ER           & \CL           & \EA             & \EM          & \ER              & \CL            & \EA           & \EM              & \ER            & \CL           \\
        \midrule
        \multicolumn{13}{c}{\textbf{Closed-Source LLMs}} \\                                                                                                                                                                          
        \midrule                                                                                                                                                                                                                     
        GPT-4o-mini*        & 71.0            & -              & -             & -             & 83.7           & -             & -                & -              & 61.5          & -                & ?              & -             \\
        GPT-4-Turbo*        & 72.2            & -              & -             & -             & 84.2           & -             & -                & -              & 63.6          & -                & ?              & -             \\
        GPT-4o*             & 70.7            & -              & -             & -             & 84.9           & -             & -                & -              & 64.0          & -                & ?              & -             \\
        \midrule
        \multicolumn{13}{c}{\textbf{Open-Source LLMs}} \\                                                                                                                                                                            
        \midrule
        OmniSQL-7B*         & 81.6            & -              & -             & -             & 89.8           & -             & -                & -              & 66.1          & -                & ?              & -             \\
        OmniSQL-14B*        & 82.0            & -              & -             & -             & 88.3           & -             & -                & -              & 65.9          & -                & ?              & -             \\
        OmniSQL-32B*        & 80.9            & -              & -             & -             & 89.8           & -             & -                & -              & 67.0          & -                & ?              & -             \\
        OmniSQL-7B-gguf     & 79.0            & 33.0           & 0.1           & \textbf{6.6s} & 79.0           & 35.7          & 0.3              & \textbf{6.7s}  & 38.0          & 0.03             & 0.8            & \textbf{8.2s} \\
        \midrule
        \multicolumn{13}{c}{\textbf{Pipelines}} \\                                                                                                                                                                          
        \midrule
        \Natural~(Baseline)  & 77.9           & 30.3           & \textbf{0.0}  & 7.3s          & 77.0           & 32.5          & \textbf{0.0}     & 7.3s           & 32.4          & 0.03             & \textbf{0.3}   & 9.3s          \\
        \Natural~(Zero-Shot) & 78.7           & 27.1           & 0.2           & 14.0s         & 77.5           & 32.7          & 0.4              & 15.3s          & 34.0          & 0.03             & 0.4            & 18.7s         \\
        \Natural~(Syn)       & \textbf{81.0}  & 41.2           & 0.3           & 16.3s         & 79.6           & 38.7          & 0.4              & 15.4s          & \textbf{53.8} & \textbf{18.8}    & 0.2            & 43.7s         \\
        \Natural~(Train)     & 80.4           & \textbf{42.2}  & 0.6           & 16.2s         & \textbf{81.4}  & \textbf{43.9} & 0.3              & 15.8s          & 48.4          & 13.2             & 0.3            & 33.7s         \\
        \Natural~(Ground)**  & 84.2           & 42.7           & 0.7           & 16.1s         & 82.8           & 39.5          & 0.3              & 16.0s          & 55.8          & 19.3             & 0.2            & 43.8s         \\
        \bottomrule
    \end{tabular}
    \caption{
        Comprehensive benchmark results across all systems and datasets.
        \EA~and \EM~are reported as percentages. \ER~is reported in failures
        per hundred queries and \CL~is reported in seconds. Systems marked with
        * are external benchmarks with unverified results from published
        papers. Systems marked with ** had access to ground truth data during
        inference, illustrating the upper bound achievable. Bold values
        indicate best verified performance, excluding systems with access to
        ground truth. \citep{OmniSQL}
    }
    \label{tab:eval:overall-results}
\end{table}

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/execution-accuracy-overview.svg}
  \caption{Execution accuracy overview across all systems and benchmarks. The \textit{Syn} configuration achieves the highest verified performance on \Spider~(dev) at 81.0\% and \Bird~(dev) at 53.8\%, while \textit{Train} leads on \Spider~(test) at 81.4\%.}
  \label{fig:eval:execution-accuracy-overview}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/exact-match-overview.svg}
  \caption{Exact match overview across all systems and benchmarks. ICL configurations demonstrate substantial improvements over baselines, with \textit{Train} achieving 43.9\% on \Spider~(test), the highest verified exact match rate across all benchmarks.}
  \label{fig:eval:exact-match-overview}
\end{figure}

\subsubsection{Baseline Performance Analysis}

One notable insight during benchmarking was the apparent gap between the
and the locally measured performance of \OmniSQL~7B (using GGUF with the F16
weights) and the reported accuracy metrics results from \cite{OmniSQL}.
As shown in Figure~\ref{fig:eval:omnisql-gap}, the $\delta$ in performance between the local measurement and the official
values ranges from 2.6\% on \Spider~(dev) to 11.8\% on \Spider~(test) and
28.1\% on \Bird~(dev).

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/omnisql-difference.svg}
  \caption{
    Performance gap between measured OmniSQL-7B-gguf and reported
    OmniSQL-7B results across benchmarks. The $\delta$ ranges from 2.6\% on
    \Spider~(dev) to 28.1\% on \Bird~(dev), highlighting differences between
    quantized local deployment and published full-precision results.
  }
  \label{fig:eval:omnisql-gap}
\end{figure}

Several factors may contribute to this stark gap between the two systems.
First, model quantization can come with a cost and different quantization
variants and model formats come with different performance characteristics. The
OmniSQL-7B-gguf system uses the F16 weights but transformed into the GGUF
model format while published results presumably use full-precision models in
the safetensors formats. Prior research has shown that bit-wise quantization
can degrade model accuracy, particularly on complex reasoning tasks on
multi-billion paramater models \citep{Int8AtScale} and it is yet unclear
whether the mere transformation into GGUF harms performance for this model
series. Second, the exact hyperparameters used during evaluation likely
differed between \cite{OmniSQL} and local benchmarks which may influence the
model accuracy in unforseen ways. Unless the exact same inference environment,
quantization and hyperparameters the performance difference might be caused by
a multitude of subtle differences. Finally, dataset version differences or
preprocessing variations could contribute to divergent results.

The \textit{Baseline} configuration of \Natural~further showed degredation of
performance compared to the OmniSQL-7B-gguf system configuration. Using the
\textit{Baseline} configuration, \Natural~only uses the inference logic and
prompt template for ICL ($\pi$) without other pipeline components, achieving
77.9\% \EA~on \Spider~(dev), 77.0\% on \Spider~(test), and 32.4\% on \Bird
(dev). These results further fall below the published \textsc{OmniSQL-7B}
performance of 81.6\%, 89.8\%, and 66.1\% respectively, representing gaps of
3.7, 12.8, and 33.7 percentage points. This continued degredation of
performance indicates that a misaligned prompt template (ie, divergent from
training phase) harms performance if not paired with counter measures such as
ICL, self-correction or majority voting. These performance numbers mark the
\textit{Baseline} configuration the worst performing variant of \Natural~on
\EA~and \EM~across \Spider~(dev and test) and \Bird~(dev).

\subsubsection{Spider Results}

The results of the \Spider~benchmarks demonstrate consistent performance
characteristics of \Natural~and \OmniSQL~across the development and test splits
(detailed breakdown in Appendix~\ref{sec:appendix:benchmark-results-spider}).
The OmniSQL-7B-gguf system outperforms the two \Natural~baseline configurations
\textit{Baseline} and \textit{Zero-Shot} in \EA~and \EM by 2.1\% and 0.3\% on
the development split and by 2.0\% and 1.5\% on the test split respectively.

The full pipeline with the \textit{Zero-Shot} configuration (all components
active but no examples) hence only yields marginal improvements (+0.8pp on dev,
+0.5pp on test) over \textit{Baseline}, suggesting that schema subsetting and
query refinement contribute minimally without example-based guidance. Notably,
as demonstrated in Figure~\ref{fig:eval:candidate-latency}, the \CL of
\textit{Zero-Shot} increases by 6.7s on dev and 8.0s on test over
\textit{Baseline}, doubling the system latency without a significant
improvement in system performance.

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/candidate-latency-spider.svg}
  \caption{
    Candidate latency across system configurations on \Spider~benchmarks. The
    \textit{Zero-Shot} configuration shows doubled latency (14.0s on dev, 15.3s
    on test) compared to \textit{Baseline} (7.3s) without commensurate
    performance gains, while ICL configurations incur additional overhead from
    example retrieval.
  }
  \label{fig:eval:candidate-latency}
\end{figure}

Introducing vector database access to the system configurations demonstrates
substantial improvement in performance. The two configurations without ground
truth examples showed a significant improvement in both \EA~and \EM:
\textit{Syn} outperformed all other system variants and baselines with a
accuracy improvement of 3.1\% over \textit{Baseline} and 2.0\% over
OmniSQL-7B-gguf on \Spider~(dev). On \Spider~(test) the \textit{Train}
configuration outperformed \textit{Syn} by 1.8\% in \EA. For \EM~both system
configurations show a steep improvement over the three baselines with largest
\EM~deltas being observed between \textit{Zero-Shot} and \textit{Train} with
+15.1\% on \Spider~(dev) and between \textit{Baseline} and \textit{Train} with
+21.4\% on \Spider~(test). These double digit improvements in accuracy point
towards the conclusion that in-context learning is an effective mechanism for
further improving the performance of already fine-tuned models. The performance
of the \textit{Ground} configuration shows the theoretical upper limits in
\EA~achievable on top of \OmniSQL~with example selection algorithms with 84.2\%
on \Spider~(dev) and 82.8\% on \Spider~(train)
(see~\ref{sec:appendix:benchmark-results-spider}).

\begin{figure}[htbp]
  \centering
  \includesvg[width=\textwidth]{assets/out/error-rates.svg}
  \caption{
      Error rates across system configurations. All configurations
      maintain error rates below 0.7 failures per hundred queries, indicating that
      failures are predominantly semantic (incorrect results) rather than
      syntactic or runtime related (malformed SQL, out of memory errors).
  }
  \label{fig:eval:error-rates}
\end{figure}

Error rates remain consistently low across all configurations (<0.7 failures
per hundred queries), see ~\ref{fig:eval:error-rates}, indicating that failures
are predominantly semantic (incorrect results) rather than syntactic (malformed
SQL), which validates the robustness of \OmniSQL's SQL generation capabilities.

\subsubsection{BIRD Results}

Measurements on the \Bird benchmark, which is designed to represent real-world
scenarios with imperfect data, external knowledge requirements and hard to
reason about database schemas yields significantly lower absolute performance
numbers but shows fundamental improvements through ICL based systems.
While the \textit{Baseline} configuration of \Natural~only achieves 32.4\%
\EA~(less than half the accuracy than on \Spider~with 77.9\% and 77.0\% for dev
and test splits respectively). This poor baseline performance confirms \Bird
being a significantly more challenging benchmark. While the \textit{Zero-Shot}
configuration showns incremental improvements over the baseline with a relative
improvement of +1.6pp in \EA~(34.0\% absolute), the overall performance shows
to be behind the reported numbers of \OmniSQL~(7B) and GPT-4 by \cite{OmniSQL}
which were reported to had an \EA of 66.1\% and 61.5\% respectively. The
locally reproduced measurement of \OmniSQL~(7B-gguf) yielded a drop in \EA~of
-28.1pp compared to the officially reported \EA~yielding 38.0\% in absolute
accuracy.

Both the \textit{Syn} and \textit{Train} configurations of \Natural~outperform
all baselines. \textit{Syn} showns an improvement of +21.4pp over
\textit{Baseline} and +15.8pp over \OmniSQL~(7B-gguf), achieving 53.8\% \EA,
the best local measured system excluding \textit{Ground}. \textit{Train} yields
improvements of +16.0pp over \textit{Baseline} and +10.4pp over
\OmniSQL~(7B-gguf).

This shows a stark contrast with \textsc{Spider}, where synthetic examples
provided only +3.1pp improvement on dev and +2.6pp improvement on train.
The effectiveness of synthetic examples on \Bird~suggests that the impact of
ICL increases with query and schema complexity. When queries involve complex
joins, nested subqueries, and aggregations, even domain-agnostic examples
provide crucial scaffolding for the generation. The \textit{Train}
configuration of \Natural~underperformed \textit{Syn} by 5.4pp. This may
indicate that \textsc{Bird}'s training examples, while domain-relevant, contain
different structural patterns than the development set, whereas
\textsc{SynSQL-2.5m} contains a broader coverage of different database schema
structures to reference from.

The other metrics \EM, \ER~and \CL~further show a stark difference between
\Spider~and \Bird. Systems which mostly relied on \OmniSQL~(\Natural
\textit{Baseline} and \textit{Zero-Shot} and \OmniSQL~(7B-gguf)) showed the
same 0.03\% in \EM indicating a potential overfitting of \OmniSQL~to the SQL
style used in \Spider. Notably ICL based systems showed a significant
improvement with +13.17pp for \textit{Train} and +18.77pp for \textit{Syn}.
The \CL~metric showed a similar pattern to the \Spider~benchmarks with the \CL
increasing with pipeline complexity from 9.3s for \textit{Baseline} to 43.8s
for \textit{Ground}.

Lastly the \textit{Ground} configuration establishes an upper bound of
55.8\% \EA, representing a +23.4pp improvement over \textit{Baseline} (with a
+72\% relative gain). Despite these substantial relative improvements within
the recorded benchmarks a significant gap of -10.3pp and -8.2pp remains when
compared to the published \OmniSQL~(7B) performance with 66.1\% and GPT-4o
performance with 64.0\%. This stark difference between local replication and
officially reported performance metrics is yet unclarified.

\subsection{Performance Characteristics}

To assess the performance of NL2SQL systems hollistically metrics beyond
semantic accuracy have to be taken into account. The two performance
characteristics \ER~and \CL~provide insight into system reliability and their
real-world applicability. These functional metrics complement accuracy by
revealing operational constraints (such as hardware requirements or
reliability) that affect deployment feasibility.

\subsubsection{Error Rate Analysis}

The error rate results demonstrate great reliability across all benchmarked
systems and datasets. The error rates remain below 0.7\% (fewer than 7 failures
per 1000 queries) across all measured systems, with the \textit{Baseline}
achieving 0.0\% error rate on \Spider~(dev) and 0.3\% on \Bird~(dev). Even
\Natural~configurations with self-refinement, query parsing and ICL maintain
low error rates: \textit{Ground} shows an \ER~of 0.7\% on \textsc{Spider}
(dev), 0.3\% on \textsc{Spider} (test) and 0.2\% on \textsc{Bird} (dev).

This consistency indicates that the \OmniSQL~model generates syntactically
valid, executable SQL with a high reliability, and that pipeline components do
not introduce significant failure modes.

Furthermore these low error rates reveal an important characteristic of
\OmniSQL~and \textsc{Natural}: Errors of these NL2SQL systems are predominantly
semantic (ie, wrong results) rather than syntactic (eg, malformed SQL or schema
violations) or runtime-related.

This distinction matters for production deployments, as semantic errors are
significantly harder to validate and catch than hard system failures. Semantic
accuracy might be further improved through user feedback which can feedback
into ICL. 

\subsubsection{Latency Analysis}

Measuring the \CL~reveals the computational trade-offs that come with the
the \textsc{Natural} pipeline architecture. The \textit{Baseline} configuration
achieves minimal candidate latency (6.6s on \textsc{Spider} dev, 6.7s on test,
8.2s on \textsc{Bird} dev), representing the time required for schema
subsetting and SQL generation without example retrieval or refinement.

The \textit{Zero-Shot} pipeline configuration approximately doubles the end to
end latency to (14.0s, 15.3s, 18.7s respectively) highlighting that the
schema subsetting, self-refinement and voting components contribute to a
significant increase in computational cost. Configurations with example
selection and in-context learning show further latency increases. The
\textit{Train} and \textit{Syn} configurations achieve 15s-17s \CL~on
\textsc{Spider} and 33s-44s on \textsc{Bird}, with the \textit{Syn}
configuration yielding the highest latency (43.7s on \textsc{Bird}).

Overall \OmniSQL~(7B-gguf) consistently showed the lowest \CL~values with
(6.6s, 6.7s and 8.2) for \Spider~(dev and test) and \Bird~respectively. This is
unsurprising as the \OmniSQL~(7B-gguf) system provides a realistic estimation
for the $\pi$ component of \Natural.

These characteristics can inform potential hardware requirements or deployment
decisions. For interactive applications sub-second response times of these
models are recommended. \Natural~clearly exceeds these thresholds by a factor
of up to 45 on complex databases and questions (eg, \Bird). Thus either
significantly better hardware is required, a simpler configuration needs to be
used or further performance optimizations need to be introduced.
