\section{Discussion}

This section is linking the evaluation results back to the research questions
of this thesis which where outlined in the introduction section. It is
discussing the findings of this thesis and deriving recommendations for actions
and future research work in the NL2SQL community.

% TODO cleanup

\subsection{Summary of Results}

The evaluation of \Natural~demonstrated the capabilities of open-source LLMs
and showed that they can achieve competitive performance on prevalent structured
NL2SQL benchmarks such as \Spider~and \Bird. Nonetheless significant
challlenges remain for an enterprise-scale adoption of NL2SQL systems.
\Natural~achieves up to 81.0\% on the \Spider~development split using the
\textit{Syn} configuration and 81.4\% on the test split using the
\textit{Train} configuration which represents a +3.1pp and +4.4pp improvement
over the 77.9-77.0\% performance of \OmniSQL-7B respectively
(Table~\ref{tab:eval:overall-results}).

On the complex \Bird benchmark, the \textit{Syn} configuration reaches a 53.8\%
\EA which represents a +21.4pp improvement over the 38.0\% achieved by the
\textit{Baseline} configuration — representing a 66\% relative gain in
execution accuracy. These results position \Natural as competitive with the
locally-reproduced \OmniSQL-7B-gguf baseline (79.0\% on \Spider~(dev), 79.0\%
on \Spider~(test), 38.0\% on \Bird) while remaining below published accuracy
scores of closed-source model results (GPT-4o: 84.9\% \Spider, 64.0\% \Bird).

The ablation study performed during evaluation reveals that pipeline components
tend to interact in synergy rather than additively, with in-context learning
and the example selection algorithm of \Natural serving as the crucial
differentiator for performance. The \textit{Zero-Shot} configuration which
uses schema subsetting, self-refinement, and majority voting but does not apply
example selection achieves only a +0.8pp improvement on \Spider
despite doubling the candidate latency from 7.3s to 14.0s. \Natural~
configurations with active ICL and example selection yield up to +2.3pp on
\Spider~dev, +3.9pp on \Spider~test and and +19.8pp on \Bird compared to
\textit{Zero-Shot}. This shows that both schema subsetting and self-refinement
techniques provide minimal benefit without in-context learning and example
selection. Further ablation would help to understand the potential cross
effects of components and the implications of disabling pipeline
components like schema-subsetting, self-refinement and majority voting.
Furthermore increasing the pipeline iterations from 1 to $k$ would yield
insights into self balacing and self correction across iterations of \Natural.

A comparison of different example sources shows that structurally diverse
synthetic examples are not necessarily performing worse than examples from the
same dataset using the \Natural~example selection algorithm. Synthetic examples
outperformed domain-similar training examples on \Bird by 5.4pp (53.8\% vs.
48.4\%), while perfect example selection yielded 55.8\% accuracy and thus adds
+2.0pp over schema-aware selection. This indicates diminishing returns of
in-context learning from increasingly sophisticated example strategies.

Observed performance characteristics reveal a complex tradeoff for real-world
applications. While all NL2SQL system that were evaluated maintain remarkably
low error rates of less than 0.7 failures per 1000 queries (see
Figure~\ref{fig:eval:error-rates}) errors are therefore becoming predominately
semantic (ie. incorrect results). Furthermore candidate latency ranges from
7.3s for \Natural~(\textit{Baseline}) to up to 16.3s for ICL configurations on
\Spider and 43.8s on \Bird. This yields \Natural viable for experimental,
analytical workflows and asynchronous report generation but unviable for
interactive systems like chatbot applications. While performance optimizations
could be applied the hardware used is the primary bottleneck for the speed that
is achievable as the majority of \CL is attributable to inference.

Lastly the \EA~gap of 28.1pp between the published \OmniSQL-7B results (66.1\%
on \Bird) and local measurements (38.0\%) raise concerns to the reliability of
measurements and highlight systemic challenges in LLM-based research. The
analysis of this gap concludes that it is likely attributable to quantization
differences, inference engine variations and prompt template / code formatting
sensitivity. Despite this remaining uncertainty in absolute performance
comparisons, clear relative improvements from pipeline composition are
observable and reproducible within a controlled environment.

\subsection{Answering the Research Questions}

To conclude this thesis, the research questions that were outlined in section
\ref{section:introduction:research-questions} are answered.

\subsubsection{Research Question 1: To what extent can open-source LLMs achieve
competitive NL2SQL performance through pipeline composition and optimization?}

The capabilities of open-source NL2SQL models achieve \textit{partially
competitive} performance on prevalent, structured NL2SQL benchmarks. Especially
through the composition of different algorithms the performance can be
significantly improved. The evaluation performed in this thesis demonstrates
that a combination of in-context learning, schema subsetting, self-refinement
and majority voting boosts the competitiveness of fine-tuned base models.
Nonetheless significant gaps in execution accuracy remain for enterprise-level
real-world deployments.

\Natural achieves up to 81.0\% \EA~on the \Spider~(dev) benchmark using the
\textit{Syn} configuration (examples selected from the SynSQL-2.5m dataset,
Table~\ref{tab:eval:overall-results}). This represents a +3.1pp increase
improvement over \textit{Baseline} configuration and a +2.0pp increase over the
baseline model performance of \OmniSQL-7B-gguf. On the test set of \Spider~the
\textit{Train} configuration is outperforming the \textit{Syn} configuration by
+1.8pp with an \EA~of 81.4\%. \textit{Train} is further showing an +2.4pp
increase over \OmniSQL-7B-gguf. On the more complex \Bird~benchmark
\textit{Syn} outperforms \textit{Train} by +5.4pp in \EA~and \OmniSQL~7B-gguf
by +15.8pp. However, on all three benchmarks noticable performance gap ranging
from 2.6pp to 28.1pp was observed which reduces the trust in the measured
results.

This yields an overall unclear picture, with \Natural~showing strong
performance gains over the locally measured baseline model performance of
\OmniSQL but no absolute gain over the published results for \OmniSQL and other
external systems by \citeauthor{OmniSQL}. For definite conclusions on the
competitiveness of local LLM-based NL2SQL systems more research is needed.

\paragraph{Pipeline Composition Effectiveness}

Notably, the pipeline composition had notable impacts on the performance on
NL2SQL benchmarks. The effectiveness of more complex pipeline configurations
scaled drastically with database and benchmark complexity. On simpler
benchmarks like \Spider~only marginal, single digit accuracy gains (+2.0pp and
+2.4pp) could be observed while approximately doubling the candidate latency
(from 6.6s and 6.7s to 16.3 and 15.4s).

On \Bird, the difference between the evaluated configurations of \Natural is
significant. \textit{Baseline} achieved only about 32.4\% \EA~while
\textit{Syn} achieved 53.8\% \EA~, representing a 66\% relative gain over
baseline performance through an altered pipeline composition.

The ablation study shows that the effectiveness of a pipeline configuration 
heavily depends on in-context learning and example selection, as the
\textit{Zero-Shot} configuration showed only marginal gains (+0.8pp on
\Spider~(dev), +0.5pp on \Spider~(test) and +1.6pp on \Bird) over the
\textit{Baseline} configuration while effectively doubling the candidate
latency from 7.3s to 14.0s on \Spider~(dev), 7.3s to 15.3s on \Spider~(test)
and 9.3s to 18.7s on \Bird. This data is showing that a naive composition
strategy of simply enabling all available algorithms is not cost effective
and might produce counter productive returns on user exerpience as the latency
increases are significant.

Lastly the \textit{Ground} configuration of \Natural~establishes a theoretical
performance ceiling using perfect data during example selection and
consistently performed best-in class during local measurements. \textit{Ground}
achieved 84.2\% on \Spider~(dev), 82.8\% on \Spider~(test) and 55.8\% on \Bird,
representing a relative gain to the next-best performing configuration of
\Natural of +3.4pp, 1.4pp and +2.0pp respectively. This shows that in-context
learning and example selection strategies have their limits and are already
performing close to them. Further improvements would likely require better
foundation models or a different architecture alltogether. A visual comparison
across the different configurations
(Figure~\ref{fig:eval:execution-accuracy-overview}) confirms that
in-context-learning-based approaches (\textit{Syn}, \textit{Train},
\textit{Ground}) outperform other configurations with diminishing
returns from increasingly sophisticated example selection strategies.

\paragraph{Comparison to Closed-Source Baselines}

As no direct local comparison of \Natural~against closed-source models has been
performed, no definite answer can be given to the performance gap between
local NL2SQL systems and external, closed-source systems. The published results
by \citeauthor{OmniSQL} in \citeyear{OmniSQL} report GPT-4o achieving 84.9\%
execution accuracy on \Spider test and 64.0\% on \Bird development
\citep{OmniSQL}. Compared to local measurements of the best-in class
configuration of \Natural compared to GPT-4o shows a performance $\Delta$ of +10.3pp
on \Spider~(dev), -3.5pp on \Spider~(test) and -10.2pp on \Bird. Generally this
suggests competitive but not superior performance, although it must be
interpreted cautiously as the performance gap between the results reported by
\cite{OmniSQL} and local measurements are non trivial. Given the uncertainties
in measurements this thesis can't claim a definite competitiveness compared to
closed-source models. Further evaluations are needed to clarify the performance
characteristics of all measured systems. Nonetheless, \Natural~presents a
portable, at least partially competitive, alternative to closed-source systems.

\paragraph{Accuracy Ceiling on Consumer Hardware}

As \Natural~was developed and evaluated on consumer hardware, the hardware
constraints induced by it imply a ceiling of possible model size and speed
characteristics. The RTX 3090 has 24GB VRAM and can theoretically accomodate
\OmniSQL~7B and \OmniSQL~14B but given that \Natural~is using additional
embedding models and leaves 2-4GB headroom for KV-caches etc. to prevent
out-of-memory crashes. Therefore only the \OmniSQL~7B model (15GB without
quantization) is supported unless heavier quantization formats are used.

This makes \Natural accessible to consumers with high-end consumer-grade
hardware (~\$1,500 as of 2024-2025) without requiring enterprise-level
hardware. The possible performance impact of using larger foundation models
such as 14B and 32B was not evaluated. For enterprise real-world scenarios 
larger base models could be used if the available hardware allows for it.
The evaluation performed in this thesis suggests that with the selected model
sizes and the available hardware \Natural~has a theoretical execution accuracy
ceiling of 84.2\% and 82.8\% on \Spider~(dev and test) and 55.8\% on \Bird.
This marks an upper bound achievable with the 7B models and the
algorithms used. Therfore the foundation model capabilities emerge as the
limiting factor rather unless an entirely different pipeline architecture such
as agentic usage of LLMs is used. Nonetheless scaling the hardware and
parameter sizes to 14B or 32B would likely yield significant gains in execution
accuracy. 

The latency characteristics of \Natural~remains in the practical range for
non-interactive workflows but poses a significant challenge for local,
interactive applications. While the \textit{Baseline} configuration achieves
7.3s \CL, the optimal configurations of \Natural~(\textit{Syn} and \textit{Train})
only achieve a \CL~of 15.4-16.3s (Table~\ref{tab:eval:overall-results}).
This makes \Natural~viable for exploratory analytics and report generation but
not for interactive environments such as SQL learning or interactive natural
language driven data analyis platforms which would require sub-second latency.

\paragraph{Conclusion}

Overall, open-source LLMs \textit{can} achieve competitive NL2SQL performance
as fine-tuning and in-context-learning prove to be effective and efficient
mechanisms to achieve high accuracy values in constrained hardware
environments. Nonetheless the measured performance yields the systems to face
significant challenges in enterprise environments as an execution accuracy of
53.8\% on \Bird with a latency of 43.7s is unacceptable for end users. This
indicates that while promising there is still a gap that remains to be closed.

\subsubsection{Research Question 2: How do NL2SQL pipeline components interact,
and which configurations optimize the accuracy-latency tradeoff?}

\paragraph{Component Interaction Analysis}

The pipeline components $\sigma$, $\phi$, $\pi$, $\rho$ and $\nu$ were shown to
work best synergetically rather than additive with $\sigma$ representing the
primary performance driver of the pipeline $nq$. The ablation study perfomed
during evaluation reveals this through a systematic evaluation of the three
types of configurations: \textit{Baseline}, \textit{Zero-Shot} and
\textit{Full}. The evaluation results reveal the behaviour of the individual
pipeline components in synergy, where \textit{Zero-Shot} only provides a
marginal improvement over \textit{Baseline} (eg, +0.8pp on \Spider) while
nearly doubling the latency. All configurations with active ICL
demonstrated significant improvement over their non-ICL counterparts; The worst
results produced by ICL configurations compared to the best-performing non-ICL
configurations resulted in an improvement through ICL by +1.7pp on
\Spider~(dev), +2.1pp on \Spider~(test) and +14.4pp on \Bird. These results
reveal that the other pipeline components bring only a minimal benefit (+0.8pp
on \Spider~(dev), +0.5pp on \Spider~(test) and +1.6pp on \Bird) to the
\EA~performance of a pipeline while significantly impacting (ie. approximately
doubling) the \CL.

The underlying pattern of component interactions becomes even more apparent
through examining \Natural's failure modes. Without examples \Natural~(and thus
\OmniSQL) generate semantically inaccurate SQL candidates more often; usually
incorrect column or table selection, misunderstanding of column semantics and
value ranges or wrong join patterns result in syntactically correct queries
which result in wrong result sets. For example the schema subsetting component
is unable to work correctly if the initial query candidate is using the wrong
set of tables alltogether, the majority voting algorithm fails to select the
best candidate when all SQL candidates correspond to different result sets and
self-refinement can only rarely repair semantically mislead initial queries. 
In-context learning helps \Natural~to guide the foundational model by supplying
a set of highly relevant, ranked examples which steers the model to put its
attention on specific parts of the SQL schema. The schema-aware example
selection algorithm works particularly well and was shown to be effective even
on data that did not originate from the training splits of the corresponding
benchmarks (\ref{tab:eval:overall-results}). 

\paragraph{Component Contribution Ranking}

The above insights allow for a ranking of components by their estimated impact
based on the ablation evidence from Table~\ref{tab:eval:overall-results} and
Figure~\ref{fig:eval:execution-accuracy-overview}. The pipeline components rank
by impact as follows:

\begin{enumerate}
    \item \textbf{Query Projection ($\pi$)} – The query projection component
        itself remains the largest single contributor in performance and the
        foundation models used predominately drives the performance of the
        system. All other components are orchestrated around query projection.
        This was shown through the \textit{Baseline} configuration which
        achieved 77.9\% and 77.0\% on \Spider~(dev and test) and 32.4\% on
        \Bird. The substantial gap between the two benchmarks highlights how
        severly the model capabilities degrade with increasing task and schema
        complexity.
    \item \textbf{Example Selection ($\sigma$)} – As discussed above, the
        example selection provides a non-neglible improvement in performance to
        the overall system while introducing relatively little candidate
        latency (eg, only +0.1-+2.3s on \Spider). The impact that ICL showed on
        \Bird demonstrates that example selection is not only helpful for LLMs
        generating SQL querys but a \textit{necessary} measure for enabling
        them to handle complex query generation tasks on real-world data.
    \item \textbf{Consensus Voting ($\nu$)} – The consesus voting algorithm
        selects the most prevalent result set amogst multiple generation
        attempts and validates that queries execute without any errors prior to
        returning them to the user. This self-consensus mechanism ensures low
        syntactic error rates across all configurations
        (Figure~\ref{fig:eval:error-rates}). However the voting strategy only
        gains in overall performance impact with an increasing number of
        internal candidates. The evaluation was performed with 1-2 internal
        candidates due to computational complexity of increasing the pipeline
        iterations. This prevents definite conclusions on the impact.
    \item \textbf{Self-Refinement ($\rho$)} – Self-refinement is effectively
        prompting the underlying model for repairing or improving its initially
        produced query candidate. This results in an overall marginal
        improvement of \EA, but comes at a significant computational cost.
        Adding self-refinement nearly doubles the \CL on average and provides
        diminishing returns. The results of the \textit{Zero-Shot}
        configuration, especially on the \Bird~benchmark, suggest that
        self-refinement is unable to correct significant semantic errors on its
        own without in-context learning.
    \item \textbf{Schema-Subsetting ($\phi$)} – Schema subsetting aims to
        reduce the noise for the model by filtering out unused table from the
        SQL schema before running query projection, thus improving token
        efficiency and accuracy. However, the exact impact of schema-subsetting
        is difficult to isolate due to the constrained ablation study that was
        performed. It is questionable whether schema-subsetting itself provides
        a statistically significant improvement in performance or even harms
        performance as the \textit{Baseline} configuration underperformed the
        model baseline.
\end{enumerate}

\paragraph{Accuracy-Latency Tradeoff}

The accuracy-latency measurements which is visualized in
Figure~\ref{fig:eval:candidate-latency} and
Table~\ref{tab:eval:overall-results} reveal three different efficiency profiles
across the systems that where benchmarked.

\begin{enumerate}
    \item \textbf{Baseline} — The baseline profile includes both the
        \textit{Baseline} configuration of \Natural~and the raw \OmniSQL~models.
        The baseline profile is generally yielding the lowest possible
        candidate latency, but also yields the lowest scores for both
        \EA~and~\EM and is thus prioritising latency over accuracy.
    \item \textbf{Zero-Shot} — The zero shot profile is denoted by a suboptimal
        accuracy latency tradeoff where there are marginal performance gain
        over the baseline results but this is accompanied by a strong average
        increase in latency (effectively doubling on average).
    \item \textbf{Full} — Lastly the full profile is a balanced profile which
        weights accuracy over latency. Systems in this class include the
        \textit{Syn}, \textit{Train} and \textit{Ground} configurations of
        \Natural. These systems generally achieve best in class results but
        also a consistently and significantly higher candidate latency.
\end{enumerate}

These three performance profiles highlight different tradeoffs that can be
conciously made depending on the deployment scenario and the target usage
pattern

\paragraph{Optimal Configurations for Different Deployment Scenarios}

Given the tradeoffs a recommendation of profiles for certain deployment
scenarios can be made. Generally the choice of the profile depends on the 
reliability requirements, the iteractivity of the use case and the complexity
of the domain. Some examplary recommendations for common use cases can be made:

\begin{enumerate}
    \item \textbf{Exploratory queries against simple databases} — For mostly
        interaction heavy systems like interactive query interfaces, data
        exploration and well-structured and smaller databases the
        \textbf{Baseline} configuration provides the overall best tradeoff
        given the lower latency which will significantly impact user
        experience. On simpler databases, the baseline configuration provides
        the best accuracy to latency ratio.

    \item \textbf{Mixed complexity use cases} — For general purposed, mixed
        complexity use cases like internal analytical tools, asynchronous
        report generation and potentially complex databases the \textbf{Full}
        profile excels. If latency is generally less important than accuracy
        the latency problem can be eased by provisioning more powerful
        hardware, especially in enterprise or professional scenarios.

    \item \textbf{Enterprise databases} — For large, complex databases with
        tens or even hundreds of tables, dirty data and potential external
        knowledge requirements (e.g, resemmbling \Bird) the \textbf{Full}
        profile is recommended. It provides meaningful assistance and paired
        with synthetic, historic and potentially domain-specialized examples it
        can achieve meaningful accuracy values in complex environments.
        Especially when manual querying would take significant amounts of time
        \Natural~can speed up the workflow.

    \item \textbf{Research deployments} — For research deployments and user
        experience evaluations of NL2SQL databases the \textbf{Full} profile
        with potential ground truth samples is deemed optimal. \Natural~could
        serve for pure, theoretical evaluation of potential NL2SQL systems and
        their upper bound performance. Such a research deployment could
        reference past conversations and historical examples to form a
        self-learning system. This is configuration is not sensible for real
        world deployments as it would require a ground truth entry for all
        possibly asked questions. 
\end{enumerate}

Concluding, for highly interactive systems like chat applications and
interactive learning tools, the \textbf{Baseline} profile will yield better
latency characteristics while maintain a significant portion of accuracy,
especially on simple tasks. For more complex usecases where accuracy is
generally more critical than latency (e.g, asynchronous report generation), the
\textbf{Full} profile is representing a more dedsirable tradeoff, which renders
the \textbf{Zero-Shot} profile mostly undesirable.

\paragraph{Implications for NL2SQL Pipeline Design}

General implications for future NL2SQL pipeline and system design can be
derived from this work. First, the example selection and self-learning
capabilities of any NL2SQL system that is based on LLMs will notably drive up
it's accuracy. Focusing on example quality, example selection and example
presentation will result in a good return of invest. Second, detailed component
ablations will help to evaluate the concrete contribution of individual
components, making iterations simpler and faster. Components should be
evaluated against realistic benchmarks (e.g, \Bird or \Spider 2.0) to measure
their real-world effects. Lastly, it is desirable to enable the system to be
fine-tunable to an specific deployment scenario. Reference data used,
fine-tuned models or specialized example generation can help to improve the
accuracy scores on a concrete deployment case beyond the theoretical benchmark
results that are cross-domain.

\paragraph{Conclusion}

\Natural is a portable NL2SQL system which synergetically achieves largely
competitive performance for a range of NL2SQL tasks. The \textbf{Full} profile
of \Natural~ was delivers significant accuracy gains over the \textbf{Baseline}
profile on complex use cases. Nonetheless \Natural~suffers from poor latency
characteristics over compared to the foundation models making it not always the
better choice.

Therefore, the optimal NL2SQL system depends on the use case at hand, for
enterprise level complexity using the advanced pipeline components can be
beneficial, especially if continously learning systems are desirable. For
exploratory and low latency systems, such as chatbots, \Natural provides only
marginal benefit over the underlying models.

\subsection{Practical Implications}

This section is discussing the practical implications of the evaluation results
ranging from deployment viability, to system integration patterns and
accessibility concerns.

\subsubsection{Deployment Viability and User Experience}

\Natural's~ deployment viability and user experience closely rely on its
the overall latency and portability characteristics of the system. Steeper
hardware requirements, suboptimal candidate latency or non trivial dependencies
make it harder and less appealing to employ NL2SQL systems.

\paragraph{System Integration Approaches}

The portability characteristic of \Natural~is predominately defined by its self
contained architecture which enable two recommended deployment architectures
which offer distinct trade-offs:

\begin{enumerate}
    \item \textbf{Database Extension} — By directly integrating \Natural~into a
        DBMS such as PostgreSQLs extension system a tight coupling between the
        underlying database and the NL2SQL system can be achieved which allow
        for potentially drastically reduced latency if \Natural~is used for
        expensive OLAP queries. This is largely implied by the pipeline
        architecture loading data from the database during inference.
        Furthermore the embedded architecture allows for enhanced data privacy
        as no data needs to leave the DBMS during inference. To enable access
        control patterns \Natural~can hook into row level access control and
        PostgreSQLs access control system. This architecture comes with two
        significant downsides. First, the database systems host needs to have
        access to a powerful GPU in order to load the model and run inference
        directly. Second, this architecture does not scale well, as a linear
        number of equally sized nodes with GPUs would be required and would
        thus likely yield a resource under utilization most of the time.
    \item \textbf{Inference Service} — The alternative architecture is the
        service oriented architecture which is trivially supported by embedding
        the Rust crate of \Natural~into a gRPC or REST API. This architecture
        would primarily excel in efficient resource utilization and deployment
        flexibility but introduce a latency increase through the large amounts
        of bidirectional network traffic between the inference service and the
        DBMS. Lastly the network traffic might result in non trivial load on
        the overall system and induce scalability challenges.
\end{enumerate}

Furthermore the portability and flexibility of \Natural's architecture allows
for new hybrid approaches to be introduced optimizing for specific deployment
scenarios.

\paragraph{Latency and Interactivity}

As shown in the evaluation results (See Table~\ref{tab:eval:overall-results})
the \CL~consistently increases with the pipeline accuracy. This means that more
powerful profiles of \Natural~require more powerful hardware in order to
maintain stable latency characteristics. As described in
\ref{sec:discussion:XYZ} (TODO) the latency of a NL2SQL system is a primary
denominator of the usecases it is viable for; e.g, high latency systems
are primarily sensible for asynchronous tasks whereas low latency systems are
viable for fast and iterative usecases like query interfaces (e.g, snowflake)
and chatbot integrations. The currently achieved latency of all NL2SQL
system that were benchmark on consumer hardware is not viable for interactive
user interfaces. Thus significantly more powerful hardware (e.g, Factor 10)
would be required to make fully interactive systems viable for users.

\paragraph{Trust, Transparency, and Error Communication}

As shown in Figure \ref{fig:eval:error-rates}~and
\ref{fig:eval:execution-accuracy-overview} the errors of \Natural~and other
NL2SQL systems are primarly semantic errors, rather than syntactic errors. This
in turn makes error detection and communication sigificantly harder, as the
pipeline does not return information on potential semantic mistakes. A
lightweight approach for improving the trust of users in downstream interfaces
could be as simple as showing the generated SQL statement, generating
multiple viable candidates and interpretations of the users natural language
query at once and offer switching between them. A second, more advanced
solution for semantic error detection would employ a new pipeline component to
interpret the users natural query in different ways and return a set of
different interpretations.

\subsubsection{Use Cases and Accessibility}

To evaluate the real world impact of \Natural~a look at potential use cases and
accessibility implications is subsequently performed.

\paragraph{Optimal Use Cases}

In general, natural language query interfaces are most viable when the query
complexity is moderate, e.g, simple joins, filtering and aggregations. The
schema is well-documented, with clear table and column names that closely match
domain language used during querying. Lastly, users need to have domain
knowledge in order gauge whether returned results are accurate
and iterations must be acceptable, e.g, no bulk deletion is automatically
performed.

Generally traditional SQL interfaces will remain superior for use cases where 
production systems are altered, complex analytical queries are asked which
require window functions, advanced aggregations and many joins as well as the
exact behavior of a query must be user controllable.

In an ideal scenario, natural language would be combined with SQL, enabling
drafting and iterating on queries with natural languages but eventually
transitioning to SQL if percise control is required (e.g, before deleting
data). This approach would leverage \Natural's strength (rapid formulation of
reasonable queries) while mitigating its primary weakness when it comes to
definite semantic accuracy. Traditional SQL tooling can be further employed for
query validation and optimization before executing them. Thus even lower level
database interfaces could surface both options simultaneously in the future,
allowing users to generate queries in natural language and refine them in SQL
within the same workflow, therefore removing a currently existing gap.

\paragraph{Accessibility and Democratization}

The overall hypothesis of NL2SQL systems to significantly lower the entry
barrier to data science and analytics holds true, but the evaluation of
\Natural~and similar NL2SQL systems showed a set of significant caveats due to
the limited semantic accuracy (See Figure
\ref{fig:eval:execution-accuracy-overview}).

The primary user groups benefitting from NL2SQL systems are domain experts with
limited SQL knowledge, occasional database users and users wanting to learn
SQL. Due to the need to verify the output candidates of \Natural~for semantic
errors it is currently unfeasible for users to have no awareness of SQL as this
would risk frequent semantic inaccuracy. Furthermore the evaluation results
show that the accuracy of \Natural~and \OmniSQL~is severly constrained on
\Bird~which contains complex schemas and external knowledge requirements. This
indicates that the accuracy of NL2SQL systems on complex, real-world relevant
databases and questions is still substantially to low in order to sunset the
need for SQL.

Nonetheless, NL2SQL can speed up various workflows, easing the need to explore
and understand the schema manually, allows users to draft sensible queries from
natural language problem statements and help users learn how different natural
language questions map to SQL statements. The democratization achieved through
\Natural~\&~co is therefore mostly \textit{vertical} (speeding up existing
users and easing workflows) instead of \textit{horizontal} (allowing entirely
unfamiliar users to use databases). With increasing NL2SQL capabilities this
trend will likely change.

\subsection{Limitations and Threats to Validity}

\subsubsection{Experimental Limitations}

\paragraph{Benchmark Representativeness}

\paragraph{Model and Hardware Specificity}

\paragraph{Reproducibility Challenges}

\subsubsection{Methodological Constraints}

\paragraph{Evaluation Metric Limitations}

\paragraph{Comparison Validity Concerns}

\paragraph{Internal Validity}



\subsection{Future Work}

\subsubsection{System Improvements}

\paragraph{Performance Optimization}

\paragraph{Component Refinement}

\subsubsection{Evaluation and Validation}

\paragraph{User Studies}

\paragraph{Extended Benchmarking}

\subsubsection{Deployment Research}

\newpage
