\section{Discussion}

This section is linking the evaluation results back to the research questions
of this thesis which where outlined in the introduction section. It is
reflecting on the practical implications of the findings in this thesis and 
deriving recommendations for actions and future research work in the NL2SQL
community.

\subsection{Summary of Results}

The evaluation of \Natural~demonstrated the capabilities of open-source LLMs
and showed that they can achieve competitive performance on prevalent structured
NL2SQL benchmarks such as \Spider~and \Bird. Nonetheless significant
challlenges remain for an enterprise-scale adoption of NL2SQL systems.
\Natural~achieves up to 81.0\% on the \Spider~development split using the
\textit{Syn} configuration and 81.4\% on the test split using the
\textit{Train} configuration representing a +3.1pp and +4.4pp improvement
over the 77.9-77.0\% baseline performance of \OmniSQL-7B respectively
(Table~\ref{tab:eval:overall-results}).

On the complex \Bird benchmark, the \textit{Syn} configuration reaches a 53.8\%
\EA which represents a +21.4pp improvement over the 38.0\% achieved by the
\textit{Baseline} configuration — representing a 66\% relative gain in
execution accuracy. These results position \Natural as competitive with the
locally-reproduced \OmniSQL-7B-gguf baseline (79.0\% on \Spider~(dev), 79.0\%
on \Spider~(test), 38.0\% on \Bird) while remaining below published accuracy
scores of closed-source model results (GPT-4o: 84.9\% \Spider, 64.0\% \Bird).

The ablation study performed during evaluation reveals that pipeline components
tend to interact in synergy rather than additively, with in-context learning
and the example selection algorithm of \Natural serving as the crucial
differentiator for performance. The \textit{Zero-Shot} configuration — which
uses schema subsetting, self-refinement, and majority voting but does not apply
example selection — achieves only a +0.8pp improvement on \Spider
despite doubling the candidate latency from 7.3s to 14.0s. \Natural~
configurations with active ICL and example selection yield up to +2.3pp on
\Spider~dev, +3.9pp on \Spider~test and and +19.8pp on \Bird compared to
\textit{Zero-Shot}. This shows that both schema subsetting and self-refinement
techniques provide minimal benefit without in-context learning and example
selection. Further ablation would help to understand the potential cross
effects of components and the potential implications of disabling pipeline
components like schema-subsetting, self-refinement and majority voting.
Furthermore increasing the pipeline iterations from 1 to $k$ would yield
insights into self balacing and self correction across iterations of \Natural.

A comparison of different example sources shows that structurally diverse
synthetic examples are not necessarily performing worse than examples from the
same dataset using the \Natural~example selection algorithm. Synthetic examples
outperformed domain-similar training examples on \Bird by 5.4pp (53.8\% vs.
48.4\%), while perfect example selection yielded 55.8\% accuracy and thus adds
+2.0pp over schema-aware selection. This indicates diminishing returns of
in-context learning from increasingly sophisticated example strategies.

Observed performance characteristics reveal a complex tradeoff for real-world
applications. While all NL2SQL system that were evaluated maintain remarkably
low error rates of less than 0.7 failures per 1000 queries (see
Figure~\ref{fig:eval:error-rates}) errors are therefore becoming predominately
semantic (ie. incorrect results). Furthermore candidate latency ranges from
7.3s for \Natural~(\textit{Baseline}) to up to 16.3s for ICL configurations on
\Spider and 43.8s on \Bird. This yields \Natural viable for experimental,
analytical workflows and asynchronous report generation but unviable for
interactive systems like chatbot applications. While performance optimizations
could be applied the hardware used is the primary bottleneck for the speed that
is achievable as the majority of \CL is attributable to inference.

Lastly the \EA~gap of 28.1pp between the published \OmniSQL-7B results (66.1\%
on \Bird) and local measurements (38.0\%) raise concerns to the reliability of
measurements and highlight systemic challenges in LLM-based research. The
analysis of this gap concludes that it is likely attributable to quantization
differences, inference engine variations and prompt template / code formatting
sensitivity. Despite this remaining uncertainty in absolute performance
comparisons, clear relative improvements from pipeline composition are
observable and reproducible within a controlled environment.

\subsection{Answering the Research Questions}

