\section{Discussion}

This section is linking the evaluation results back to the research questions
of this thesis which were outlined in the introduction section. It is
discussing the findings of this thesis and deriving recommendations for actions
and future research work in the NL2SQL community.

% TODO: Clean this up

\subsection{Summary of Results}

The evaluation of \Natural~demonstrated the capabilities of open-source LLMs
and showed that they can achieve competitive performance on prevalent structured
NL2SQL benchmarks such as \Spider~and \Bird. Nonetheless significant
challenges remain for an enterprise-scale adoption of NL2SQL systems.
\Natural~achieves up to 81.0\% on the \Spider~development split using the
\textit{Syn} configuration and 81.4\% on the test split using the
\textit{Train} configuration which represents a +3.1pp and +4.4pp improvement
over the 77.9-77.0\% performance of \OmniSQL-7B respectively
(Table~\ref{tab:eval:overall-results}).

On the complex \Bird benchmark, the \textit{Syn} configuration reaches a 53.8\%
\EA which represents a +21.4pp improvement over the 38.0\% achieved by the
\textit{Baseline} configuration — representing a 66\% relative gain in
execution accuracy. These results position \Natural as competitive with the
locally-reproduced \OmniSQL-7B-gguf baseline (79.0\% on \Spider~(dev), 79.0\%
on \Spider~(test), 38.0\% on \Bird) while remaining below published accuracy
scores of closed-source model results (GPT-4o: 84.9\% \Spider, 64.0\% \Bird).

% TODO: Add a table summarizing these key findings for quick reference
The ablation study performed during evaluation reveals that pipeline components
tend to interact in synergy rather than additively, with in-context learning
and the example selection algorithm of \Natural serving as the crucial
differentiator for performance. The \textit{Zero-Shot} configuration which
uses schema subsetting, self-refinement, and majority voting but does not apply
example selection achieves only a +0.8pp improvement on \Spider
despite doubling the candidate latency from 7.3s to 14.0s. \Natural~
configurations with active ICL and example selection yield up to +2.3pp on
\Spider~dev, +3.9pp on \Spider~test and and +19.8pp on \Bird compared to
\textit{Zero-Shot}. This shows that both schema subsetting and self-refinement
techniques provide minimal benefit without ICL and example
selection. Further ablation would help to understand the potential cross
effects of components and the implications of disabling pipeline
components like schema-subsetting, self-refinement and majority voting.
Furthermore increasing the pipeline iterations from 1 to $k$ would yield
insights into self balancing and self correction across iterations of \Natural.

A comparison of different example sources shows that structurally diverse
synthetic examples are not necessarily performing worse than examples from the
same dataset using the \Natural~example selection algorithm. Synthetic examples
outperformed domain-similar training examples on \Bird by 5.4pp (53.8\% vs.
48.4\%), while perfect example selection yielded 55.8\% accuracy and thus adds
+2.0pp over schema-aware selection. This indicates diminishing returns of
ICL from increasingly sophisticated example strategies.

% TODO: Discuss implications of the 28.1pp gap more prominently earlier in this section
Observed performance characteristics reveal a complex tradeoff for real-world
applications. While all NL2SQL system that were evaluated maintain remarkably
low error rates of less than 0.7 failures per 1000 queries (see
Figure~\ref{fig:eval:error-rates}) errors are therefore becoming predominately
semantic (ie. incorrect results). Furthermore candidate latency ranges from
7.3s for \Natural~(\textit{Baseline}) to up to 16.3s for ICL configurations on
\Spider and 43.8s on \Bird. This yields \Natural~viable for experimental,
analytical workflows and asynchronous report generation but unviable for
interactive systems like chatbot applications. While performance optimizations
could be applied the hardware used is the primary bottleneck for the speed that
is achievable as the majority of \CL is attributable to inference.

Lastly the \EA~gap of 28.1pp between the published \OmniSQL-7B results (66.1\%
on \Bird) and local measurements (38.0\%) raise concerns to the reliability of
measurements and highlight systemic challenges in LLM-based research. The
analysis of this gap concludes that it is likely attributable to quantization
differences, inference engine variations and prompt template and code formatting
sensitivity. Despite this remaining uncertainty in absolute performance
comparisons, clear relative improvements from pipeline composition are
observable and reproducible within a controlled environment.

\subsection{Answering the Research Questions}

To conclude this thesis, the research questions that were outlined in section
\ref{section:introduction:research-questions} are answered.

\subsubsection{Research Question 1: To what extent can open-source LLMs achieve
competitive NL2SQL performance through pipeline composition and optimization?}

The capabilities of open-source NL2SQL models achieve \textit{partially
competitive} performance on prevalent, structured NL2SQL benchmarks. Especially
through the composition of different algorithms the performance can be
significantly improved. The evaluation performed in this thesis demonstrates
that a combination of ICL, schema subsetting, self-refinement
and majority voting boosts the competitiveness of fine-tuned base models.
Nonetheless significant gaps in execution accuracy remain for enterprise-level
real-world deployments.

\Natural achieves up to 81.0\% \EA~on the \Spider~(dev) benchmark using the
\textit{Syn} configuration (examples selected from the SynSQL-2.5m dataset,
Table~\ref{tab:eval:overall-results}). This represents a +3.1pp increase
improvement over \textit{Baseline} configuration and a +2.0pp increase over the
baseline model performance of \OmniSQL-7B-gguf. On the test set of \Spider~the
\textit{Train} configuration is outperforming the \textit{Syn} configuration by
+1.8pp with an \EA~of 81.4\%. \textit{Train} is further showing an +2.4pp
increase over \OmniSQL-7B-gguf. On the more complex \Bird~benchmark
\textit{Syn} outperforms \textit{Train} by +5.4pp in \EA~and \OmniSQL~7B-gguf
by +15.8pp. However, on all three benchmarks noticeable performance gap ranging
from 2.6pp to 28.1pp was observed which reduces the trust in the measured
results.

This yields an overall unclear picture, with \Natural~showing strong
performance gains over the locally measured baseline model performance of
\OmniSQL but no absolute gain over the published results for \OmniSQL and other
external systems by \citeauthor{OmniSQL}. For definite conclusions on the
competitiveness of local LLM-based NL2SQL systems more research is needed.

\paragraph{Pipeline Composition Effectiveness}

Notably, the pipeline composition had notable impacts on the performance on
NL2SQL benchmarks. The effectiveness of more complex pipeline configurations
scaled drastically with database and benchmark complexity. On simpler
benchmarks like \Spider~only marginal, single digit accuracy gains (+2.0pp and
+2.4pp) could be observed while approximately doubling the candidate latency
(from 6.6s and 6.7s to 16.3 and 15.4s).

On \Bird, the difference between the evaluated configurations of \Natural is
significant. \textit{Baseline} achieved only about 32.4\% \EA~while
\textit{Syn} achieved 53.8\% \EA~, representing a 66\% relative gain over
baseline performance through an altered pipeline composition.

The ablation study shows that the effectiveness of a pipeline configuration 
heavily depends on ICL and example selection, as the
\textit{Zero-Shot} configuration showed only marginal gains (+0.8pp on
\Spider~(dev), +0.5pp on \Spider~(test) and +1.6pp on \Bird) over the
\textit{Baseline} configuration while effectively doubling the candidate
latency from 7.3s to 14.0s on \Spider~(dev), 7.3s to 15.3s on \Spider~(test)
and 9.3s to 18.7s on \Bird. This data is showing that a naive composition
strategy of simply enabling all available algorithms is not cost effective
and might produce counter productive returns on user experience as the latency
increases are significant.

Lastly the \textit{Ground} configuration of \Natural~establishes a theoretical
performance ceiling using perfect data during example selection and
consistently performed best-in class during local measurements. \textit{Ground}
achieved 84.2\% on \Spider~(dev), 82.8\% on \Spider~(test) and 55.8\% on \Bird,
representing a relative gain to the next-best performing configuration of
\Natural of +3.4pp, 1.4pp and +2.0pp respectively. This shows that in-context
learning and example selection strategies have their limits and are already
performing close to them. Further improvements would likely require better
foundation models or a different architecture altogether. A visual comparison
across the different configurations
(Figure~\ref{fig:eval:execution-accuracy-overview}) confirms that
ICL-based approaches (\textit{Syn}, \textit{Train},
\textit{Ground}) outperform other configurations with diminishing
returns from increasingly sophisticated example selection strategies.

\paragraph{Comparison to Closed-Source Baselines}

As no direct local comparison of \Natural~against closed-source models has been
performed, no definite answer can be given to the performance gap between
local NL2SQL systems and external, closed-source systems. The published results
by \citeauthor{OmniSQL} in \citeyear{OmniSQL} report GPT-4o achieving 84.9\%
execution accuracy on \Spider test and 64.0\% on \Bird development
\citep{OmniSQL}. Compared to local measurements of the best-in class
configuration of \Natural compared to GPT-4o shows a performance $\Delta$ of +10.3pp
on \Spider~(dev), -3.5pp on \Spider~(test) and -10.2pp on \Bird. Generally this
suggests competitive but not superior performance, although it must be
interpreted cautiously as the performance gap between the results reported by
\cite{OmniSQL} and local measurements are non trivial. Given the uncertainties
in measurements this thesis can't claim a definite competitiveness compared to
closed-source models. Further evaluations are needed to clarify the performance
characteristics of all measured systems. Nonetheless, \Natural~presents a
portable, at least partially competitive, alternative to closed-source systems.

\paragraph{Accuracy Ceiling on Consumer Hardware}

As \Natural~was developed and evaluated on consumer hardware, the hardware
constraints induced by it imply a ceiling of possible model size and speed
characteristics. The RTX 3090 has 24GB VRAM and can theoretically accommodate
\OmniSQL~7B and \OmniSQL~14B but given that \Natural~is using additional
embedding models and leaves 2-4GB headroom for KV-caches etc. to prevent
out-of-memory crashes. Therefore only the \OmniSQL~7B model (15GB without
quantization) is supported unless heavier quantization formats are used.

This makes \Natural accessible to consumers with high-end consumer-grade
hardware (~\$1,500 as of 2024-2025) without requiring enterprise-level
hardware. The possible performance impact of using larger foundation models
such as 14B and 32B was not evaluated. For enterprise real-world scenarios 
larger base models could be used if the available hardware allows for it.
The evaluation performed in this thesis suggests that with the selected model
sizes and the available hardware \Natural~has a theoretical execution accuracy
ceiling of 84.2\% and 82.8\% on \Spider~(dev and test) and 55.8\% on \Bird.
This marks an upper bound achievable with the 7B models and the
algorithms used. Therefore the foundation model capabilities emerge as the
limiting factor rather unless an entirely different pipeline architecture such
as agentic usage of LLMs is used. Nonetheless scaling the hardware and
parameter sizes to 14B or 32B would likely yield significant gains in execution
accuracy. 

The latency characteristics of \Natural~remains in the practical range for
non-interactive workflows but poses a significant challenge for local,
interactive applications. While the \textit{Baseline} configuration achieves
7.3s \CL, the optimal configurations of \Natural~(\textit{Syn} and \textit{Train})
only achieve a \CL~of 15.4-16.3s (Table~\ref{tab:eval:overall-results}).
This makes \Natural~viable for exploratory analytics and report generation but
not for interactive environments such as SQL learning or interactive natural
language driven data analysis platforms which would require sub-second latency.

\paragraph{Conclusion}

Overall, open-source LLMs \textit{can} achieve competitive NL2SQL performance
as fine-tuning and in-context-learning prove to be effective and efficient
mechanisms to achieve high accuracy values in constrained hardware
environments. Nonetheless the measured performance yields the systems to face
significant challenges in enterprise environments as an execution accuracy of
53.8\% on \Bird with a latency of 43.7s is unacceptable for end users. This
indicates that while promising there is still a gap that remains to be closed.

\subsubsection{Research Question 2: How do NL2SQL pipeline components interact,
and which configurations optimize the accuracy-latency tradeoff?}

\paragraph{Component Interaction Analysis}

The pipeline components $\sigma$, $\phi$, $\pi$, $\rho$ and $\nu$ were shown to
work best synergistically rather than additive with $\sigma$ representing the
primary performance driver of the pipeline $nq$. The ablation study performed
during evaluation reveals this through a systematic evaluation of the three
types of configurations: \textit{Baseline}, \textit{Zero-Shot} and
\textit{Full}. The evaluation results reveal the behaviour of the individual
pipeline components in synergy, where \textit{Zero-Shot} only provides a
marginal improvement over \textit{Baseline} (eg, +0.8pp on \Spider) while
nearly doubling the latency. All configurations with active ICL
demonstrated significant improvement over their non-ICL counterparts; The worst
results produced by ICL configurations compared to the best-performing non-ICL
configurations resulted in an improvement through ICL by +1.7pp on
\Spider~(dev), +2.1pp on \Spider~(test) and +14.4pp on \Bird. These results
reveal that the other pipeline components bring only a minimal benefit (+0.8pp
on \Spider~(dev), +0.5pp on \Spider~(test) and +1.6pp on \Bird) to the
\EA~performance of a pipeline while significantly impacting (ie. approximately
doubling) the \CL.

The underlying pattern of component interactions becomes even more apparent
through examining \Natural's failure modes. Without examples \Natural~(and thus
\OmniSQL) generate semantically inaccurate SQL candidates more often; usually
incorrect column or table selection, misunderstanding of column semantics and
value ranges or wrong join patterns result in syntactically correct queries
which result in wrong result sets. For example the schema subsetting component
is unable to work correctly if the initial query candidate is using the wrong
set of tables altogether, the majority voting algorithm fails to select the
best candidate when all SQL candidates correspond to different result sets and
self-refinement can only rarely repair semantically mislead initial queries. 
In-context learning helps \Natural~to guide the foundational model by supplying
a set of highly relevant, ranked examples which steers the model to put its
attention on specific parts of the SQL schema. The schema-aware example
selection algorithm works particularly well and was shown to be effective even
on data that did not originate from the training splits of the corresponding
benchmarks (\ref{tab:eval:overall-results}). 

\paragraph{Component Contribution Ranking}

The above insights allow for a ranking of components by their estimated impact
based on the ablation evidence from Table~\ref{tab:eval:overall-results} and
Figure~\ref{fig:eval:execution-accuracy-overview}. The pipeline components rank
by impact as follows:

\begin{enumerate}
    \item \textbf{Query Projection ($\pi$)} – The query projection component
        itself remains the largest single contributor in performance and the
        foundation models used predominately drives the performance of the
        system. All other components are orchestrated around query projection.
        This was shown through the \textit{Baseline} configuration which
        achieved 77.9\% and 77.0\% on \Spider~(dev and test) and 32.4\% on
        \Bird. The substantial gap between the two benchmarks highlights how
        severely the model capabilities degrade with increasing task and schema
        complexity.
    \item \textbf{Example Selection ($\sigma$)} – As discussed above, the
        example selection provides a non-negligible improvement in performance to
        the overall system while introducing relatively little candidate
        latency (eg, only +0.1-+2.3s on \Spider). The impact that ICL showed on
        \Bird demonstrates that example selection is not only helpful for LLMs
        generating SQL queries but a \textit{highly impactful} measure for enabling
        them to handle complex query generation tasks on real-world data.
    \item \textbf{Consensus Voting ($\nu$)} – The consesus voting algorithm
        selects the most prevalent result set amogst multiple generation
        attempts and validates that queries execute without any errors prior to
        returning them to the user. This self-consensus mechanism ensures low
        syntactic error rates across all configurations
        (Figure~\ref{fig:eval:error-rates}). However the voting strategy only
        gains in overall performance impact with an increasing number of
        internal candidates. The evaluation was performed with 1-2 internal
        candidates due to computational complexity of increasing the pipeline
        iterations. This prevents definite conclusions on the impact.
    \item \textbf{Self-Refinement ($\rho$)} – Self-refinement is effectively
        prompting the underlying model for repairing or improving its initially
        produced query candidate. This results in an overall marginal
        improvement of \EA, but comes at a significant computational cost.
        Adding self-refinement nearly doubles the \CL on average and provides
        diminishing returns. The results of the \textit{Zero-Shot}
        configuration, especially on the \Bird~benchmark, suggest that
        self-refinement is unable to correct significant semantic errors on its
        own without ICL.
    \item \textbf{Schema-Subsetting ($\phi$)} – Schema subsetting aims to
        reduce the noise for the model by filtering out unused table from the
        SQL schema before running query projection, thus improving token
        efficiency and accuracy. However, the exact impact of schema-subsetting
        is difficult to isolate due to the constrained ablation study that was
        performed. It is questionable whether schema-subsetting itself provides
        a statistically significant improvement in performance or even harms
        performance as the \textit{Baseline} configuration underperformed the
        model baseline.
\end{enumerate}

\paragraph{Accuracy-Latency Tradeoff}

The accuracy-latency measurements which is visualized in
Figure~\ref{fig:eval:candidate-latency} and
Table~\ref{tab:eval:overall-results} reveal three different efficiency profiles
across the systems that where benchmarked.

\begin{enumerate}
    \item \textbf{Baseline} — The baseline profile includes both the
        \textit{Baseline} configuration of \Natural~and the raw \OmniSQL~models.
        The baseline profile is generally yielding the lowest possible
        candidate latency, but also yields the lowest scores for both
        \EA~and~\EM and is thus prioritising latency over accuracy.
    \item \textbf{Zero-Shot} — The zero shot profile is denoted by a suboptimal
        accuracy latency tradeoff where there are marginal performance gain
        over the baseline results but this is accompanied by a strong average
        increase in latency (effectively doubling on average).
    \item \textbf{Full} — Lastly the full profile is a balanced profile which
        weights accuracy over latency. Systems in this class include the
        \textit{Syn}, \textit{Train} and \textit{Ground} configurations of
        \Natural. These systems generally achieve best in class results but
        also a consistently and significantly higher candidate latency.
\end{enumerate}

These three performance profiles highlight different tradeoffs that can be
consciously made depending on the deployment scenario and the target usage
pattern

\paragraph{Optimal Configurations for Different Deployment Scenarios}\label{sec:discussion:deployment-scenarios}

Given the tradeoffs a recommendation of profiles for certain deployment
scenarios can be made. Generally the choice of the profile depends on the 
reliability requirements, the interactivity of the use case and the complexity
of the domain. Some exemplary recommendations for common use cases can be made:

\begin{enumerate}
    \item \textbf{Exploratory queries against simple databases} — For mostly
        interaction heavy systems like interactive query interfaces, data
        exploration and well-structured and smaller databases the
        \textbf{Baseline} configuration provides the overall best tradeoff
        given the lower latency which will significantly impact user
        experience. On simpler databases, the baseline configuration provides
        the best accuracy to latency ratio.

    \item \textbf{Mixed complexity use cases} — For general purposed, mixed
        complexity use cases like internal analytical tools, asynchronous
        report generation and potentially complex databases the \textbf{Full}
        profile excels. If latency is generally less important than accuracy
        the latency problem can be eased by provisioning more powerful
        hardware, especially in enterprise or professional scenarios.

    \item \textbf{Enterprise databases} — For large, complex databases with
        tens or even hundreds of tables, dirty data and potential external
        knowledge requirements (e.g, resembling \Bird) the \textbf{Full}
        profile is recommended. It provides meaningful assistance and paired
        with synthetic, historic and potentially domain-specialized examples it
        can achieve meaningful accuracy values in complex environments.
        Especially when manual querying would take significant amounts of time
        \Natural~can speed up the workflow.

    \item \textbf{Research deployments} — For research deployments and user
        experience evaluations of NL2SQL databases the \textbf{Full} profile
        with potential ground truth samples is deemed optimal. \Natural~could
        serve for pure, theoretical evaluation of potential NL2SQL systems and
        their upper bound performance. Such a research deployment could
        reference past conversations and historical examples to form a
        self-learning system. This is configuration is not sensible for real
        world deployments as it would require a ground truth entry for all
        possibly asked questions. 
\end{enumerate}

Concluding, for highly interactive systems like chat applications and
interactive learning tools, the \textbf{Baseline} profile will yield better
latency characteristics while maintain a significant portion of accuracy,
especially on simple tasks. For more complex use cases where accuracy is
generally more critical than latency (e.g, asynchronous report generation), the
\textbf{Full} profile is representing a more desirable tradeoff, which renders
the \textbf{Zero-Shot} profile mostly undesirable.

\paragraph{Implications for NL2SQL Pipeline Design}

General implications for future NL2SQL pipeline and system design can be
derived from this work. First, the example selection and self-learning
capabilities of any NL2SQL system that is based on LLMs will notably drive up
it's accuracy. Focusing on example quality, example selection and example
presentation will result in a good return of invest. Second, detailed component
ablations will help to evaluate the concrete contribution of individual
components, making iterations simpler and faster. Components should be
evaluated against realistic benchmarks (e.g, \Bird or \Spider 2.0) to measure
their real-world effects. Lastly, it is desirable to enable the system to be
fine-tunable to an specific deployment scenario. Reference data used,
fine-tuned models or specialized example generation can help to improve the
accuracy scores on a concrete deployment case beyond the theoretical benchmark
results that are cross-domain.

\paragraph{Conclusion}

% TODO: Tighten this conclusion - remove redundancy with earlier sections
\Natural is a portable NL2SQL system which synergistically achieves largely
competitive performance for a range of NL2SQL tasks. The \textbf{Full} profile
of \Natural~delivers significant accuracy gains over the \textbf{Baseline}
profile on complex use cases. Nonetheless \Natural~suffers from poor latency
characteristics over compared to the foundation models making it not always the
better choice.

Therefore, the optimal NL2SQL system depends on the use case at hand, for
enterprise level complexity using the advanced pipeline components can be
beneficial, especially if continuously learning systems are desirable. For
exploratory and low latency systems, such as chatbots, \Natural provides only
marginal benefit over the underlying models.

\subsection{Practical Implications}

This section is discussing the practical implications of the evaluation results
ranging from deployment viability, to system integration patterns and
accessibility concerns.

\subsubsection{Deployment Viability and User Experience}

\Natural's~ deployment viability and user experience closely rely on its
the overall latency and portability characteristics of the system. Steeper
hardware requirements, suboptimal candidate latency or non trivial dependencies
make it harder and less appealing to employ NL2SQL systems.

\paragraph{System Integration Approaches}

The portability characteristic of \Natural~is predominately defined by its self
contained architecture which enable two recommended deployment architectures
which offer distinct trade-offs:

\begin{enumerate}
    \item \textbf{Database Extension} — By directly integrating \Natural~into a
        DBMS such as PostgreSQLs extension system a tight coupling between the
        underlying database and the NL2SQL system can be achieved which allow
        for potentially drastically reduced latency if \Natural~is used for
        expensive Online Analytical Processing (OLAP) queries. This is largely
        implied by the pipeline architecture loading data from the database
        during inference. Furthermore the embedded architecture allows for
        enhanced data privacy as no data needs to leave the DBMS during
        inference. To enable access control patterns \Natural~can hook into row
        level access control and PostgreSQLs access control system. This
        architecture comes with two significant downsides. First, the database
        systems host needs to have access to a powerful GPU in order to load
        the model and run inference directly. Second, this architecture does
        not scale well, as a linear number of equally sized nodes with GPUs
        would be required and would thus likely yield a resource under
        utilization most of the time.
    \item \textbf{Inference Service} — The alternative architecture is the
        service oriented architecture which is trivially supported by embedding
        the Rust crate of \Natural~into a gRPC or REST API. This architecture
        would primarily excel in efficient resource utilization and deployment
        flexibility but introduce a latency increase through the large amounts
        of bidirectional network traffic between the inference service and the
        DBMS. Lastly the network traffic might result in non trivial load on
        the overall system and induce scalability challenges.
\end{enumerate}

Furthermore the portability and flexibility of \Natural's architecture allows
for new hybrid approaches to be introduced optimizing for specific deployment
scenarios.

\paragraph{Latency and Interactivity}

As shown in the evaluation results (See Table~\ref{tab:eval:overall-results})
the \CL~consistently increases with the pipeline accuracy. This means that more
powerful profiles of \Natural~require more powerful hardware in order to
maintain stable latency characteristics. As described in
\ref{sec:discussion:deployment-scenarios} the latency of a NL2SQL system is a primary
denominator of the use cases it is viable for; e.g, high latency systems
are primarily sensible for asynchronous tasks whereas low latency systems are
viable for fast and iterative use cases like query interfaces (e.g, snowflake)
and chatbot integrations. The currently achieved latency of all NL2SQL
system that were benchmark on consumer hardware is not viable for interactive
user interfaces. Thus significantly more powerful hardware (e.g, Factor 10)
would be required to make fully interactive systems viable for users.

\paragraph{Trust, Transparency, and Error Communication}

As shown in Figure \ref{fig:eval:error-rates}~and
\ref{fig:eval:execution-accuracy-overview} the errors of \Natural~and other
NL2SQL systems are primarily semantic errors, rather than syntactic errors. This
in turn makes error detection and communication significantly harder, as the
pipeline does not return information on potential semantic mistakes. A
% TODO: Propose specific UI/UX patterns for presenting multiple candidates to users
lightweight approach for improving the trust of users in downstream interfaces
could be as simple as showing the generated SQL statement, generating
multiple viable candidates and interpretations of the users natural language
query at once and offer switching between them. A second, more advanced
solution for semantic error detection would employ a new pipeline component to
interpret the users natural query in different ways and return a set of
different interpretations.

\subsubsection{Use Cases and Accessibility}

To evaluate the real world impact of \Natural~a look at potential use cases and
accessibility implications is subsequently performed.

\paragraph{Optimal Use Cases}

In general, natural language query interfaces are most viable when the query
complexity is moderate, e.g, simple joins, filtering and aggregations. The
schema is well-documented, with clear table and column names that closely match
domain language used during querying. Lastly, users need to have domain
knowledge in order gauge whether returned results are accurate
and iterations must be acceptable, e.g, no bulk deletion is automatically
performed.

Generally traditional SQL interfaces will remain superior for use cases where 
production systems are altered, complex analytical queries are asked which
require window functions, advanced aggregations and many joins as well as the
exact behavior of a query must be user controllable.

In an ideal scenario, natural language would be combined with SQL, enabling
drafting and iterating on queries with natural languages but eventually
transitioning to SQL if precise control is required (e.g, before deleting
data). This approach would leverage \Natural's strength (rapid formulation of
reasonable queries) while mitigating its primary weakness when it comes to
definite semantic accuracy. Traditional SQL tooling can be further employed for
query validation and optimization before executing them. Thus even lower level
database interfaces could surface both options simultaneously in the future,
allowing users to generate queries in natural language and refine them in SQL
within the same workflow, therefore removing a currently existing gap.

\paragraph{Accessibility and Democratization}

The overall hypothesis of NL2SQL systems to significantly lower the entry
barrier to data science and analytics holds true, but the evaluation of
% TODO: Discuss ethical implications of errors (e.g., incorrect medical/financial queries)
\Natural~and similar NL2SQL systems showed a set of significant caveats due to
the limited semantic accuracy (See Figure
\ref{fig:eval:execution-accuracy-overview}).

The primary user groups benefitting from NL2SQL systems are domain experts with
limited SQL knowledge, occasional database users and users wanting to learn
SQL. Due to the need to verify the output candidates of \Natural~for semantic
errors it is currently unfeasible for users to have no awareness of SQL as this
would risk frequent semantic inaccuracy. Furthermore the evaluation results
show that the accuracy of \Natural~and \OmniSQL~is severely constrained on
\Bird~which contains complex schemas and external knowledge requirements. This
indicates that the accuracy of NL2SQL systems on complex, real-world relevant
databases and questions is still substantially to low in order to sunset the
need for SQL.

Nonetheless, NL2SQL can speed up various workflows, easing the need to explore
and understand the schema manually, allows users to draft sensible queries from
natural language problem statements and help users learn how different natural
language questions map to SQL statements. The democratization achieved through
\Natural~\&~co is therefore mostly \textit{vertical} (speeding up existing
users and easing workflows) instead of \textit{horizontal} (allowing entirely
unfamiliar users to use databases). With increasing NL2SQL capabilities this
trend will likely change.

\subsection{Limitations and Threats to Validity}

Given the stark differences between published and locally reproduced result,
the constrained benchmark selection (excluding more advanced benchmarks like
\Spider~2.0) and the constrained hardware that was available during the
evaluation the results of this thesis need to be treated as rather unreliable.

\subsubsection{Experimental Limitations}

The performed evaluation had severe limitations with regards to the model
sizes, reproducibility and the constrained benchmark selection.

\paragraph{Benchmark Representativeness}

The evaluation performed in this thesis relies on only two benchmarks -
\Spider~and \Bird~- which likely do not accurately represent the real-world
query distributions and databases schemas that NL2SQL systems will encounter.
Thus the results are not indicative of the guaranteed real-world performance,
but rather highlighting patterns where \Natural~is performing well (e.g,
structured schemas, simpler questions) and problematic patterns that need
improvement (e.g, external knowledge requirements and dirty schemas).

This results in a set of limitations this thesis does not address; First, the 
evaluation inevitably is a constrained domain coverage, second the query
complexity and distributions are fairly constrained with advanced complexity
benchmarks like \Spider~2.0 not being used. Lastly, the schema design patterns
are limited to the patterns used by \Spider~and \Bird and the SQLite dialect.

\paragraph{Model and Hardware Specificity}

Furthermore, all results shown in Table~\ref{tab:eval:overall-results} are
subject to the configurations and models used due to the hardware constraints
that existed during benchmarking. More powerful hardware than the RTX 3090
would allow for larger models (e.g, \OmniSQL-32B and better embedding models)
as well as likely substantially improve the candidate latency measurements.

Additionally there exists a risk of \OmniSQL~being fine-tuned to perform well
on \Spider~training and development sets which would further limit the ability
to transfer the performance into real-world deployments. 

\paragraph{Reproducibility Challenges}

Likely the most significant threat to validity on this thesis is the observed
performance gap between the published \OmniSQL results and locally achieved
results. This gap is substantial on \Bird, where published results report
66.1\% \EA for \OmniSQL~7B while locally only 38.0\% \EA~could be achieved. 
This represents a 28.1pp difference in accuracy. As discussed in
section~\ref{sec:eval:performance-gap}, this might be caused by a multitude of
different factors such as model format differences, diverging hyperparameters,
prompt drifting and dataset versioning.

The existence of this gap \textbf{severely} undermines the confidence in the
absolute performance of \Natural~compared to \OmniSQL~and other systems,
resulting in uncertainty on whether the steep, relative performance gains shown
by \Natural~are transferable into environments where this gap does not exist.
It highlights general reproducibility challenges that are prevalent in
LLM-based research.

The implications for this thesis are twofold; First, no absolute performance
assessment can be made and any claims of absolute performance superiority of
\Natural~over \OmniSQL~or GPT-4 would be premature. Further research and
extensive evaluations are required to solve this gap and make a reliable,
absolute assessment possible. Second, while the absolute performance is
unreliable, the relative performance increases can be treated with more
confidence. It was shown that the machine learning algorithm introduced in
\Natural~offer a reliable improvement of up to +21.4pp over the
\textit{baseline} configuration. Nonetheless it is unclear whether this
improvement in accuracy would persist or substantially shrink with increasingly
better \textit{baseline} performance.

The conclusion of this thesis is therefore that relative improvements achieved
through pipeline composition can be substantial, but must be further verified
through independent research before any deployment decision is made.

\subsubsection{Methodological Constraints}

Beyond representativeness and reliability of measurements the chosen evaluation
metrics, comparisons with external systems and internal validity are
constrained.

\paragraph{Evaluation Metric Limitations}

While the two prevalent metrics \EA~and \EM~are indicative of the general
pipeline performance and ability to generate sound SQL queries, a series of
metrics is missing to get a full understanding of NL2SQL performance that is
relevant in real-world deployments.

Primarily the metrics are lacking further dimensions, such as the performance
of a query candidate (e.g, inefficient queries should be penalized), user
satisfaction and perceived usefulness are not evaluated and learning
effectiveness and productivity gains are not measured. This makes the
evaluation primarily machine learning bound, rather than impact bound which is
acceptable given the rather early stage of NL2SQL systems but limit the ability
to forecast the actual benefit of using it.

Additionally, the \EM~metric is overly strict, limiting the solution space to
one exact style of writing SQL and risks that optimizing for \EM~needs
fine-tuned models in order to perform well.

Overall, these limitations indicate that a system with good metric performance
might still bring questionable real world benefits to users (e.g, through a
poor user experience, inefficient SQL etc.). Future evaluations should define
reasonable metrics that look beyond the functional performance of a system.

\paragraph{Comparison Validity}

As mentioned above, the comparisons with SOTA results (GPT-4, \OmniSQL
variants, etc.) face additional validity challenges beyond the observed
performance gaps. Variables that influence the validity of the performed
comparisons include the potentially diverging hardware platforms, model
configurations and evaluation environments that were used. Additionally, the 
results from \OmniSQL~would benefit from an independent reproduction which
would substantially strengthen their reliability and enable a conclusion to be
drawn on whether the local measurements are faulty and what could have caused
them to diverge.

\paragraph{Internal Validity}

Lastly, beyond the comparisons validity with other systems, the internal
validity is also questionable as there were several significant limitations
during the evaluation.

First, the hyperparameter, as well as example selection weights were entirely
experimentally estimated, not empirically optimized. Furthermore, the
configurations that were used were only a small subset of the possible
configurations due to the extreme length of benchmark runtimes (6-24h). Lastly,
no statistical testing and observation of performance characteristics was
performed. Further evaluations should include detailed case studies,
fine-grained performance metrics by query and schema complexity and more
benchmark datasets.

\subsection{Future Work}

The work on \Natural~revealed several important areas that subsequent research
should focus on. These areas include system improvements, further and extensive
evaluations and user oriented research.

\subsubsection{System Improvements}

One significant opportunity for future research would be further system
and performance improvements in order to enable deployments on less powerful
hardware. The overall candidate latency of \Natural~emerged as a critical
blocker for real world adoption, as requiring users to procure expensive
research or enterprise grade GPUs is suboptimal. Therefore substantial amounts
of performance optimization are needed in order to achieve a democratization
effect.

\paragraph{Model Quantization and Distillation}

Through further research on quantization effects on NL2SQL models as well as
potentially applied model distillation, significantly smaller LLMs could yield
a noticeable increase in performance and therefore drop the hardware
requirements. For users with existing high-end hardware, this could yield a
significant candidate latency reduction and unlock interactive use cases that
are not viable with the latency values measured in this thesis.

\paragraph{Component Refinement}

% TODO: Expand on specific optimization techniques (e.g., INT8 quantization, speculative decoding)
Furthermore, additional component refinement as well as removing components
which turn out to add minimal or no additional performance would both increase
the semantic accuracy of \Natural~as well as reduce any potentially needless
computational overheads. Especially the schema subsetting component has shown
little to no notable performance improvement, and could potentially even
harm performance.  

\paragraph{Multi-Model Architectures}

Lastly, this thesis did not utilize a multi-model, or agentic architecture as
proposed by \cite{XiYan}. Future research could integrate the learnings of this
thesis with a multi-model, or agentic architecture to see whether a
reimplementation of the \Natural's components as tool calls provide a
performance gain over agentic architectures without example selection
algorithms, self refinement.
