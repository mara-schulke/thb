\section{Discussion}

This section is linking the evaluation results back to the research questions
of this thesis which where outlined in the introduction section. It is
discussing the findings of this thesis and deriving recommendations for actions
and future research work in the NL2SQL community.

% TODO cleanup

\subsection{Summary of Results}

The evaluation of \Natural~demonstrated the capabilities of open-source LLMs
and showed that they can achieve competitive performance on prevalent structured
NL2SQL benchmarks such as \Spider~and \Bird. Nonetheless significant
challlenges remain for an enterprise-scale adoption of NL2SQL systems.
\Natural~achieves up to 81.0\% on the \Spider~development split using the
\textit{Syn} configuration and 81.4\% on the test split using the
\textit{Train} configuration which represents a +3.1pp and +4.4pp improvement
over the 77.9-77.0\% performance of \OmniSQL-7B respectively
(Table~\ref{tab:eval:overall-results}).

On the complex \Bird benchmark, the \textit{Syn} configuration reaches a 53.8\%
\EA which represents a +21.4pp improvement over the 38.0\% achieved by the
\textit{Baseline} configuration — representing a 66\% relative gain in
execution accuracy. These results position \Natural as competitive with the
locally-reproduced \OmniSQL-7B-gguf baseline (79.0\% on \Spider~(dev), 79.0\%
on \Spider~(test), 38.0\% on \Bird) while remaining below published accuracy
scores of closed-source model results (GPT-4o: 84.9\% \Spider, 64.0\% \Bird).

The ablation study performed during evaluation reveals that pipeline components
tend to interact in synergy rather than additively, with in-context learning
and the example selection algorithm of \Natural serving as the crucial
differentiator for performance. The \textit{Zero-Shot} configuration which
uses schema subsetting, self-refinement, and majority voting but does not apply
example selection achieves only a +0.8pp improvement on \Spider
despite doubling the candidate latency from 7.3s to 14.0s. \Natural~
configurations with active ICL and example selection yield up to +2.3pp on
\Spider~dev, +3.9pp on \Spider~test and and +19.8pp on \Bird compared to
\textit{Zero-Shot}. This shows that both schema subsetting and self-refinement
techniques provide minimal benefit without in-context learning and example
selection. Further ablation would help to understand the potential cross
effects of components and the implications of disabling pipeline
components like schema-subsetting, self-refinement and majority voting.
Furthermore increasing the pipeline iterations from 1 to $k$ would yield
insights into self balacing and self correction across iterations of \Natural.

A comparison of different example sources shows that structurally diverse
synthetic examples are not necessarily performing worse than examples from the
same dataset using the \Natural~example selection algorithm. Synthetic examples
outperformed domain-similar training examples on \Bird by 5.4pp (53.8\% vs.
48.4\%), while perfect example selection yielded 55.8\% accuracy and thus adds
+2.0pp over schema-aware selection. This indicates diminishing returns of
in-context learning from increasingly sophisticated example strategies.

Observed performance characteristics reveal a complex tradeoff for real-world
applications. While all NL2SQL system that were evaluated maintain remarkably
low error rates of less than 0.7 failures per 1000 queries (see
Figure~\ref{fig:eval:error-rates}) errors are therefore becoming predominately
semantic (ie. incorrect results). Furthermore candidate latency ranges from
7.3s for \Natural~(\textit{Baseline}) to up to 16.3s for ICL configurations on
\Spider and 43.8s on \Bird. This yields \Natural viable for experimental,
analytical workflows and asynchronous report generation but unviable for
interactive systems like chatbot applications. While performance optimizations
could be applied the hardware used is the primary bottleneck for the speed that
is achievable as the majority of \CL is attributable to inference.

Lastly the \EA~gap of 28.1pp between the published \OmniSQL-7B results (66.1\%
on \Bird) and local measurements (38.0\%) raise concerns to the reliability of
measurements and highlight systemic challenges in LLM-based research. The
analysis of this gap concludes that it is likely attributable to quantization
differences, inference engine variations and prompt template / code formatting
sensitivity. Despite this remaining uncertainty in absolute performance
comparisons, clear relative improvements from pipeline composition are
observable and reproducible within a controlled environment.

\subsection{Answering the Research Questions}

To conclude this thesis, the research questions that were outlined in section
\ref{section:introduction:research-questions} are answered.

\subsubsection{Research Question 1: To what extent can open-source LLMs achieve
competitive NL2SQL performance through pipeline composition and optimization?}

The capabilities of open-source NL2SQL models achieve \textit{partially
competitive} performance on prevalent, structured NL2SQL benchmarks. Especially
through the composition of different algorithms the performance can be
significantly improved. The evaluation performed in this thesis demonstrates
that a combination of in-context learning, schema subsetting, self-refinement
and majority voting boosts the competitiveness of fine-tuned base models.
Nonetheless significant gaps in execution accuracy remain for enterprise-level
real-world deployments.

\Natural achieves up to 81.0\% \EA~on the \Spider~(dev) benchmark using the
\textit{Syn} configuration (examples selected from the SynSQL-2.5m dataset,
Table~\ref{tab:eval:overall-results}). This represents a +3.1pp increase
improvement over \textit{Baseline} configuration and a +2.0pp increase over the
baseline model performance of \OmniSQL-7B-gguf. On the test set of \Spider~the
\textit{Train} configuration is outperforming the \textit{Syn} configuration by
+1.8pp with an \EA~of 81.4\%. \textit{Train} is further showing an +2.4pp
increase over \OmniSQL-7B-gguf. On the more complex \Bird~benchmark
\textit{Syn} outperforms \textit{Train} by +5.4pp in \EA~and \OmniSQL~7B-gguf
by +15.8pp. However, on all three benchmarks noticable performance gap ranging
from 2.6pp to 28.1pp was observed which reduces the trust in the measured
results.

This yields an overall unclear picture, with \Natural~showing strong
performance gains over the locally measured baseline model performance of
\OmniSQL but no absolute gain over the published results for \OmniSQL and other
external systems by \citeauthor{OmniSQL}. For definite conclusions on the
competitiveness of local LLM-based NL2SQL systems more research is needed.

\paragraph{Pipeline Composition Effectiveness}

Notably, the pipeline composition had notable impacts on the performance on
NL2SQL benchmarks. The effectiveness of more complex pipeline configurations
scaled drastically with database and benchmark complexity. On simpler
benchmarks like \Spider~only marginal, single digit accuracy gains (+2.0pp and
+2.4pp) could be observed while approximately doubling the candidate latency
(from 6.6s and 6.7s to 16.3 and 15.4s).

On \Bird, the difference between the evaluated configurations of \Natural is
significant. \textit{Baseline} achieved only about 32.4\% \EA~while
\textit{Syn} achieved 53.8\% \EA~, representing a 66\% relative gain over
baseline performance through an altered pipeline composition.

The ablation study shows that the effectiveness of a pipeline configuration 
heavily depends on in-context learning and example selection, as the
\textit{Zero-Shot} configuration showed only marginal gains (+0.8pp on
\Spider~(dev), +0.5pp on \Spider~(test) and +1.6pp on \Bird) over the
\textit{Baseline} configuration while effectively doubling the candidate
latency from 7.3s to 14.0s on \Spider~(dev), 7.3s to 15.3s on \Spider~(test)
and 9.3s to 18.7s on \Bird. This data is showing that a naive composition
strategy of simply enabling all available algorithms is not cost effective
and might produce counter productive returns on user exerpience as the latency
increases are significant.

Lastly the \textit{Ground} configuration of \Natural~establishes a theoretical
performance ceiling using perfect data during example selection and
consistently performed best-in class during local measurements. \textit{Ground}
achieved 84.2\% on \Spider~(dev), 82.8\% on \Spider~(test) and 55.8\% on \Bird,
representing a relative gain to the next-best performing configuration of
\Natural of +3.4pp, 1.4pp and +2.0pp respectively. This shows that in-context
learning and example selection strategies have their limits and are already
performing close to them. Further improvements would likely require better
foundation models or a different architecture alltogether. A visual comparison
across the different configurations
(Figure~\ref{fig:eval:execution-accuracy-overview}) confirms that
in-context-learning-based approaches (\textit{Syn}, \textit{Train},
\textit{Ground}) outperform other configurations with diminishing
returns from increasingly sophisticated example selection strategies.

\paragraph{Comparison to Closed-Source Baselines}

As no direct local comparison of \Natural~against closed-source models has been
performed, no definite answer can be given to the performance gap between
local NL2SQL systems and external, closed-source systems. The published results
by \citeauthor{OmniSQL} in \citeyear{OmniSQL} report GPT-4o achieving 84.9\%
execution accuracy on \Spider test and 64.0\% on \Bird development
\citep{OmniSQL}. Compared to local measurements of the best-in class
configuration of \Natural compared to GPT-4o shows a performance $\Delta$ of +10.3pp
on \Spider~(dev), -3.5pp on \Spider~(test) and -10.2pp on \Bird. Generally this
suggests competitive but not superior performance, although it must be
interpreted cautiously as the performance gap between the results reported by
\cite{OmniSQL} and local measurements are non trivial. Given the uncertainties
in measurements this thesis can't claim a definite competitiveness compared to
closed-source models. Further evaluations are needed to clarify the performance
characteristics of all measured systems. Nonetheless, \Natural~presents a
portable, at least partially competitive, alternative to closed-source systems.

\paragraph{Accuracy Ceiling on Consumer Hardware}

As \Natural~was developed and evaluated on consumer hardware, the hardware
constraints induced by it imply a ceiling of possible model size and speed
characteristics. The RTX 3090 has 24GB VRAM and can theoretically accomodate
\OmniSQL~7B and \OmniSQL~14B but given that \Natural~is using additional
embedding models and leaves 2-4GB headroom for KV-caches etc. to prevent
out-of-memory crashes. Therefore only the \OmniSQL~7B model (15GB without
quantization) is supported unless heavier quantization formats are used.

This makes \Natural accessible to consumers with high-end consumer-grade
hardware (~\$1,500 as of 2024-2025) without requiring enterprise-level
hardware. The possible performance impact of using larger foundation models
such as 14B and 32B was not evaluated. For enterprise real-world scenarios 
larger base models could be used if the available hardware allows for it.
The evaluation performed in this thesis suggests that with the selected model
sizes and the available hardware \Natural~has a theoretical execution accuracy
ceiling of 84.2\% and 82.8\% on \Spider~(dev and test) and 55.8\% on \Bird.
This marks an upper bound achievable with the 7B models and the
algorithms used. Therfore the foundation model capabilities emerge as the
limiting factor rather unless an entirely different pipeline architecture such
as agentic usage of LLMs is used. Nonetheless scaling the hardware and
parameter sizes to 14B or 32B would likely yield significant gains in execution
accuracy. 

The latency characteristics of \Natural~remains in the practical range for
non-interactive workflows but poses a significant challenge for local,
interactive applications. While the \textit{Baseline} configuration achieves
7.3s \CL, the optimal configurations of \Natural~(\textit{Syn} and \textit{Train})
only achieve a \CL~of 15.4-16.3s (Table~\ref{tab:eval:overall-results}).
This makes \Natural~viable for exploratory analytics and report generation but
not for interactive environments such as SQL learning or interactive natural
language driven data analyis platforms which would require sub-second latency.

\paragraph{Conclusion}

Overall, open-source LLMs \textit{can} achieve competitive NL2SQL performance
as fine-tuning and in-context-learning prove to be effective and efficient
mechanisms to achieve high accuracy values in constrained hardware
environments. Nonetheless the measured performance yields the systems to face
significant challenges in enterprise environments as an execution accuracy of
53.8\% on \Bird with a latency of 43.7s is unacceptable for end users. This
indicates that while promising there is still a gap that remains to be closed.

\subsubsection{Research Question 2: How do NL2SQL pipeline components interact,
and which configurations optimize the accuracy-latency tradeoff?}

\paragraph{Component Interaction Analysis}

The pipeline components $\sigma$, $\phi$, $\pi$, $\rho$ and $\nu$ were shown to
work best synergetically rather than additive with $\sigma$ representing the
primary performance driver of the pipeline $nq$. The ablation study perfomed
during evaluation reveals this through a systematic evaluation of the three
types of configurations: \textit{Baseline}, \textit{Zero-Shot} and
\textit{Full}. The evaluation results reveal the behaviour of the individual
pipeline components in synergy, where \textit{Zero-Shot} only provides a
marginal improvement over \textit{Baseline} (eg, +0.8pp on \Spider) while
nearly doubling the latency. All configurations with active ICL
demonstrated significant improvement over their non-ICL counterparts; The worst
results produced by ICL configurations compared to the best-performing non-ICL
configurations resulted in an improvement through ICL by +1.7pp on
\Spider~(dev), +2.1pp on \Spider~(test) and +14.4pp on \Bird. These results
reveal that the other pipeline components bring only a minimal benefit (+0.8pp
on \Spider~(dev), +0.5pp on \Spider~(test) and +1.6pp on \Bird) to the
\EA~performance of a pipeline while significantly impacting (ie. approximately
doubling) the \CL.

The underlying pattern of component interactions becomes even more apparent
through examining \Natural's failure modes. Without examples \Natural~(and thus
\OmniSQL) generate semantically inaccurate SQL candidates more often; usually
incorrect column or table selection, misunderstanding of column semantics and
value ranges or wrong join patterns result in syntactically correct queries
which result in wrong result sets. For example the schema subsetting component
is unable to work correctly if the initial query candidate is using the wrong
set of tables alltogether, the majority voting algorithm fails to select the
best candidate when all SQL candidates correspond to different result sets and
self-refinement can only rarely repair semantically mislead initial queries. 
In-context learning helps \Natural~to guide the foundational model by supplying
a set of highly relevant, ranked examples which steers the model to put its
attention on specific parts of the SQL schema. The schema-aware example
selection algorithm works particularly well and was shown to be effective even
on data that did not originate from the training splits of the corresponding
benchmarks (\ref{tab:eval:overall-results}). 

\paragraph{Component Contribution Ranking}

The above insights allow for a ranking of components by their estimated impact
based on the ablation evidence from Table~\ref{tab:eval:overall-results} and
Figure~\ref{fig:eval:execution-accuracy-overview}. The pipeline components rank
by impact as follows:

\begin{enumerate}
    \item \textbf{Query Projection ($\pi$)} – The query projection component
        itself remains the largest single contributor in performance and the
        foundation models used predominately drives the performance of the
        system. All other components are orchestrated around query projection.
        This was shown through the \textit{Baseline} configuration which
        achieved 77.9\% and 77.0\% on \Spider~(dev and test) and 32.4\% on
        \Bird. The substantial gap between the two benchmarks highlights how
        severly the model capabilities degrade with increasing task and schema
        complexity.
    \item \textbf{Example Selection ($\sigma$)} – As discussed above, the
        example selection provides a non-neglible improvement in performance to
        the overall system while introducing relatively little candidate
        latency (eg, only +0.1-+2.3s on \Spider). The impact that ICL showed on
        \Bird demonstrates that example selection is not only helpful for LLMs
        generating SQL querys but a \textit{necessary} measure for enabling
        them to handle complex query generation tasks on real-world data.
    \item \textbf{Consensus Voting ($\nu$)} – The consesus voting algorithm
        selects the most prevalent result set amogst multiple generation
        attempts and validates that queries execute without any errors prior to
        returning them to the user. This self-consensus mechanism ensures low
        syntactic error rates across all configurations
        (Figure~\ref{fig:eval:error-rates}). However the voting strategy only
        gains in overall performance impact with an increasing number of
        internal candidates. The evaluation was performed with 1-2 internal
        candidates due to computational complexity of increasing the pipeline
        iterations. This prevents definite conclusions on the impact.
    \item \textbf{Self-Refinement ($\rho$)} – Self-refinement is effectively
        prompting the underlying model for repairing or improving its initially
        produced query candidate. This results in an overall marginal
        improvement of \EA, but comes at a significant computational cost.
        Adding self-refinement nearly doubles the \CL on average and provides
        diminishing returns. The results of the \textit{Zero-Shot}
        configuration, especially on the \Bird~benchmark, suggest that
        self-refinement is unable to correct significant semantic errors on its
        own without in-context learning.
    \item \textbf{Schema-Subsetting ($\phi$)} – Schema subsetting aims to
        reduce the noise for the model by filtering out unused table from the
        SQL schema before running query projection, thus improving token
        efficiency and accuracy. However, the exact impact of schema-subsetting
        is difficult to isolate due to the constrained ablation study that was
        performed. It is questionable whether schema-subsetting itself provides
        a statistically significant improvement in performance or even harms
        performance as the \textit{Baseline} configuration underperformed the
        model baseline.
\end{enumerate}

\paragraph{Accuracy-Latency Tradeoff}

The accuracy-latency measurements which is visualized in
Figure~\ref{fig:eval:candidate-latency} and
Table~\ref{tab:eval:overall-results} reveal three different efficiency profiles
across the systems that where benchmarked.

\begin{enumerate}
    \item \textbf{Baseline} — The baseline profile includes both the
        \textit{Baseline} configuration of \Natural~and the raw \OmniSQL~models.
        The baseline profile is generally yielding the lowest possible
        candidate latency, but also yields the lowest scores for both
        \EA~and~\EM and is thus prioritising latency over accuracy.
    \item \textbf{Zero-Shot} — The zero shot profile is denoted by a suboptimal
        accuracy latency tradeoff where there are marginal performance gain
        over the baseline results but this is accompanied by a strong average
        increase in latency (effectively doubling on average).
    \item \textbf{Full} — Lastly the full profile is a balanced profile which
        weights accuracy over latency. Systems in this class include the
        \textit{Syn}, \textit{Train} and \textit{Ground} configurations of
        \Natural. These systems generally achieve best in class results but
        also a consistently and significantly higher candidate latency.
\end{enumerate}

These three performance profiles highlight different tradeoffs that can be
conciously made depending on the deployment scenario and the target usage
pattern. For highly interactive systems like chat applications and interactive
learning tools, the \textbf{Baseline} profile will yield better latency
characteristics while maintain a significant portion of accuracy, especially on
simple tasks. For more complex usecases where accuracy is generally more
critical than latency (e.g, asynchronous report generation), the \textbf{Full}
profile is representing a more dedsirable tradeoff, which renders the
\textbf{Zero-Shot} profile mostly undesirable.

\paragraph{Optimal Configurations for Different Deployment Scenarios}

\paragraph{Implications for NL2SQL Pipeline Design}

\paragraph{Conclusion}



\subsection{Practical Implications}

\subsubsection{Deployment Viability and User Experience}

\paragraph{System Integration Approaches}

\paragraph{Latency and Interactivity}

\paragraph{Trust, Transparency, and Error Communication}

\subsubsection{Use Case Analysis and Accessibility Benefits}

\paragraph{Optimal Use Cases}

\paragraph{Accessibility and Democratization Impact}



\subsection{Limitations and Threats to Validity}

\subsubsection{Experimental Limitations}

\paragraph{Benchmark Representativeness}

\paragraph{Model and Hardware Specificity}

\paragraph{Reproducibility Challenges}

\subsubsection{Methodological Constraints}

\paragraph{Evaluation Metric Limitations}

\paragraph{Comparison Validity Concerns}

\paragraph{Internal Validity}



\subsection{Future Work}

\subsubsection{System Improvements}

\paragraph{Performance Optimization}

\paragraph{Component Refinement}

\subsubsection{Evaluation and Validation}

\paragraph{User Studies}

\paragraph{Extended Benchmarking}

\subsubsection{Deployment Research}

\newpage
