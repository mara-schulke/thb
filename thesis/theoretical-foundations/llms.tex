\subsection{Large Language Models}

Large language models (LLMs) represented a paradigm shift in natural language
processing, demonstrating state of the art capabilities in understanding, reasoning and
text generation. These models form the core of most modern NL2SQL systems
like \textsc{XiYan-SQL}, \textsc{DAIL} and others. They have a wide language understanding as they have been trained on large text corpora,
are exposed to different domains of traning data and have an inherent
understanding of the SQL syntax as they have been exposed to it during
pre-training. This shifted the approach while developing NL2SQL systems to
focus on in-context learning methods instead of training dedicated models from
scratch.

\paragraph{Feedforward and Residual Connections}

Every layer in a transformer combines a multi-head attention with position-wise
feedforward networks:

\begin{equation}
\text{FFN}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2
\end{equation}

Residual connections and layer normalization stabilize training of complex networks:

\begin{equation}
    \text{Output} = x + \text{Sublayer}(\text{LayerNorm}(x))
\end{equation}

where $\text{Sublayer}$ represents either multi-head attention or the
feedforward network. These components enable transformers to scale to billions
of parameters while maintaining training stability.

\paragraph{Positional Encoding}

Since transformers are able to process sequences in parallel, positional
encodings inject information about token positions. Original transformers use
sinusoidal functions, while modern LLMs often employ learned positional
embeddings or relative position encodings. This positional information is
inherently important for NL2SQL systems, where the order of generated SQL
clauses (e.g., \texttt{SELECT} must come before before \texttt{WHERE}) carries
significant semantic meaning.

\subsubsection{Pre-training and Fine-tuning}

Large language models are trained in a two-stage process that separates the
training of general language understanding from from task-specific
capabilities.

\paragraph{Pre-training}

During pre-training LLMs are exposed to vasts amounts of unlabeled text
gathered from the internet, books etc. Using self-supervised objectives, no
manual labeling of the datasets is required. The most common pre-training
objective is causal language modeling, where a model learns to predict the next
token given a previous context:

\begin{equation}
\mathcal{L}_{\text{LM}} = -\sum_{t=1}^{T} \log P(x_t \mid x_{<t}; \theta)
\end{equation}

This objective engrains general language patterns, common knowledge,
reasoning capabilities, and even programming knowledge into models 
when code is included in the training datasets. Models like GPT, LLaMA, and Qwen
are pre-trained using this objective, aiming to predict coherent
continuations of text sequences.

\paragraph{Fine-tuning}

Fine-tuning is the process of adapting a pre-trained model, for example for
solving NL2SQL tasks, by continuing model training on a labeled dataset of
question-answer pairs (eg, natural language to sql). During fine-tuning, the
model can learn to:

\begin{itemize}
    \item Map natural language to database schema elements (also known as schema-linking)
    \item Generate SQL queries following proper grammar
    \item Compose more complex SQL queries involving joins, aggregations, and subqueries
    \item Understand domain-specific terminology (depending on the dataset)
\end{itemize}

Fine-tuning usualy require significantly smaller datasets (thousands to
hundreds of thousands of examples) compared to pre-training (billions to
trillions of tokens), largely relying on the natural language understanding
capabilities acquired during the pre-training phase.

Models like \textsc{OmniSQL} demonstrate that smaller fine-tuned models 
can outperform significantly larger LLMs for NL2SQL tasks using a fraction of
the parameters \citep{OmniSQL}.

\subsubsection{In-Context Learning}

In-context learning (ICL) is the process of teaching a model how to perform a
certain task by providing examples in its prompt, without updating models
parameters. Thus ICL and Fine-tuning differ by their timing: runtime vs learning.

This capability emerged with sufficiently large models which have large enough
context windows to fit in examples of solutions for similar problems. This
method was shown to be very effective by \citeauthor{DAIL} in \citeyear{DAIL},
improving the performance of general purpose LLMs as well as fine-tuned LLMs
when being presented with relevant examples \citep{DAIL}. The combination of
fine-tuned models like \textsc{OmniSQL} with ICL is therefore a promising
research for NL2SQL systems.
