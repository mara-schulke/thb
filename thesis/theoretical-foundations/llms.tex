\subsection{Large Language Models}

Large language models (LLMs) are an advancement built on the transformer
architecture for natural language processing, demonstrating state of the art
capabilities in understanding, reasoning and text generation. These models form
the core of most modern NL2SQL systems like \textsc{XiYan-SQL}, \textsc{DAIL}
and others. They have wide language understanding as they have been trained on
large text corpora, are exposed to different domains of traning data and have
an inherent understanding of the SQL syntax as they have been exposed to it
during pre-training. This shifted the approach while developing NL2SQL systems
to focus on in-context learning methods instead of training dedicated models
from scratch.

\subsubsection{Feedforward and Residual Connections}

Every layer in a transformer combines a multi-head attention with position-wise
feedforward networks:

\begin{equation}
\text{FFN}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2
\end{equation}

Residual connections and layer normalization stabilize training of complex
networks:

\begin{equation}
    \text{Output} = x + \text{Sublayer}(\text{LayerNorm}(x))
\end{equation}

where $\text{Sublayer}$ represents either multi-head attention or the
feedforward network. These components enable transformers to scale to billions
of parameters while maintaining training stability.

\subsubsection{Positional Encoding}

Since transformers process sequences in parallel, positional encodings inject
information about token positions. Original transformers use sinusoidal
functions, while modern LLMs often employ learned positional embeddings or
relative position encodings. This positional information is important for
NL2SQL systems, where the order of generated SQL clauses (e.g., \texttt{SELECT}
must come before \texttt{WHERE}) carries semantic meaning.

\subsubsection{Pre-training and Fine-tuning}

Large language models are trained in a two-fold process that separates the
training of general language understanding from task-specific capabilities.

\paragraph{Pre-training}

During pre-training LLMs are exposed to vasts amounts of unlabeled text
gathered from the internet, books etc. Using self-supervised objectives, no
manual labeling of the datasets is required. The most common pre-training
objective is causal language modeling, where a model learns to predict the next
token given a previous context:

\begin{equation}
\mathcal{L}_{\text{LM}} = -\sum_{t=1}^{T} \log P(x_t \mid x_{<t}; \theta)
\end{equation}

This objective ingrains general language patterns, common knowledge,
reasoning capabilities, and even programming knowledge into models 
when code is included in the training datasets. Models like GPT, LLaMA, and
Qwen are pre-trained using this objective, aiming to predict coherent
continuations of text sequences.

\paragraph{Fine-tuning}

Fine-tuning is the process of adapting a pre-trained model, for example for
solving NL2SQL tasks, by continuing model training on a labeled dataset
(eg, natural language question to expected sql query). During fine-tuning, the
model can learn to map natural language questions to expected SQL queries when
being presented with a question and a database schema.

Fine-tuning usualy requires smaller datasets (thousands to hundreds of
thousands of examples) compared to pre-training (billions to trillions of
tokens), largely relying on the natural language understanding capabilities
acquired during the pre-training phase.

Models like \textsc{OmniSQL} demonstrate that smaller fine-tuned models can
outperform larger LLMs for NL2SQL tasks using a fraction of the parameters
\citep{OmniSQL}.

\subsubsection{In-Context Learning}

In-context learning (ICL) is the process of teaching a model how to perform a
certain task by providing examples in its prompt, without updating models
parameters. Thus ICL and Fine-tuning differ by their timing: runtime vs
learning.

This capability emerged with sufficiently large models which have large enough
context windows to fit in examples of solutions for similar problems. This
method was shown to be very effective by \citeauthor{DAIL-SQL} in \citeyear{DAIL-SQL},
improving the performance of general purpose LLMs as well as fine-tuned LLMs
when being presented with relevant examples \citep{DAIL-SQL}. The combination of
fine-tuned models like \textsc{OmniSQL} with ICL is therefore a promising
research for NL2SQL systems.

\subsubsection{Prompt Engineering}

Prompt engineering is the process of structuring the input to LLMs in a way
where they adhere to desired behaviours. For NL2SQL systems previous research
has found diverging effectiveness between different example, schema and
question presentation mechanisms \citep{DAIL-SQL, OmniSQL}.

\subsubsection{Chain-of-Thought Reasoning}

Prompting or training models to output their chain-of-thought encourages models 
to generate reasoning traces before producing their final answer to a question
or input. For NL2SQL, this usually involves generating explanations of query
logic, analysing the input database schema, and planning the query structure
before generating a query.

Fine-tuned models like \textsc{OmniSQL} are fine-tuned to produce
chain-of-thought output during generation in markdown which enables debugging
model behaviour and understanding the query generation approach. 

\subsubsection{Model Limitations}

Understanding the limitations of large language models is essential for
designing robust NL2SQL systems.

\paragraph{Hallucination}

As LLMs are just predicting a sequence of output tokens, they have no inherent
understanding of the actual problem domain and solution space they are
presented with. Thus in NL2SQL systems, LLMs might generate invalid SQL queries
which might reference non existent tables, columns or relationships, output
invalid syntax or generate promising but semantically invalid queries.

These flaws are inherent to a text based representation and are non
recoverable, although through candidate validation and self-refinement
approaches most obviously invalid queries can be recovered. Detecting semantic
invalidity is a hard research problem on its own.

\paragraph{Context Window Limitations}

As the underlying transformer architecture is still bound to finite context
windows, extensive database schemas or long lists of examples can exceed the
window, thus causing the query generation to fail.
