\section{Implementation}

This chapter presents the practical implementation of the \textsc{Natural}
system, translating the theoretical framework outlined in Chapter
\ref{section:design} into a working NL2SQL system.

Primary attention is given to the engineering challenges encountered,
performance bottlenecks identified, and optimization strategies implemented to
achieve practical deployment viability on consumer hardware.
The chapter is comprised of a software architecture and infrastructure
discussion as well as implementation outlines of each pipeline component.

\subsection{Architecture and Infrastructure}

The implementation of the system design phase is split into inference, sampling
and evaluation code. The runtime code focuses on the actual algorithm
implementations outlined in section \ref{section:design}, sampling code focuses on
indexing samples and computing distance indices and the evaluation code runs
the \textsc{Natural} pipeline on prevalent benchmarks.

This subsection focuses on outlining the scope of each technological component,
the design rationale behind them and discusses the technology stack
decisions.

\subsubsection{Software Architecture}

\textsc{Natural} consists of five software components:

\begin{enumerate}
    \item \texttt{natural-models} – For handling the actual model
        execution on GPUs for inference and embedding.
    \item \texttt{natural-graphs} – Graph library for representing
        database schemas in graphs as well as computing graph similarities and
        distances.
    \item \texttt{natural-inference} – The core pipeline implementation using
        other software components during inference time.
    \item \texttt{natural-sampling} – The sampling setup that focuses on
        indexing samples and computing graph distance indices.
    \item \texttt{natural-benchmark} – The benchmarking setup used to run
        experiments, continuously evaluate the accuracy of the system and
        compile statistics.
\end{enumerate}

% Inter-crate communication patterns and shared data structures
\begin{verbatim}
    THIS NEEDS A DIAGRAM TO UNDERSTAND THE SOFTWARE ARCH.
\end{verbatim}

This separation of concerns emerged as the split between inference, sampling
and evaluation code required shared fundamentals like model execution and graph
representation which in turn improves testability and maintainability. Furthermore
separating the inference, sampling and evaluation code yields a smaller
footprint when embedding the inference code into databases.

Challenges associated with this multi component architecture are mostly tied to
dependency management and increased build complexity although these can be
mitigated through \texttt{cargo}'s workspace support.

\subsubsection{Resource Management Strategy}

Given the constrained 24 gigabytes of VRAM capacity of the RTX 3090 not all
available model sizes can be used. While 14B model variants theoretically work,
the need for an embedding model typically exhausts the VRAM unless using
steep quantization formats. The \textsc{OmniSQL} 7B models were shown to have
an average performance degradation of only 0.3\% compared to their larger 14B
counterparts by \citeauthor{OmniSQL} in \citeyear{OmniSQL}. On the spider test
dataset the 7B model surprisingly outperformed its 14B counterpart by 0.6\%.
Due to the unclear performance gains of the 14B model, the significant increase
in inference time and the limited VRAM available, this thesis focuses on the 7B
variant of \textsc{OmniSQL}.

As an embedding model is required for doing semantic search of samples in the
pipeline a small companion model is required to embed both the sample datasets
(\textsc{SynSQL}, \textsc{Bird} and \textsc{Spider}) during sampling time and
the user provided natural language question during inference. The Qwen3 series
embedding models ranked first place on the MTEB multilingual leaderboard. With
the Q8\_0 quantized version of the 8B model consumes 8 gigabytes of VRAM which
is not possible to fit into 24 gigabytes of VRAM when accounting for KV-Cache
requirements. Therefore, instead of choosing a heavily quantized version of the
8B variant (ie, Q4\_K\_M or below), the 4B model with Q8\_0 shows similar
performance characteristics on simple use cases while offering a significantly
smaller memory footprint at 2 gigabytes.
\textcolor{red}{https://huggingface.co/Qwen/Qwen3-Embedding-8B cite}.

As models have significant loading times, the system loads models globally and 
at startup and hands around references using atomic reference counting
(\texttt{std::sync::Arc}) to ensure that models are not loaded twice, can be
reused between different inference calls and GPU memory is not exhausted which
would lead to a program crash.

\subsubsection{Technology Stack Decisions}

The choice of programming language, inference frameworks, and supporting
libraries implies the system's performance and deployment characteristics,
as well as development velocity. This section analyzes the key decisions
made for \textsc{Natural}, discussing their trade-offs between research
flexibility and production readiness, performance optimization and development
speed, and ecosystem maturity versus cutting-edge capabilities.

The critical decisions are the programming language, the model
inference framework, and the vector similarity search solution. Each
decision was evaluated against the constraints of limited hardware resources
(24GB VRAM), the need for database integration capabilities, and the
requirement for reproducible research outcomes.

\paragraph{Language and Ecosystem}

The most apparent and impactful decision is likely the language and ecosystem
choice made. Viable languages for implementing natural language processing and
machine learning heavy systems are Python, R, Julia, Rust, C / C++ and Java as
well as other general purpose languages.

While interpreted languages like Python, R and Julia tend to be significantly
easier to use for rapid prototyping and approach validation due to their loose
type systems and great scientific ecosystem, they come with serious drawbacks
with regards to deployability, speed and robustness compared to compiled and
strongly typed languages.

Languages like Java, C / C++ and Rust offer greater stability at runtime,
better interoperability into other programs (eg, database extensions) and better
resource utilization they represent an interesting trade-off between performance
optimization and portability vs. speed of iteration and research ecosystems.

R and Julia are inferior to Python when it comes to adoption, machine learning
frameworks and natural language processing. Java requires a Java Virtual
Machine (JVM) at runtime which yields worse portability than C / C++ and Rust
while offering little advantage over interpreted languages like Python. Thus
Python, C / C++ and Rust emerge as strong contenders for the implementation of
\textsc{Natural}. C and C++ have the primary downside that they are prone to
program crashes and memory safety issues whereas Rust resolves most of these
downsides while maintaining similar performance, memory management and
portability characteristics.

Given that performance plays a critical role when developing machine learning
systems with limited access to hardware, the path to a potential production
deployments of NL2SQL is easier and the language ecosystem offers bindings
for the most notable scientific libraries, Rust offers a great value
proposition for ML systems at the cost of development speed.

\paragraph{Inference Framework}

For local model inference, two primary frameworks emerged: llama.cpp (FFI), 
Candle (Rust-native). While Rust-native frameworks generally offer better 
portability, llama.cpp provided a better balance between performance, ecosystem 
maturity, and implementation simplicity.

llama.cpp provides out-of-the-box optimizations with comprehensive GGUF 
quantization support, yielding \textcolor{red}{25-30 tokens per second} on
consumer grade RTX 3090 hardware. It offers high-level abstractions for model
loading and tokenization and sampling. The mature GGML ecosystem and advanced
quantization schemes (like \texttt{Q4\_K\_M}, \texttt{Q8\_0}) justified the FFI
complexity.

Candle, even though it offered comparable raw inference speeds, required
rather extensive low-level implementation work and proved incompatibility with
scenarios where the resulting binary is embedded into a database (e.g. PostgreSQL).
The combination of manual tensor management, more convoluted quantization support,
and deployment constraints made llama.cpp a better alternative for
production-ready systems.

\paragraph{Similarity Search Framework}

For lightweight similarity search SQLite in combination with it's
\texttt{sqlite-vec} extension is a sensible option. Especially for smaller data
loads introducing the complexity of \texttt{faiss} and comparable vector search
solutions like \texttt{qdrant} outweighs their speed benefits.

The \texttt{sqlite-vec} extension advertises to be a ``fast enough'' vector
search solution, allowing for reduced complexity and compatibility with
graphical database interfaces that support SQLite to inspect the embedding
space.

\paragraph{Trade-offs}

Overall the development overhead of choosing Rust over Python for the
implementation phase was noticeable; Rust's machine learning and natural
language processing frameworks are less advanced, porting research code written
in python from other papers (like \cite{OmniSQL}) turned out to be non trivial.
This decision likely doubled the time needed for the implementation, but in
turn provides a clear path for the algorithms in this thesis to be
productionized. The outcome of the implementation has significantly better
performance and portability characteristics than using Python would have
allowed for.

\subsection{Pipeline Implementation}

This subsection describes the system design realisation of each core pipeline
component ($\sigma$, $\phi$, $\pi$, $\rho$, $\nu$) into working code, their
algorithmic details, performance characteristics and trade-offs made.

\subsubsection{Example Selection Engine ($\sigma$)}

The example selection is implemented using a multi-dimensional similarity
scoring system that identifies the most relevant samples that were previously
indexed for subsequent in-context learning. The component addresses the
fundamental challenge of selecting contextually appropriate examples from large
training corpus while balancing semantic relevance and structural compatibility.

\paragraph{Similarity Computation}

The selection mechanism combines three distinct similarity measures implemented
in the selection module in \texttt{natural-sampling::selection}. The 
\texttt{selection} algorithm computes semantic similarity through cosine
distance of the masked question embedding. Structural SQL similarity is
computed via measuring the cosine distance of the masked SQL embedding, and
schema compatibility using WWL kernel distance from the \texttt{natural-graphs}
component. The embeddings are computed using the Qwen3-Embedding model (4B)
using the query masking algorithm in \texttt{natural-sampling::masking}.

\paragraph{Weighting Strategy and Performance Characteristics}

As this algorithm is aware of three distinct ways to measure sample similarity
(masked question, masked query, structural). The implementation uses
empirically optimized weights defined as constants: 70\% for semantic question
similarity and 30\% for SQL structural similarity. Additionally, the final
scoring combines 70\% sample-level similarity with 30\% schema-level
compatibility.

The selection algorithm yields a maximum of 32 candidates (\texttt{TOP\_K =
32}) to limit computational overhead while maintaining selection quality.
Vector similarity queries are performed using SQLite with the sqlite-vec
extension, achieving sub-second retrieval times for vector databases exceeding
2,000,000 samples.

\paragraph{Implementation Architecture}

Using the \texttt{Selector} struct the embedding computation and representation
is encapsulated from the actual selection algorithm. Thus via
\texttt{Selector::new} the consuming code can compute the masked embeddings and
use them subsequently to run example selection.

The \texttt{selection} algorithm implementation follows a two-stage approach,
split into initial candidate retrieval through vector search and subsequent
reweighting using the above described weights and a precomputed WWL distance
index from \texttt{natural-graphs}.

