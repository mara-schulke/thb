\section{Implementation}

This chapter presents the practical implementation of the \textsc{Natural}
system, translating the theoretical framework outlined in Chapter
\ref{section:design} into a working NL2SQL system.

Primary attention is given to the engineering challenges encountered,
performance bottlenecks identified, and optimization strategies implemented to
achieve practical deployment viability on consumer hardware.
The chapter is comprised of a software architecture and infrastructure
discussion as well as implementation outlines of each pipeline component.

\subsection{Architecture and Infrastructure}

The implementation of the system design phase is split into inference, sampling
and evaluation code. The runtime code focuses on the actual algorithm
implementations outlined in section \ref{section:design}, sampling code focuses on
indexing samples and computing distance indices and the evaluation code runs
the \textsc{Natural} pipeline on prevalent benchmarks.

This subsection focuses on outlining the scope of each technological component,
the design rationale behind them and discusses the technology stack
decisions.

\subsubsection{Software Architecture}

\textsc{Natural} consists of five software components:

\begin{enumerate}
    \item \texttt{natural-models} – For handling the actual model
        execution on GPUs for inference and embedding.
    \item \texttt{natural-graphs} – Graph library for representing
        database schemas in graphs as well as computing graph similarities and
        distances.
    \item \texttt{natural-inference} – The core pipeline implementation using
        other software components during inference time.
    \item \texttt{natural-sampling} – The sampling setup that focuses on
        indexing samples and computing graph distance indices.
    \item \texttt{natural-benchmark} – The benchmarking setup used to run
        experiments, continuously evaluate the accuracy of the system and
        compile statistics.
\end{enumerate}

% Inter-crate communication patterns and shared data structures
\begin{verbatim}
    THIS NEEDS A DIAGRAM TO UNDERSTAND THE SOFTWARE ARCH.
\end{verbatim}

This separation of concerns emerged as the split between inference, sampling
and evaluation code required shared fundamentals like model execution and graph
representation which in turn improves testability and maintainability. Furthermore
separating the inference, sampling and evaluation code yields a smaller
footprint when embedding the inference code into databases.

Challenges associated with this multi component architecture are mostly tied to
dependency management and increased build complexity although these can be
mitigated through \texttt{cargo}'s workspace support.

\subsubsection{Resource Management Strategy}

Given the constrained 24 gigabytes of VRAM capacity of the RTX 3090 not all
available model sizes can be used. While 14B model variants theoretically work,
the need for an embedding model typically exhausts the VRAM unless using
steep quantization formats. The \textsc{OmniSQL} 7B models were shown to have
an average performance degradation of only 0.3\% compared to their larger 14B
counterparts by \citeauthor{OmniSQL} in \citeyear{OmniSQL}. On the spider test
dataset the 7B model surprisingly outperformed its 14B counterpart by 0.6\%.
Due to the unclear performance gains of the 14B model, the significant increase
in inference time and the limited VRAM available, this thesis focuses on the 7B
variant of \textsc{OmniSQL}.

As an embedding model is required for doing semantic search of samples in the
pipeline a small companion model is required to embed both the sample datasets
(\textsc{SynSQL}, \textsc{Bird} and \textsc{Spider}) during sampling time and
the user provided natural language question during inference. The Qwen3 series
embedding models ranked first place on the MTEB multilingual leaderboard. With
the Q8\_0 quantized version of the 8B model consumes 8 gigabytes of VRAM which
is not possible to fit into 24 gigabytes of VRAM when accounting for KV-Cache
requirements. Therefore, instead of choosing a heavily quantized version of the
8B variant (ie, Q4\_K\_M or below), the 4B model with Q8\_0 shows similar
performance characteristics on simple use cases while offering a significantly
smaller memory footprint at 2 gigabytes.
\textcolor{red}{https://huggingface.co/Qwen/Qwen3-Embedding-8B cite}.

As models have significant loading times, the system loads models globally and 
at startup and hands around references using atomic reference counting
(\texttt{std::sync::Arc}) to ensure that models are not loaded twice, can be
reused between different inference calls and GPU memory is not exhausted which
would lead to a program crash.

\subsubsection{Technology Stack Decisions}

The choice of programming language, inference frameworks, and supporting
libraries implies the system's performance and deployment characteristics,
as well as development velocity. This section analyzes the key decisions
made for \textsc{Natural}, discussing their trade-offs between research
flexibility and production readiness, performance optimization and development
speed, and ecosystem maturity versus cutting-edge capabilities.

The critical decisions are the programming language, the model
inference framework, and the vector similarity search solution. Each
decision was evaluated against the constraints of limited hardware resources
(24GB VRAM), the need for database integration capabilities, and the
requirement for reproducible research outcomes.

\paragraph{Language and Ecosystem}

The most apparent and impactful decision is likely the language and ecosystem
choice made. Viable languages for implementing natural language processing and
machine learning heavy systems are Python, R, Julia, Rust, C / C++ and Java as
well as other general purpose languages.

While interpreted languages like Python, R and Julia tend to be significantly
easier to use for rapid prototyping and approach validation due to their loose
type systems and great scientific ecosystem, they come with serious drawbacks
with regards to deployability, speed and robustness compared to compiled and
strongly typed languages.

Languages like Java, C / C++ and Rust offer greater stability at runtime,
better interoperability into other programs (eg, database extensions) and better
resource utilization they represent an interesting trade-off between performance
optimization and portability vs. speed of iteration and research ecosystems.

R and Julia are inferior to Python when it comes to adoption, machine learning
frameworks and natural language processing. Java requires a Java Virtual
Machine (JVM) at runtime which yields worse portability than C / C++ and Rust
while offering little advantage over interpreted languages like Python. Thus
Python, C / C++ and Rust emerge as strong contenders for the implementation of
\textsc{Natural}. C and C++ have the primary downside that they are prone to
program crashes and memory safety issues whereas Rust resolves most of these
downsides while maintaining similar performance, memory management and
portability characteristics.

Given that performance plays a critical role when developing machine learning
systems with limited access to hardware, the path to a potential production
deployments of NL2SQL is easier and the language ecosystem offers bindings
for the most notable scientific libraries, Rust offers a great value
proposition for ML systems at the cost of development speed.

\paragraph{Inference Framework}

For local model inference, two primary frameworks emerged: llama.cpp (FFI), 
Candle (Rust-native). While Rust-native frameworks generally offer better 
portability, llama.cpp provided a better balance between performance, ecosystem 
maturity, and implementation simplicity.

llama.cpp provides out-of-the-box optimizations with comprehensive GGUF 
quantization support, yielding \textcolor{red}{25-30 tokens per second} on
consumer grade RTX 3090 hardware. It offers high-level abstractions for model
loading and tokenization and sampling. The mature GGML ecosystem and advanced
quantization schemes (like \texttt{Q4\_K\_M}, \texttt{Q8\_0}) justified the FFI
complexity.

Candle, even though it offered comparable raw inference speeds, required
rather extensive low-level implementation work and proved incompatibility with
scenarios where the resulting binary is embedded into a database (e.g. PostgreSQL).
The combination of manual tensor management, more convoluted quantization support,
and deployment constraints made llama.cpp a better alternative for
production-ready systems.

\paragraph{Similarity Search Framework}

For lightweight similarity search SQLite in combination with it's
\texttt{sqlite-vec} extension is a sensible option. Especially for smaller data
loads introducing the complexity of \texttt{faiss} and comparable vector search
solutions like \texttt{qdrant} outweighs their speed benefits.

The \texttt{sqlite-vec} extension advertises to be a ``fast enough'' vector
search solution, allowing for reduced complexity and compatibility with
graphical database interfaces that support SQLite to inspect the embedding
space.

\paragraph{Trade-offs}

Overall the development overhead of choosing Rust over Python for the
implementation phase was noticeable; Rust's machine learning and natural
language processing frameworks are less advanced, porting research code written
in python from other papers (like \cite{OmniSQL}) turned out to be non trivial.
This decision likely doubled the time needed for the implementation, but in
turn provides a clear path for the algorithms in this thesis to be
productionized. The outcome of the implementation has significantly better
performance and portability characteristics than using Python would have
allowed for.

\subsection{Pipeline Implementation}

This subsection describes the system design realisation of each core pipeline
component ($\sigma$, $\phi$, $\pi$, $\rho$, $\nu$) into working code, their
algorithmic details, performance characteristics and trade-offs made.

\subsubsection{Example Selection Engine ($\sigma$)}

The example selection system is implemented using a multi-dimensional similarity
scoring system that identifies the most relevant samples that were previously
indexed for subsequent in-context learning. The component addresses the
fundamental challenge of selecting contextually appropriate examples from large
training corpora whilst balancing semantic relevance and structural compatibility.

\paragraph{Similarity Computation}

The selection mechanism combines three distinct similarity measures implemented
in the selection module in \texttt{natural-sampling::selection}. The 
\texttt{selection} algorithm computes semantic similarity through cosine
distance of the masked question embedding. Structural SQL similarity is
computed via measuring the cosine distance of the masked SQL embedding, and
schema compatibility using WWL kernel distance from the \texttt{natural-graphs}
component. The embeddings are computed using the Qwen3-Embedding model (4B)
using the query masking algorithm in \texttt{natural-sampling::masking}.

\paragraph{Weighting Strategy and Performance Characteristics}

As this algorithm is aware of three distinct ways to measure sample similarity
(masked question, masked query, structural). The implementation employs
empirically optimized weights defined as constants: 70\% for semantic question
similarity and 30\% for SQL structural similarity. Additionally, the final
scoring combines 70\% sample-level similarity with 30\% schema-level
compatibility.

The selection algorithm yields a maximum of 32 candidates (\texttt{TOP\_K =
32}) to limit computational overhead whilst maintaining selection quality.
Vector similarity queries are performed using SQLite with the \texttt{sqlite-vec}
extension, achieving sub-second retrieval times for vector databases exceeding
2,000,000 samples.

\paragraph{Implementation Architecture}

Using the \texttt{Selector} struct the embedding computation and representation
is encapsulated from the actual selection algorithm. Thus via
\texttt{Selector::new} the consuming code can compute the masked embeddings and
use them subsequently to run example selection.

The \texttt{selection} algorithm implementation follows a two-stage approach,
split into initial candidate retrieval through vector search (using the
\texttt{Vector} struct and SQLite) and subsequent reweighting using the above
described weights and a precomputed WWL distance index from
\texttt{natural-graphs}.

\subsubsection{Schema Subsetting System ($\phi$)}

The schema subsetting system is implemented in
\texttt{natural-inference::pipeline::subsetting} as
\texttt{SchemaSubsetter} struct. The core algorithm is the \texttt{optimize}
method (lines 21-47) which performs query validation to determine
which tables are crucial for the query candidates provided.

\paragraph{Query Validation}

Query validation employs a trial-and-error approach where for each query
candidate and table in the schema, an in-memory SQLite database
is created using \texttt{Connection::open\_in\_memory()}. Subsequently the
connection is initialized using the schema with all tables in the schema except
the current one.

Once the connection is ready, the \texttt{SchemaSubsetter} prepares every
query candidate against this reduced schema. If preparation fails, this
indicates the table removed is crucial for the query, thus it gets added to the
list of crucial tables. This process is repeated for all query-table
combinations to build a minimal schema containing only essential tables for the
execution of the set of query candidates provided to the subsetting algorithm.

\paragraph{Performance and Trade-off Characteristics}

As a new in-memory database for each table-query combination is created, this
leads to $O(queries \times tables)$ complexity. \textcolor{red}{For schemas
with XXX+ tables and multiple query candidates, this results in XXX-XXXms
processing time compared to 10-15ms for heuristic approaches.}

However, this execution-based approach provides superior correctness guarantees since heuristic
approaches cannot determine whether candidates will execute successfully in real
database environments. Using a real SQLite in memory instance both the correctness of
query candidates and the actually referenced set of tables can be known prior
to actual query execution.

\paragraph{Pipeline Integration}

The \texttt{SchemaSubsetter} is used prior to prompting the model using ICL to
ensure that the model context is used efficiently and attention is given to the
relevant parts of the schema. The \texttt{optimize} method returns a
\texttt{Schema} object containing only the tables identified as crucial through
execution testing which is in turn processed by the subsequent pipeline stages
like generation and refinement.

\subsubsection{Query Projection ($\pi$)}

The ICL module is implementing the query projection algorithm described in
section~\ref{design:projection-function}. It is implemented in the
\texttt{ICLGenerator} struct and wraps the \texttt{SqlModel} struct from
\texttt{natural-models}. Using llama.cpp it runs model inference using a
prompt optimized for in-context-learning with \textsc{OmniSQL}.

\paragraph{In-Context Learning}

The \texttt{ICLGenerator::generate} method implements few-shot prompting
with relevance filtering. Relevance filtering refers to removing all selected
samples with a similarity of less than 0.5 (this value was emperically derived
from evaluations). Thus only semantically or structurally similar samples are
actually provided to the model.

The prompt is constructed based on an adapted version of the \textsc{OmniSQL}
format \citep{OmniSQL}: task overview, sql schema, filtered examples with
similarity scores, explicit instructions, and the target question.
The biggest differentiation to the prompt of \citeauthor{OmniSQL} is the
example section including similarity. For the actual SQL query presentation the
code representation prompt format is used, inspired by DAIL-SQL
\citep{DAIL-SQL}. Every example includes the masked question, similarity score,
and formatted SQL query for clarity.

\paragraph{Prompt Engineering Strategy}

As outlined above the \textsc{OmniSQL} prompt was used as base for
\textsc{Natural}'s prompt together with a code representation prompt.
\textsc{Natural} uses more explicit instructions compared to DAIL-SQL. Key
differentiations include the inclusion of similarity scores to give the model
the ability to weight the samples itself.

This prompt steers the model towards precision and chain-of-thought reasoning. 
It addresses apparent LLM issues like verbosity, over complexity or missing
query constraints, as well as the hallucination and accuracy concerns
identified in literature review section where LLMs generate plausible but
incorrect SQL queries.

\begin{verbatim}
EXAMPLE OF THE PROMPT
\end{verbatim}

\paragraph{Model Integration and Performance}

The \texttt{SqlModel} is wrapped around \texttt{llama-cpp-2} bindings and
loaded globally at startup to avoid a 30-60 second initialization times per
query. Using the \texttt{prompt} method from \texttt{natural-models}
tokenization, inference, and decoding with configurable PromptParams for
context size and generation limits is handled automatically.

\paragraph{Output Processing and Validation}

As \textsc{OmniSQL} was finetuned to output its thoughts and predictions using
markdown a markdown postprocessing module is needed, as well as a module to
identify whether a SQL query is syntactically valid.

To extract possible candidates all generated responses are post-processed
through a markdown parser (\texttt{pulldown-cmark}) which parses the model
output and looks for the code-block fence characters \texttt{```}.
Subsequently all candidates are processed in reversed order and the first full
valid query is returned as potential candidate. The reversing of processing
order is needed as the model outputs it's thoughts top-to-bottom as a markdown
document with the most likely answer usually being output at the end. This
approach ensures that a returned query candidates are executable and
ensured to contain valid SQL. Thus \textsc{Natural} can aid the
difficulty of generating perfect SQL queries by acknowledging
limitations from large language models and implementing recovery and refinement
mechanisms in the subsequent pipeline flow.

\subsubsection{Self-Refinement Mechanism ($\rho$)}

The self-refinement algorithm described in~\ref{design:refinement-function}
is implemented in \texttt{natural-inference::pipeline::refinement}.
This implementation corresponds to the $\rho$ function, providing
automated error correction through execution feedback and iterative
improvement of generated SQL queries.

\paragraph{Error Correction Through Execution Feedback}

The \texttt{Refinement::optimize} method takes a \texttt{RawQueryCandidate} and
attempts to improve it through targeted prompting. The refinement prompt
explicitly instructs the model to ``spot any errors in the SQL query, correct
them'' and provides the original question, schema context, and the candidate
query that needs improvement.

Contrary to the ICL generation which uses few-shot examples, the
refinement process focuses on single-query self-correction with explicit
error-fixing instructions.

\paragraph{Prompt Engineering for Correction}

The refinement prompt uses a structured format similar to ICL generation but
with key differences: it includes the original question as context, provides
the candidate query that needs fixing, and uses explicit correction language.

\begin{verbatim}
    SHOW PROMPT LAYOUT
\end{verbatim}

\paragraph{Model Resource Management}

As both the ICL implementation and the refinement are ultimately generating
SQL, they reuse the same underlying \texttt{SqlModel} from the shared pipeline
context rather than loading separate models. This design choice
significantly reduces memory requirements but potentially impacts refinement
quality compared to having dedicated refinement models with different prompt
conditioning. Due to the limited available hardware the effects of having a
separate, distinc refinement model could not be verified. \citeauthor{XiYan}
have implemented multi model generation pipelines in \citeyear{XiYan} and
achieved promising results which.

\paragraph{Output Processing and Validation}

Similar to ICL generation, refined responses are processed through the same
\texttt{Markdown} parser to extract the predicted SQL. The system validates each
extracted candidate by trying to parse it using \texttt{QueryCandidate::try\_from}
and returns the first syntactically valid refinement starting from the bottom.
This ensures that refinement produces executable SQL whilst falling back
gracefully if refinement fails.

\paragraph{Integration with Pipeline}

The refinement module is integrated into the main pipeline after the ICL
generator. It processes the \texttt{RawQueryCandidate} and provides a
self-correction mechanism that improves overall pipeline robustness. Notably both
the unrefined and the refined query candidates are kept in the candidate set
for majority voting.

\subsubsection{Consensus Voting System ($\nu$)}

The \texttt{natural-inference::pipeline::voting} implements the majority
function $\nu$ – this implementation closely follows the design and algorithm
outlined in the section~\ref{design:voting-function}. The function \texttt{voting}
providing a result-based self-consensus mechanism which takes in the set of
candidates that were predicted during the generation phase and returns the most
likely query.

\paragraph{Result-Based Self-Consensus}

\citeauthor{OmniSQL} have shown in \citeyear{OmniSQL} that self-consensus can 
significantly improve the accuracy of models that are already showing 
state-of-the-art performance. The result-based self-consensus mechanism is
executing every query candidate, verifies that it works on the database, and
loads it's results. By partitioning the available candidates into buckets based
on their hashed result, queries can be deemed semantically equivalent if they
end up in the same bucket. After every query has been exeuted and partitioned,
the algorithm groups the buckets in a hash map \texttt{buckets} with the result
hash as key and bucket as value.

\paragraph{Bucket Selection Heuristic}

The heuristic method employed by the voting algorithm is steering the pipeline
to agree with itself – if multiple generation attempts yielded the same result,
these generation attempts are more likely than outliers.

\paragraph{Error Handling and Fallback}

Queries that voting function fails with execution are that contains in an
\texttt{errors} hash set to provide than being included the calling side all
candidates fail execution, the voting function fails with an error that
contains all collected execution errors to provide diagnostic information for
the calling side.

\paragraph{Limitations and Trade-offs}

The result-based partitioning approach has limitations with regards to subtle
errors that differ by few rows, as these would be treated as completely
different result buckets. Furthermore executing every query candidate against
the real database comes along with a significant performance penalty for
expensive queries where the voting step might incur load onto the database, use
significant amounts of memory for loading in all data into memory and slow down
the voting as the entire result needs to be hashed. This system lacks model
uncertainty calibration for SQL confidence scoring which could be used as
another dimension for partitioning.

Another more advanced optimization could be to group by result-schema and row
count, which still needs to execute the candidate queries partially but does
not load the actual data from the database into memory. For the scope of this
thesis and small real-world application scenarios the cost of this stage is
negligible compared to the llm inference done in previous steps. 

\paragraph{Integration with Pipeline}

The voting mechanism represents the last step of the pipeline function 
and selects the final consensus from all generated and refined candidates, 
and ultimately returns the output of the \textsc{Natural} system.

\subsection{Supporting Systems and Optimizations}

The supporting systems of \textsc{Natural} are important tools to simplify the 
actual pipeline development, employing performance optimizations and enable
rapid development of new approaches and hypotheses. The primary support systems
are \texttt{Vector} – a vector database abstraction on top of SQLite,
\texttt{wwl} – a library that implements Wasserstein-Weisfeiler-Leman kernels
\citep{WWL} and the \texttt{embedding} and \texttt{masking} modules to compare
and semantically search the embedding space.

\subsubsection{Vector Database}

The vector database implementation \texttt{Vector} is used both during
sampling to construct the vector db and runtime to run the sample selection
algorithm. Using SQLite and \texttt{sqlite-vec} to implement cosine search in
text corpora of up to a few million samples dramatically simplified the
development of \textsc{Natural} as no manual performance optimizations had to
be implemnented. SQLite has a second architectural benefit besides relatively
good performance, which is the portability characteristics of having all
samples indexed in a single file on disk. This enables swapping out the
respectively used samples by using another \texttt{.vector} file on disk.

\begin{verbatim}
let question = "How many ..";

// Initialize vector database for similarity search
let vector = Vector::infer();
let model = EmbeddingModel::from_env().expect("failed to load embedding model");

// Query distance indices
let index = DistanceIndexRow::get("test_db", &vector)?;

// Embed question and perform similarity search
let selector = Selector::new(&question, None, &model, &vector)?;
let samples = selection::<4>(
    &Selector::new(
        question.as_ref(),
        None,
        &model,
        &vector,
    )?,
    &index.into(),
    &vector,
)?;

\end{verbatim}

\paragraph{Database Schema}

As the \texttt{.vector} file is still a regular SQLite database, alongside
indexing tables using embeddings (\texttt{float[4096]} or \texttt{float[2048]})
regular database tables can be maintained to save data derived at sampling
time. \texttt{Vector} hosts the following database schema:

\begin{verbatim}
CREATE TABLE IF NOT EXISTS whitelist (
  word             text not null primary key,
  frequency        integer not null,
  domain_count     integer not null
);

CREATE TABLE IF NOT EXISTS databases (
  id               text not null,
  name             text not null,
  graph            text not null
);

CREATE TABLE IF NOT EXISTS distance_indices (
  db_id            text not null,
  distances        text not null
);

CREATE VIRTUAL TABLE IF NOT EXISTS samples USING vec0 (
  id                 text not null,
  db_id              text not null,

  question           text not null,
  question_masked    text not null,
  question_embedding float[2048],

  sql                text not null,
  sql_masked         text not null,
  sql_embedding      float[2048]
);
\end{verbatim}

Which maintains the word whitelist for query masking, the database graphs of
the sample databases and the precomputed database indices. Thus at pipeline
runtime the database indices and whitelists can be reconstructed through a
cheap selection from \texttt{Vector} even though they are not indexed through
embeddings.

The schema of the \texttt{samples} table largely drives the capabilities for
example selection at runtime, it maintains embeddings of the masked sql query
and natural language question respectively. Thus after cosine similarity search
the original query or question can be reconstructed as well as the graph layout
of the underlying database can be accessed through the \texttt{database} table.

The \texttt{database} and \texttt{database\_indices} table contain \texttt{JSON}
columns respectively for serializing complex graph and graph index structures
to the database.

\subsubsection{Embedding and Masking}

The embedding and masking functionalities in \texttt{natural-sampling} abstract
batch embedding of masked question and SQL queries using the GPU so that the
runtime and sampling code can efficiently compute masked embeddings.

\paragraph{Architecture}

The two primary types for embedding plain and masked text are the
\texttt{SemanticString} and \texttt{MaskedSemanticString} structs which
encapsulate strings alognside their embedding vectors. The
\texttt{MaskedSemanticString} version additionally contains the masked and
unmasked versions of the embedded string. Furthermore they offer
\texttt{embed\_seq} and \texttt{embed\_chunked} methods which can be used for
batch computation of embeddings.

\paragraph{Batch Computation}

During sampling, offloading data to the GPUs memory is an expensive operation.
When sampling hundred thousands or millions of samples, incurring the IO
roundtrip from CPU to GPU for computing a single embedding is a
performance bottleneck that increases the sampling time to an order of days
(eg, when sampling the \textsc{SynSQL}) dataset. Thus batch processing can help to
minimise the IO roundtrips needed between the CPU and GPU before computing
embeddings and better utilise the available computing power.

Using the \texttt{embed\_seq} method, a sequence of strings can be embedded and
a sequence of \texttt{SemanticStrings} is returned. Furthermore through the
\texttt{embed\_chunked} method, chunks of unrelated data can be embedded in a
single operation. The method maintains the original chunk layout which makes it
possible to reconstruct the input semantic seamlessly. This optimization is
used to embed the questions and sql queries alongside while maintaining clear
separation in the embedding output.

\begin{verbatim}
    INSERT DIAGRAM
\end{verbatim}

% NOTE: Batch embedding computation time down from 200ms per item to
% approximately 2ms per item when processing 1000-item batches.
% PUT INTO TEXT/diagram description

\paragraph{Integration}

The \texttt{masking} module tightly integrates wiuth the embedding pipeline.
During sampling, questions and queries are first masked, then embedded together
using \texttt{embed\_chunked} and finally combined into
\texttt{MaskedSemanticString} instances for storage in \texttt{Vector}.

\subsubsection{Wasserstein-Weisfeiler-Leman Kernels}


\subsection{Benchmarking Infrastructure}

\textsc{Natural}'s benchmarking infrastructure is implemented in
\texttt{natural-benchmark}. This benchmarking infrastructure is enabling the
development, verification and ultimately deployment of \textsc{Natural}
pipeline. This section focuses on the respective benchmarking infrastructure
development, performance optimizations that were required and engineering
challenges encountered. 

Whilst the benchmarking infrastructure is not part of the core pipeline, they
help to develop and test hypotheses for extensions or design changes
confidently, compare performance across pipeline versions, models and
understand the benchmarking data.

